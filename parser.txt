import base64
import re
import os
import json
from datetime import datetime 
from email.utils import parsedate_to_datetime, parseaddr
from db import insert_email_text, insert_or_update_application, init_db
from bs4 import BeautifulSoup
import html
import joblib  # For loading ML model artifacts
from pathlib import Path


# --- Versioning ---
PARSER_VERSION = '2.2.0'  # Incremented for ML integration
SCHEMA_VERSION = '1.2.0'  # Incremented for predicted_company column

PATTERNS_PATH = Path(__file__).parent / "patterns.json"

if PATTERNS_PATH.exists():
    with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
        patterns_data = json.load(f)

    # Lowercased set for fast membership checks
    KNOWN_COMPANIES = {c.lower() for c in patterns_data.get("aliases", {}).values()}

    # Lowercase keys for domain matching
    DOMAIN_TO_COMPANY = {k.lower(): v for k, v in patterns_data.get("domain_to_company", {}).items()}
else:
    KNOWN_COMPANIES = set()
    DOMAIN_TO_COMPANY = {}


# --- Load ML model artifacts at startup ---
try:
    clf = joblib.load("model/company_classifier.pkl")
    vectorizer = joblib.load("model/vectorizer.pkl")
    label_encoder = joblib.load("model/label_encoder.pkl")
    ml_enabled = True
    print("Ì¥ñ ML model loaded for company prediction.")
except FileNotFoundError:
    ml_enabled = False
    print("‚ö†Ô∏è ML model not found ‚Äî skipping prediction.")

def predict_company(subject, body):
    """Predict company name using the trained ML model."""
    if not ml_enabled:
        return None
    text = (subject or "") + " " + (body or "")
    X = vectorizer.transform([text])
    pred_encoded = clf.predict(X)[0]
    return label_encoder.inverse_transform([pred_encoded])[0]

def get_label_map(service):
    """Fetch Gmail label ID ‚Üí name mapping."""
    labels = service.users().labels().list(userId='me').execute()
    return {label['id']: label['name'] for label in labels['labels']}

def extract_metadata(service, msg_id):
    """Extract subject, date, thread_id, labels, and body text from a Gmail message."""
    msg = service.users().messages().get(userId='me', id=msg_id, format='full').execute()
    headers = msg['payload']['headers']
    
    # Subject extraction
    subject = next((h['value'] for h in headers if h['name'] == 'Subject'), '')

    # Date extraction and parsing
    date_raw = next((h['value'] for h in headers if h['name'] == 'Date'), '')
    # Parse date safely
    try:
        date_obj = parsedate_to_datetime(date_raw)

        date = date_obj.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        date = date_raw
    # Sender and domain extraction
    sender = next((h['value'] for h in headers if h['name'].lower() == 'from'), '')
    match = re.search(r'@([A-Za-z0-9.-]+)', sender)
    sender_domain = match.group(1).lower() if match else ''

    thread_id = msg['threadId']
    label_ids = msg.get('labelIds', [])

    # Map label IDs to human-readable names
    label_map = get_label_map(service)
    label_names = [label_map.get(lid, lid) for lid in label_ids]

    # Decode body
    body = ''
    parts = msg['payload'].get('parts', [])
    for part in parts:
        mime_type = part.get('mimeType')
        data = part['body'].get('data')
        if not data:
            continue

        decoded = base64.urlsafe_b64decode(data).decode('utf-8', errors='ignore')

        if mime_type == 'text/plain':
            body = decoded.strip()
            break  # Prefer plain text
        elif mime_type == 'text/html' and not body:
            soup = BeautifulSoup(decoded, 'html.parser')
            body = soup.get_text(separator=' ', strip=True)
            body = html.unescape(body)

    return {
        'thread_id': thread_id,
        'subject': subject,
        'body': body,
        'date': date,
        'labels': ','.join(label_names),
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'sender': sender,
        'sender_domain': sender_domain,
        'parser_version': PARSER_VERSION
    }

def classify_message(body):
    """Classify message type based on patterns.json categories."""
    body_lower = body.lower()
    for category, phrases in PATTERNS.items():
        for phrase in phrases:
            if phrase in body_lower:
                return category
    return 'outreach'

def extract_status_dates(body, received_date):
    """Extract key status dates from email body."""
    body_lower = body.lower()
    dates = {
        'response_date': '',
        'rejection_date': '',
        'interview_date': '',
        'follow_up_dates': ''
    }

    if any(p in body_lower for p in PATTERNS.get('response', [])):
        dates['response_date'] = received_date
    if any(p in body_lower for p in PATTERNS.get('rejection', [])):
        dates['rejection_date'] = received_date
    if any(p in body_lower for p in PATTERNS.get('interview', [])):
        dates['interview_date'] = received_date
    if any(p in body_lower for p in PATTERNS.get('follow_up', [])):
        dates['follow_up_dates'] = received_date

    return dates

def normalize_text(text):
    """Lowercase, strip punctuation, collapse whitespace."""
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.lower().strip()

def should_ignore(subject, body):
    """Check if subject/body matches any ignore patterns."""
    subject_lower = subject.lower()
    body_lower = body.lower()
    for phrase in PATTERNS.get("ignore", []):
        if phrase in subject_lower or phrase in body_lower:
            print(f"Ignored due to pattern: '{phrase}' in subject/body")
            return True
    return False

# Load known companies from file (one per line)
KNOWN_COMPANIES = set()
if os.path.exists("known_companies.txt"):
    with open("known_companies.txt", "r", encoding="utf-8") as f:
        KNOWN_COMPANIES = {line.strip().lower() for line in f if line.strip()}

# Load domain-to-company mapping from JSON
DOMAIN_TO_COMPANY = {}
if os.path.exists("domain_to_company.json"):
    with open("domain_to_company.json", "r", encoding="utf-8") as f:
        DOMAIN_TO_COMPANY = json.load(f)



ATS_DOMAINS = {
    "myworkday.com",
    "jobvite.com",
    "greenhouse-mail.io",
    "smartrecruiters.com",
    "pageuppeople.com",
    "icims.com"
}

def parse_subject(subject, sender=None, sender_domain=None):
    """Extract company, job title, and job ID from subject line, sender, and optionally sender domain."""
    company = ''
    job_title = ''
    job_id = ''

    # Normalize once
    subject_clean = subject.strip()
    subj_lower = subject_clean.lower()
    domain_lower = sender_domain.lower() if sender_domain else None

    # 1. Colon-prefix rule
    m = re.match(r"^([A-Z][A-Za-z0-9&.\- ]+):", subject_clean)
    if m:
        company = m.group(1).strip()

    # 2. Known companies list match
    if not company and KNOWN_COMPANIES:
        for known in KNOWN_COMPANIES:
            if known in subj_lower:
                company = known.title()
                break

    # 3. Sender domain mapping
    if not company and domain_lower and domain_lower in DOMAIN_TO_COMPANY:
        company = DOMAIN_TO_COMPANY[domain_lower]

    # 3b. ATS domain ‚Üí extract from display name
    if not company and domain_lower in ATS_DOMAINS and sender:
        display_name, _ = parseaddr(sender)
        cleaned = re.sub(
            r'\b(Workday|Recruiting Team|Careers|Talent Acquisition Team|HR|Hiring)\b',
            '',
            display_name,
            flags=re.I
        ).strip()
        if cleaned:
            company = cleaned

    # 4‚Äì8. Existing regex patterns in order
    patterns = [
        (r'application (?:to|for|with)\s+([A-Z][\w\s&\-]+)', re.IGNORECASE),
        (r'(?:from|with|at)\s+([A-Z][\w\s&\-]+)', re.IGNORECASE),
        (r'^([A-Z][\w\s&\-]+)\s+(Job|Application|Interview)', 0),
        (r'-\s*([A-Z][\w\s&\-]+)\s*-\s*', 0),
        (r'^([A-Z][\w\s&\-]+)\s+application', 0)
    ]
    for pat, flags in patterns:
        if not company:
            match = re.search(pat, subject_clean, flags)
            if match:
                company = match.group(1).strip()

    # Job title extraction
    title_match = re.search(
        r'job\s+(?:submission\s+for|application\s+for|title\s+is)?\s*([\w\s\-]+)',
        subject_clean,
        re.IGNORECASE
    )
    job_title = title_match.group(1).strip() if title_match else ''

    # Job ID extraction
    id_match = re.search(
        r'(?:Job\s*#?|Position\s*#?|jobId=)([\w\-]+)',
        subject_clean,
        re.IGNORECASE
    )
    job_id = id_match.group(1).strip() if id_match else ''

    return {
        'company': company,
        'job_title': job_title,
        'job_id': job_id
    }
    
def ingest_message(service, msg_id, conn=None):
    try:
        metadata = extract_metadata(service, msg_id)
    except Exception as e:
        print(f"Failed to extract data for {msg_id}: {e}")
        return 

    # ‚úÖ Apply ignore patterns from patterns.json
    if should_ignore(metadata['subject'], metadata['body']):
        print(f"Ignored: {metadata['subject']}")
        return 
    
    # Store subject and body for ML training
    insert_email_text(msg_id, metadata['subject'], metadata['body'])

    # Classify and extract dates
    status = classify_message(metadata['body'])
    status_dates = extract_status_dates(metadata['body'], metadata['date'])

    # Parse subject for company/job info
    parsed_subject = parse_subject(metadata['subject'], metadata.get('sender_domain'))
    company = parsed_subject['company']
    job_title = parsed_subject['job_title']
    job_id = parsed_subject['job_id']

    # ML fallback if no company OR placeholder "Intel"
    predicted_company = ''
    if not company or company.lower() == "intel":
        predicted = predict_company(metadata['subject'], metadata['body'])
        print(f"Predicted company: {predicted} for message ID {msg_id}")
        if predicted:
            predicted_company = predicted
            company = predicted
            print(f"Ì¥Æ [ML USED] Predicted company '{predicted_company}' for message {msg_id}")

    # Build full record for DB
    record = {
        'thread_id': metadata['thread_id'],
        'company': company,
        'predicted_company': predicted_company,
        'job_title': job_title,
        'job_id': job_id,
        'first_sent': metadata['date'],
        'response_date': status_dates['response_date'],
        'follow_up_dates': status_dates['follow_up_dates'],
        'rejection_date': status_dates['rejection_date'],
        'interview_date': status_dates['interview_date'],
        'status': status,
        'labels': metadata['labels'],
        'subject': metadata['subject'],
        'sender_doman': metadata['sender_domain'],
        'sender': metadata['sender'],
        'last_updated': metadata['last_updated']
    }
    
    print(f"[DEBUG] sender={metadata.get('sender')!r}, sender_domain={metadata.get('sender_domain')!r}")

    insert_or_update_application(record)
    print(f"Logged: {metadata['subject']} ‚Üí {status}")
