

===== START OF FILE: .gitignore =====

credentials.json
settings.json
token.pickle
*.db
*.pkl
__pycache__/
*.pyc
remind/
scripts/
update_copilot/
venv/
model/*
.pytest_cache/
.DS_Store
.vscode/
*.patch
staticfiles/
.secrets*
.coverage
*.txt
*/.DS_Store

===== END OF FILE: .gitignore =====



===== START OF FILE: .pre-commit-config.yaml =====

repos:
-   repo: https://github.com/Yelp/detect-secrets
    rev: v1.5.0
    hooks:
    -   id: detect-secrets
        args: ['--baseline', '.secrets.baseline']


===== END OF FILE: .pre-commit-config.yaml =====



===== START OF FILE: BACKLOG.md =====

# Job Tracker Backlog

## Now / Next / Later Roadmap

### **Now** (In Progress)
- [Story 1: Add company/job correlation index](#story-1-add-companyjob-correlation-index)
- [Story 9: Environment readiness checker and admin diagnostics]

### **Now** (Active Focus)

- [Story 3: Refine company name extraction](#story-3-refine-company-name-extraction)
- [Story 6: Refine status classification](#story-6-refine-status-classification)

### **Next** (Queued for Upcoming Work)

- [Story 2: Baseline HTML dashboard](#story-2-baseline-html-dashboard)
- [Story 4: Improve job title extraction](#story-4-improve-job-title-extraction)
- [Story 5: Implement job ID extraction](#story-5-implement-job-id-extraction)

### **Later** (Future Enhancements / Nice-to-Haves)

- [Story 7: Security & maintainability foundation](#story-7-security--maintainability-foundation)
- [Story 8: Documentation & onboarding](#story-8-documentation--onboarding)

--

## Epic: Professional Job Tracker Dashboard

As a job seeker, I want a secure, modern dashboard so I can track, visualize, and manage all my job applications in one place.

---

### Story 1: Add company/job correlation index

**Description:**  
As a developer, I want to add a composite index `(company, job_title, job_id)` so the system can group all related messages for visualization.

**Acceptance Criteria:**

- DB schema updated with composite index.
- Historical migration script populates missing values.
- Index creation is idempotent and safe to run multiple times.
- Unit test verifies correlation queries return correct grouped results.

**Security & Documentation:**

- Validate DB inputs to prevent SQL injection.
- Document schema changes in `SCHEMA_CHANGELOG.md` with version/date.
- Include rollback instructions in case of migration failure.

---

### Story 2: Baseline HTML dashboard

**Description:**  
As a user, I want a clean, responsive HTML dashboard to view and filter my applications.

**Acceptance Criteria:**

- Displays applications grouped by `(company, job_title, job_id)`.
- Filters by status, company, date range.
- Responsive layout for desktop and mobile.
- No inline JavaScript; all scripts loaded from vetted sources.
- Tested in at least Chrome, Firefox, and Edge.

**Security & Documentation:**

- Escape all dynamic content to prevent XSS.
- Use HTTPS for all asset loading.
- Document dashboard architecture in `DASHBOARD_OVERVIEW.md`.

---

### Story 3: Refine company name extraction

**Description:**  
As a developer, I want to use sender domain/email to fill in missing company names when auto‚Äëextraction fails.

**Acceptance Criteria:**

- If `company` is empty, lookup from `domain_to_company` mapping.
- Mapping stored in `patterns.json` or DB table for easy updates.
- Unit tests cover fallback logic.

**Security & Documentation:**

- Sanitize domain inputs before lookup.
- Document fallback logic in `EXTRACTION_LOGIC.md`.

---

### Story 4: Improve job title extraction

**Description:**  
As a user, I want accurate job titles extracted from subjects and bodies.

**Acceptance Criteria:**

- Regex + ML hybrid extraction.
- Stopword filtering for generic titles.
- Unit tests with real‚Äëworld examples.

**Security & Documentation:**

- Avoid regex patterns that can cause catastrophic backtracking.
- Document extraction patterns and ML model training data sources.

---

### Story 5: Implement job ID extraction

**Description:**  
As a user, I want job IDs extracted from ATS emails.

**Acceptance Criteria:**

- Pattern library for Workday, Greenhouse, Lever, etc.
- Unit tests for each ATS pattern.
- Fallback to `null` if no match.

**Security & Documentation:**

- Validate extracted IDs to avoid injection into queries.
- Document patterns in `EXTRACTION_LOGIC.md`.

---

### Story 6: Refine status classification

**Description:**  
As a user, I want more accurate status detection for applications.

**Acceptance Criteria:**

- Combine keyword rules + ML classification.
- Unit tests for each status type.
- Confidence score stored in DB.

**Security & Documentation:**

- Ensure ML model loading is safe and versioned.
- Document classification logic and training data.

---

### Story 7: Security & maintainability foundation

**Description:**  
As a developer, I want to ensure the codebase follows secure coding and maintainability best practices.

**Acceptance Criteria:**

- Input validation for all external data (Gmail API, DB, dashboard).
- Centralized config for DB path, patterns, and model locations.
- Logging with no sensitive data exposure.
- Unit test coverage ‚â• 80%.

**Security & Documentation:**

- Follow OWASP Top 10 guidelines.
- Maintain `SECURITY.md` with threat model and mitigations.
- Maintain `README.md` with setup, run, and test instructions.

---

### Story 8: Documentation & onboarding

**Description:**  
As a future maintainer, I want clear documentation so I can onboard quickly.

**Acceptance Criteria:**

- `README.md` with quick start.
- `CONTRIBUTING.md` with coding standards.
- `CHANGELOG.md` for version history.
- Inline docstrings for all public functions.

**Security & Documentation:**

- Document all security‚Äërelevant decisions.
- Keep docs in sync with code changes.
### Story 9: Environment readiness checker and admin diagnostics

**Description:**  
As a developer or maintainer, I want a script and admin page that verifies the presence and freshness of critical files (models, DB, JSONs, OAuth credentials) so I can ensure the system is ready to run or deploy.

**Acceptance Criteria:**

- `check_env.py` script verifies:
  - Required files: DB, model artifacts, patterns.json, companies.json
  - Optional files: labeled_subjects.csv, alias mappings
  - Directory write permissions
  - Git branch and commit
  - Django migration status
  - OAuth credentials
  - `detect-secrets` baseline presence and scan results
- Admin page at `/admin/environment_status/` displays:
  - File presence and last modified timestamps
  - Git info
  - Secret scan status
- Automatically generates `.secrets.baseline` if missing

**Security & Documentation:**

- No secrets or credentials printed in logs or UI
- Documented in `SECURITY.md` and `README.md` under setup and deployment

===== END OF FILE: BACKLOG.md =====



===== START OF FILE: CHANGELOG.md =====

# Changelog

All notable changes to this project will be documented here.

The format follows [Keep a Changelog](https://keepachangelog.com/en/1.1.0/)  
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).
---

### üîπ 3. **Update `CHANGELOG.md`**

Add an entry like:

```markdown
## [Unreleased]
### Added
- `check_env.py` script to verify environment readiness (DB, models, patterns, permissions, Git, OAuth, detect-secrets)
- Admin page `/admin/environment_status/` to display environment diagnostics
- Integrated `detect-secrets` scan into `check_env.py`, with auto-generation of `.secrets.baseline` if missing
- Training summary output now displayed on the label admin page after labeling
- ML label and confidence now shown in debug output during message ingestion

### Changed
- `label_messages` view updated to trigger `train_model.py` after labeling
- `label_messages.html` updated to include collapsible training summary block
- `check_env.py` extended to include Git status, Django migrations, directory permissions, and OAuth credential checks

### Added
- `ingest_gmail` command with subject parsing and ignore logic
- `IngestionStats` model for daily ingestion tracking
- `IgnoredMessage` model for ML retraining
- Debug flag to control verbosity

### Changed
- `parse_subject()` now returns structured output with ignore flag
- `ingest_message()` refactored for clean filtering and enrichment
 
---
## 2025-09-11 [dashboard-init-20250911]
- Initial Django dashboard structure pushed
- VS Code settings tailored for ML/Django workflows
- Repo synced from WSL Ubuntu with SSH key authentication

## 2025-09-09 Ingestion Pipeline ‚Äì Company Extraction & Index Cleanup
- Added Tier‚ÄØ3 domain mapping enrichment: If company is blank after whitelist/heuristics, now populated from patterns.json ‚Üí domain_to_company.
- Refined Tier‚ÄØ1‚Äì4 company cleanup:
- Tier‚ÄØ1: Keep if in known_companies.txt whitelist.
- Tier‚ÄØ2: Keep if passes is_valid_company() heuristics.
- Tier‚ÄØ3: Fallback to domain mapping.
- Tier‚ÄØ4: ML prediction if still blank or generic.
- Removed redundant build_company_job_index() call to avoid unnecessary recomputation.
- Centralized is_valid_company() in db.py for shared use by parser and training logic.
- Impact: New ingestions will have cleaner company values and fewer junk company_job_index entries. Historical rows unchanged; optional backfill can align them.


## [Unreleased]
### Added
- **Story 1:** Added composite index `(company, job_title, job_id)` to DB schema for correlation and dashboard grouping.
- **Story 3:** Implemented domain/email fallback for company name extraction when auto‚Äëextraction fails.
- **Story 6:** Enhanced status classification with keyword + ML hybrid approach.
- `db_helpers.py` with `get_application_by_sender()` for sender/domain correlation checks.
- Correlation logic in `ingest_message()` to keep follow‚Äëups from known senders within 1 year.
- SQL indexes on `(sender, first_sent DESC)` and `(sender_domain, first_sent DESC)` for fast correlation lookups.

### Changed
- Updated `should_ignore()` to run after correlation check, reducing false positives.
- Cleaned `ignore` patterns in `patterns.json` to remove overly broad terms.
- Ensured `status` and `status_dates` are always assigned before ignore logic to prevent `UnboundLocalError`.

### Fixed
- Corrected DB filename mismatch (`applications.db` ‚Üí `job_tracker.db`) in `db_helpers.py`.
- Resolved ingestion crash when correlation logic ran before DB schema initialization.

### Security
- Sanitized domain inputs before company name fallback lookup.
- All DB queries use parameterized statements to prevent SQL injection.

### Notes
- These changes lay the groundwork for the MVP dashboard (Stories 1, 3, 6 in `BACKLOG.md`).
- Next step: baseline HTML dashboard with grouping, filters, and responsive design.

---

## [2025-09-08]
### Added
- Colon‚Äëprefix detection to `parse_subject()` for cases like `"MITRE: ..."`
- Known companies preload from `known_companies.txt`
- Sender‚Äëdomain mapping from `domain_to_company.json`
- Manual labeling script (`label_companies.py`) for training data

### Changed
- `ingest_message()` now runs ML fallback when company is empty or `"Intel"`
- ML predictions stored in `predicted_company` for review

### Notes
- `known_companies.txt` and `domain_to_company.json` can be expanded anytime
- Next step: integrate `sender_domain` capture in `extract_metadata()`

---

## [2.1.0] - 2025-09-08
### Added
- Fallback parsing for HTML‚Äëonly email bodies using BeautifulSoup
- Centralized SQLite connection logic with retry‚Äësafe `get_db_connection()` function
- New `email_text` table for ML training (subject + body)
- `insert_email_text()` and `load_training_data()` functions in `db.py`

### Changed
- Parser now prefers `text/plain` but gracefully falls back to `text/html` when needed
- All DB access now routed through `get_db_connection()` for lock safety

### Fixed
- Ignored messages no longer written to `email_text`, preserving ML training quality
- Improved error handling during DB initialization in `main.py`

### Notes
- This version sets the stage for ML‚Äëbased company prediction using subject + body text
- Recommended tag: `v2.1.0` before vectorization and model training

---

## [2.0.0] - 2025-09-07
### Added
- Regex‚Äëbased company extraction for diverse subject formats
- Tokenized ignore logic with normalization
- Dynamic Gmail query builder from `patterns.json`

### Improved
- Subject parsing coverage for ATS and recruiter formats
- Classification logic excluding ignore phrases

### Notes
- This version serves as baseline before ML integration

---

## [1.1.0] - 2025-09-07
### Added
- SQLite ingestion logic via `db.py`, including `applications` and `follow_ups` tables
- `last_updated` timestamp field for audit clarity and sync tracking
- Indexing on `status` and `company` fields for performance optimization
- `meta` table for schema versioning and future migration support
- Parser versioning (`PARSER_VERSION`) and timezone‚Äëaware timestamps in `parser.py`
- Modular Gmail message ingestion via `main.py`, with classification and DB integration

### Fixed
- Deprecated `datetime.utcnow()` replaced with `datetime.now(timezone.utc)` for proper UTC handling
- Ensured `last_updated` field is passed through ingestion pipeline and written to DB

### Next
- Parse company, job_title, and job_id from message headers and subject lines
- Modularize date extraction for `response_date`, `rejection_date`, `interview_date`, and `follow_up_dates`
- Scaffold CLI reporting and export tools for job metrics and application status

---

## [0.1.0] - 2025-09-06
### New
- Gmail OAuth authentication via `gmail_auth.py`
- Message parsing and classification via `parser.py`
- External pattern file `patterns.json` for classification logic
- Initial project structure and Git setup

### Upcoming
- Build SQLite ingestion logic (`db.py`)
- Populate tracker with parsed messages
- Add CLI reporting for job metrics

===== END OF FILE: CHANGELOG.md =====



===== START OF FILE: DASHBOARD_OVERVIEW.md =====

# DASHBOARD_OVERVIEW.md

## Overview

This dashboard provides a secure, local-only interface for tracking job applications, interviews, and message threads.

---

## Features

- Threaded message viewer per company
- Weekly/monthly rejection and interview stats
- Upcoming interview calendar
- Labeling interface with auto-retraining
- Environment diagnostics via `/admin/environment_status/`

---

## Architecture

- Django-based admin and dashboard views
- SQLite backend (`job_tracker.db`)
- Models include `Application`, `Message`, `IgnoredMessage`, `IngestionStats`
- ML artifacts stored in `/model/` as `.pkl` files

---

## Security

- No external data transmission
- All dynamic content escaped to prevent XSS
- Secret scanning enforced via `detect-secrets`
- Admin-only access to environment diagnostics

---

## Next Steps

- Implement baseline dashboard UI (Story 2)
- Add filters by status, company, and date
- Group applications by `(company, job_title, job_id)`

===== END OF FILE: DASHBOARD_OVERVIEW.md =====



===== START OF FILE: EXTRACTION_LOGIC.md =====

# EXTRACTION_LOGIC.md

## Overview

This document outlines how company names, job titles, job IDs, and status values are extracted from Gmail messages.

---

## Company Name Extraction

### Tiered Logic

1. **Tier 1**: Whitelist match from `known_companies.txt`
2. **Tier 2**: Heuristic validation via `is_valid_company()`
3. **Tier 3**: Fallback to domain mapping from `patterns.json ‚Üí domain_to_company`
4. **Tier 4**: ML prediction if still blank or generic

### Notes

- Domain inputs are sanitized before lookup
- Final company value is stored in `company_obj` and `company_job_index`

---

## Job Title Extraction

- Currently uses subject line parsing
- ML hybrid extraction planned (see Story 4 in `BACKLOG.md`)
- Stopword filtering and regex tuning in progress

---

## Job ID Extraction

- Placeholder logic present
- Pattern library for ATS systems (Workday, Greenhouse, Lever) planned
- See Story 5 in `BACKLOG.md`

---

## Status Classification

- Combines keyword rules from `patterns.json` with ML classification
- Confidence score stored in DB
- Status values include: `interview`, `rejection`, `job_alert`, `application`, `follow_up`, `noise`

===== END OF FILE: EXTRACTION_LOGIC.md =====



===== START OF FILE: NOTES.md =====

# Version Release Workflow (Authoritative Local Copy)

This project uses the local `main` branch as the **source of truth**.  
When creating a new version, follow this process to update the repository and push to the remote.

---

## 1. Commit All Local Changes

```bash
git status
if there are changes:
git add .
git commit -m "feat: <short description of changes>"

## 2. Update CHANGELOG.md
- Move items from [Unreleased] into a new version section with today‚Äôs date.
- Save and commit:
git add CHANGELOG.md
git commit -m "docs: update changelog for vX.Y.Z"

## 3. Create Annotated Tag
git tag -a vX.Y.Z -m "Release vX.Y.Z: <short summary from changelog>"

## 4. Push Local Main to Remote (Overwrite Remote)
git push origin main --force

## 5. Push the New Tag
git push origin vX.Y.Z

## Example for v2.2.2
git add .
git commit -m "feat: implement job title extraction improvements"
git add CHANGELOG.md
git commit -m "docs: update changelog for v2.2.2"
git tag -a v2.2.2 -m "Release v2.2.2: job title extraction improvements"
git push origin main --force
git push origin v2.2.2

===== END OF FILE: NOTES.md =====



===== START OF FILE: README.md =====

# GmailJobTracker Dashboard

A local-only Django dashboard for tracking job applications, interviews, and message threads.  
No data leaves your machine. No external servers. Just clean, private job tracking.

## Features

- Threaded message viewer per company
- Weekly/monthly rejection/interview stats
- Upcoming interview calendar
- Clickable company listing with full message history
- Requires Gmail OAuth setup and label configuration.
## Setup

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python manage.py migrate
python manage.py runserver

# Check environment readiness
python check_env.py

# Visit the admin panel and label messages
# After labeling, the model will retrain automatically and show training output
To view environment diagnostics:
- Visit /admin/environment_status/ (admin login required)

## Privacy Statement

This tool stores all data locally in db.sqlite3. It does not communicate with any external server.


##Features

```markdown
- Environment diagnostics via `/admin/environment_status/`
- Secret scanning with `detect-secrets` baseline enforcement
- Auto-retraining of ML model after labeling

---

## üß† Next Step: Scaffold Django App

Let me know when you‚Äôre ready and I‚Äôll generate:
- `models.py` with threading logic
- `views.py` for dashboard metrics
- `urls.py` routing
- Starter templates for metrics + company threads


===== END OF FILE: README.md =====



===== START OF FILE: SECURITY.md =====

# SECURITY.md

## Overview

This project is designed for local-only operation. No data is transmitted externally, and no third-party services are used beyond Gmail OAuth authentication.

## Threat Model

| Threat | Mitigation |
|--------|------------|
| Credential leakage | Integrated `detect-secrets` scanning with `.secrets.baseline` enforcement |
| SQL injection | All DB queries use parameterized statements |
| XSS in dashboard | Dynamic content is escaped in templates |
| Insecure model loading | ML models are loaded from local `.pkl` files only; no remote fetches |
| Git exposure | `.secrets.baseline` ensures secrets are not committed |

## Secure Coding Practices

- All external inputs (Gmail, DB, dashboard) are validated or sanitized
- No secrets are printed in logs or exposed in the admin UI
- Directory permissions and file presence are checked via `check_env.py`
- Git status and commit info are surfaced for audit traceability

## Tools

- `detect-secrets` for pre-commit and CI/CD scanning
- `check_env.py` for runtime environment validation

## Deployment Notes

- All data is stored locally in `job_tracker.db`
- OAuth credentials (`token.json`, `credentials.json`) are never uploaded or shared
- Admin-only diagnostics available at `/admin/environment_status/`

===== END OF FILE: SECURITY.md =====



===== START OF FILE: SESSION_STATE.md =====

## Session: Sept 12, 2025
- Django dashboard wired
- Drill-down view rendering
- job_tracker.db migrated
- Gmail ingestion logic not yet reconnected
- Next: threaded message view, timeline logic

===== END OF FILE: SESSION_STATE.md =====



===== START OF FILE: __init__.py =====


===== END OF FILE: __init__.py =====



===== START OF FILE: alias_candidates.json =====

[
  {
    "alias": "UIC & the Bowhead Family of Companies",
    "suggested": "UIC & the Bowhead",
    "count": 1,
    "similarity": 0.0,
    "reason": "cleaned_overlap:0.5714285714285714",
    "timestamp": "2025-10-11T22:52:34",
    "source_thread_id": "1997b2a75c9c3396",
    "source_subject": "Thank you for your application with UIC & the Bowhead Family of Companies"
  }
]

===== END OF FILE: alias_candidates.json =====



===== START OF FILE: alias_candidates.py =====

import pandas as pd
import re
import json
from difflib import SequenceMatcher
from db import get_db_connection, PATTERNS_PATH, COMPANIES_PATH, is_valid_company
import argparse
import csv
from datetime import datetime

parser = argparse.ArgumentParser(description="Detect alias candidates from company names")
parser.add_argument("--export", help="Path to export alias suggestions (JSON or CSV)")
args = parser.parse_args()

# --- Config ---
MIN_COUNT = 1  # flag companies with <= this many occurrences
SIMILARITY_THRESHOLD = 0.75  # 0..1, higher = stricter match
DEBUG = True  # set True to enable verbose debug prints

# --- Load raw company names from DB ---
def load_raw_companies():
    conn = get_db_connection()
    query = """
        SELECT DISTINCT a.company, COUNT(*) as count, a.thread_id, m.subject
        FROM applications a
        JOIN tracker_message m ON a.thread_id = m.thread_id
        WHERE a.company IS NOT NULL
            AND a.company != ''
            AND (m.ml_label IS NULL OR m.ml_label NOT IN ('noise', 'job_alert'))
        GROUP BY a.company, a.thread_id, m.subject
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

# --- Heuristic: looks like a clean company name ---
def is_clean_company(name):
    if not name or not name.strip():
        return False
    # Short (<= 4 words), no obvious job/status keywords
    if len(name.split()) > 4:
        return False
    if re.search(r'(interview|application|position|received|manager|engineer|researcher|job|role|title|follow-up)', name, re.I):
        return False
    return True

# --- Similarity helper ---
def similar(a, b):
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

# additional heuristics
PERSONAL_NAME_RE = re.compile(r'^[A-Z][a-z]{1,20} [A-Z][a-z]{1,20}$')
NOISE_TOKEN_RE = re.compile(r'(please|sign|attached|thanks|thank you|sincerely|regards|cv|resume|mr|ms|mrs)', re.I)

def is_personal_name_candidate(name):
    # loose personal-name detector: two capitalized words and not corporate suffix
    if not name:
        return False
    name_stripped = name.strip()
    parts = name_stripped.split()
    if len(parts) != 2:
        return False
    if not PERSONAL_NAME_RE.match(name_stripped):
        return False
    # allow through common corporate suffixes
    if re.search(r'\b(Inc|LLC|Ltd|Corp|Company|Co\.|PLC|Systems|Technologies|Group|Holdings)\b', name_stripped, re.I):
        return False
    # avoid flagging likely short brand names that are two words if any token contains a non-name char
    if any(re.search(r'[^A-Za-z\-]', p) for p in parts):
        return False
    # require tokens not to be excessively long
    if max(len(parts[0]), len(parts[1])) > 18:
        return False
    return True

def token_set_overlap(a, b):
    a_tokens = {t for t in re.findall(r'[A-Za-z0-9&\-]+', (a or "").lower())}
    b_tokens = {t for t in re.findall(r'[A-Za-z0-9&\-]+', (b or "").lower())}
    if not a_tokens or not b_tokens:
        return 0.0
    return len(a_tokens & b_tokens) / len(a_tokens | b_tokens)

def cleaned_candidate(name):
    if not name:
        return ""
    s = name

    # drop leading articles and leading/trailing noise
    s = re.sub(r'^\s*(the|a|an)\s+', '', s, flags=re.I)

    # remove common family-of-company phrases and parenthetical-like tails
    s = re.sub(r'\b(&\s*the\s+.*Family of Companies|Family of Companies|Family of|Group of Companies|Group of)\b', '', s, flags=re.I)
    s = re.sub(r'\b(,?\s*(Inc|LLC|Ltd|Corp|Co\.|Company|PLC|Systems|Technologies|Holdings|Group))\b', '', s, flags=re.I)

    # remove role/job-title fragments that sometimes got captured as company
    s = re.sub(r'\b(the\s+)?(senior|lead|principal|junior|machine learning|engineer|developer|manager|analyst|recruiter|consultant)\b', '', s, flags=re.I)

    # remove known noise tokens and punctuation
    s = re.sub(r'\b(Application|Interview|Update|Confirmation|Availability|Please|Attached|Signature|Sign|Resume|CV)\b', '', s, flags=re.I)
    s = re.sub(r'[^A-Za-z0-9&\- ]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()

    return s

def likely_noise(name):
    if not name:
        return True

    # role/title heavy -> noise
    if re.search(r'\b(senior|lead|principal|junior|machine learning|engineer|developer|manager|analyst|recruiter|intern)\b', name, re.I):
        return True

    # exclude very long strings or those containing typical message tokens
    if len(name.split()) > 8:
        return True
    if NOISE_TOKEN_RE.search(name):
        return True

    # lots of punctuation (more than 6 non-word, non-space, non-&- chars)
    if len(re.findall(r'[^\w\s&\-]', name)) > 6:
        return True

    return False

def best_canonical_by_overlap(name, canonical_names, min_overlap=0.4):
    best = None
    best_score = 0.0
    for canon in canonical_names:
        ov = token_set_overlap(name, canon)
        if ov > best_score:
            best_score = ov
            best = canon
    return (best, round(best_score, 2)) if best_score >= min_overlap else (None, 0.0)

# --- Main ---
if __name__ == "__main__":
    df = load_raw_companies()

    # Load patterns.json and companies.json
    try:
        with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
            with open(COMPANIES_PATH, "r", encoding="utf-8") as e:
                patterns = json.load(f)
                aliases = json.load(e)
    except FileNotFoundError:
        patterns = {}
        aliases = {}

    ignore_patterns = set(p.lower() for p in patterns.get("ignore", []))
    existing_aliases = set(aliases.get("aliases", {}).keys())
    canonical_names = set(aliases.get("aliases", {}).values())

    print(f"üìä Found {len(df)} unique company names in DB")
    print(f"üìÇ Existing aliases in patterns.json: {len(existing_aliases)}")
    print(f"üö´ Ignoring {len(ignore_patterns)} patterns from patterns.json")

    # Filter: low-frequency, not ignored, not already an alias, not clean
    candidates = df[
        (df['count'] <= MIN_COUNT) &
        (~df['company'].isin(existing_aliases)) &
        (~df['company'].str.lower().isin(ignore_patterns)) &
        (~df['company'].apply(is_clean_company))
    ]
    print(f"  candidates after initial filter: {len(candidates)}")

    # Drop candidates that fail global company validity check
    candidates = candidates[candidates["company"].apply(is_valid_company)]
    print(f"  candidates after is_valid_company filter: {len(candidates)}")

    if candidates.empty:
        print("\n‚úÖ No new alias candidates found.")
    else:
        print("\nüîç Potential alias mappings to add:")
        # Will collect final export rows
        export_data = []

        for idx, row in candidates.iterrows():
          
            name = row['company']
            count = row['count']
            
            # Fetch thread_id and subject for audit
            thread_id = df.iloc[idx]['thread_id'] if 'thread_id' in df.columns else None
            subject = df.iloc[idx]['subject'] if 'subject' in df.columns else None

            # skip obviously noisy candidates early (but log why)
            noisy = likely_noise(name)
            personal = is_personal_name_candidate(name)
            # cleaned fallback
            clean = cleaned_candidate(name)

            # 1) try exact/similarity-based canonical candidates
            sims = [canon for canon in canonical_names if similar(name, canon) >= SIMILARITY_THRESHOLD]
            best_canon = sims[0] if sims else None
            sim_score = round(similar(name, best_canon) if best_canon else 0.0, 2)

            # 2) if no good similarity match, try token-overlap based canonical matching (captures "UIC & the Bowhead...")
            if not best_canon:
                overlap_canon, overlap_score = best_canonical_by_overlap(name, canonical_names, min_overlap=0.35)
                if overlap_canon:
                    best_canon = overlap_canon
                    sim_score = overlap_score  # reuse sim_score field for export/decision
        
            clean_vs_name_overlap = token_set_overlap(clean, name)
            clean_shorter = 0 < len(clean) < len(name)

            # Log full candidate diagnostics when DEBUG
            if DEBUG:
                print("----")
                print(f'candidate: "{name}" (count={count})')
                print(f'  clean: "{clean}"')
                print(f'  best_canon: "{best_canon}" sim_score={sim_score}')
                print(f'  clean_vs_name_overlap={clean_vs_name_overlap}')
                print(f'  clean_shorter={clean_shorter}')
                print(f'  likely_noise={noisy} is_personal_name_candidate={personal}')

            # Decision rules (ordered by trust)
            suggestion = None
            reason = None

            # 1) If a canonical exists with good similarity, prefer it (>= 0.75)
            if best_canon and sim_score >= 0.75 and best_canon.lower() != name.lower():
                suggestion = best_canon
                reason = f"canonical_sim:{sim_score}"

            # 2) If cleaned is meaningfully simpler and looks clean, prefer cleaned
            elif clean and clean_shorter and is_clean_company(clean):
                if clean_vs_name_overlap >= 0.35 or len(clean.split()) <= 3:
                    if clean.lower() != name.lower():
                        suggestion = clean
                        reason = f"cleaned_overlap:{clean_vs_name_overlap}"

            # 3) If canonical exists but sim lower than 0.75 and cleaned is poor, still allow canonical if sim >= 0.6
            elif best_canon and 0.6 <= sim_score < 0.75 and best_canon.lower() != name.lower():
                suggestion = best_canon
                reason = f"weak_canonical_sim:{sim_score}"

            # 4) Skip clear personal-name candidates but allow organizations that include '&' or corporate suffixes
            if not suggestion and is_personal_name_candidate(name) and '&' not in name and re.search(
                r'\b(Inc|LLC|Ltd|Corp|Co\.|Company|PLC|Systems|Technologies|Group|Holdings)\b', name, re.I
            ) is None:
                if DEBUG:
                    print(f"  Skipping personal-name: {name}")
                continue

            # 5) If likely noise and no suggestion, skip
            if not suggestion and noisy:
                if DEBUG:
                    print(f'  SKIP: likely noise and no suggestion')
                continue

            if suggestion:
                print(f'  "{name}" -> "{suggestion}" [{reason}, count={count}]')
                
                export_data.append({
                    "alias": name,
                    "suggested": suggestion,
                    "count": count,
                    "similarity": sim_score,
                    "reason": reason,
                    "timestamp": datetime.now().isoformat(timespec='seconds'),
                    "source_thread_id": thread_id,
                    "source_subject": subject
                })

                if DEBUG:
                    print(f'  ‚úÖ Exported: "{name}" ‚Üí "{suggestion}" | reason={reason} | thread_id={thread_id}')

        # Export if requested
        if args.export:
            if args.export.endswith(".json"):
                with open(args.export, "w", encoding="utf-8") as f:
                    json.dump(export_data, f, indent=2)
            elif args.export.endswith(".csv"):
                with open(args.export, "w", newline="", encoding="utf-8") as f:
                    writer = csv.DictWriter(f, fieldnames=["alias", "suggested", "count", "similarity", "reason"])
                    writer.writeheader()
                    writer.writerows(export_data)

            print(f"‚úÖ Exported {len(export_data)} alias suggestions to {args.export}")

===== END OF FILE: alias_candidates.py =====



===== START OF FILE: alias_suggestions.json =====

[
  {
    "alias": "UIC & the Bowhead Family of Companies",
    "suggested": "UIC & the Bowhead",
    "count": 1,
    "similarity": 0.0,
    "reason": "cleaned_overlap:0.5714285714285714"
  }
]

===== END OF FILE: alias_suggestions.json =====



===== START OF FILE: changelog_cli.py =====

import argparse
from changelog_parser import parse_changelog

def main():
    parser = argparse.ArgumentParser(description="Audit CHANGELOG.md entries")
    parser.add_argument('--version', help="Filter by version (e.g., 1.1.0)")
    parser.add_argument('--section', help="Filter by section (e.g., Added, Fixed, Next)")
    parser.add_argument('--keyword', help="Search for keyword in entries")
    parser.add_argument('--file', default='CHANGELOG.md', help="Path to changelog file")

    args = parser.parse_args()
    entries = parse_changelog(args.file)

    for entry in entries:
        if args.version and entry['version'] != args.version:
            continue

        print(f"\n## Version {entry['version']} ({entry['date']})")
        for section, items in entry['sections'].items():
            if args.section and section.lower() != args.section.lower():
                continue
            print(f"  ### {section}")
            for item in items:
                if args.keyword and args.keyword.lower() not in item.lower():
                    continue
                print(f"    - {item}")

if __name__ == '__main__':
    main()

===== END OF FILE: changelog_cli.py =====



===== START OF FILE: changelog_parser.py =====

import re
from collections import defaultdict

def parse_changelog(path='CHANGELOG.md'):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    changelog = []
    current_entry = {}
    current_section = None

    version_pattern = re.compile(r'^## \[(\d+\.\d+\.\d+)\] - (\d{4}-\d{2}-\d{2})')
    section_pattern = re.compile(r'^### (\w+)')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        version_match = version_pattern.match(line)
        if version_match:
            if current_entry:
                changelog.append(current_entry)
            current_entry = {
                'version': version_match.group(1),
                'date': version_match.group(2),
                'sections': defaultdict(list)
            }
            current_section = None
            continue

        section_match = section_pattern.match(line)
        if section_match:
            current_section = section_match.group(1)
            continue

        if current_section and current_entry:
            current_entry['sections'][current_section].append(line.lstrip('- ').strip())

    if current_entry:
        changelog.append(current_entry)

    return changelog

===== END OF FILE: changelog_parser.py =====



===== START OF FILE: check_env.py =====

import os
import subprocess
from pathlib import Path

BASE_DIR = Path(__file__).parent.resolve()

REQUIRED_FILES = {
    "SQLite DB": BASE_DIR / "job_tracker.db",
    "patterns.json": BASE_DIR / "patterns.json",
    "companies.json": BASE_DIR / "companies.json",
    "message_classifier.pkl": BASE_DIR / "model" / "message_classifier.pkl",
    "message_vectorizer.pkl": BASE_DIR / "model" / "message_vectorizer.pkl",
    "message_label_encoder.pkl": BASE_DIR / "model" / "message_label_encoder.pkl",
}

OPTIONAL_FILES = {
    "labeled_subjects.csv": BASE_DIR / "data" / "labeled_subjects.csv",
    "alias_candidates.json": BASE_DIR / "alias_candidates.json",
    "company_aliases.json": BASE_DIR / "company_aliases.json",
}

def check_file(path, label, required=True):
    if path.exists():
        print(f"‚úÖ {label}: Found at {path}")
    else:
        status = "‚ùå MISSING" if required else "‚ö†Ô∏è Optional but missing"
        print(f"{status}: {label} ({path})")

def check_django_migrations():
    print("\nüß± Django Migrations:")
    try:
        result = subprocess.run(
            ["python", "manage.py", "showmigrations", "--plan"],
            capture_output=True,
            text=True,
            check=True
        )
        print("‚úÖ Migration plan retrieved.")
        print(result.stdout)
    except Exception as e:
        print(f"‚ùå Failed to check migrations: {e}")

def check_oauth_credentials():
    print("\nüîê OAuth Credentials:")
    token_path = BASE_DIR / "token.json"
    creds_path = BASE_DIR / "credentials.json"
    check_file(token_path, "token.json", required=False)
    check_file(creds_path, "credentials.json", required=False)

def check_directory_permissions():
    print("\nüìÇ Directory Permissions:")
    writable_dirs = ["model", "data", "."]
    for d in writable_dirs:
        path = BASE_DIR / d
        if os.access(path, os.W_OK):
            print(f"‚úÖ Writable: {path}")
        else:
            print(f"‚ùå Not writable: {path}")

def check_git_status():
    print("\nüîß Git Status:")
    try:
        branch = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            check=True
        ).stdout.strip()
        commit = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            check=True
        ).stdout.strip()
        print(f"‚úÖ Git branch: {branch}")
        print(f"‚úÖ Latest commit: {commit}")
    except Exception as e:
        print(f"‚ö†Ô∏è Git status unavailable: {e}")

def main():
    print("üîç Checking environment readiness...\n")

    print("üì¶ Required files:")
    for label, path in REQUIRED_FILES.items():
        check_file(path, label, required=True)

    print("\nüìÅ Optional files:")
    for label, path in OPTIONAL_FILES.items():
        check_file(path, label, required=False)

    check_django_migrations()
    check_oauth_credentials()
    check_directory_permissions()
    check_git_status()
    check_detect_secrets()

    print("\n‚úÖ Done. Review any ‚ùå or ‚ö†Ô∏è items above to complete setup.")

def check_detect_secrets():
    print("\nüîí Secret Scanning (detect-secrets):")
    baseline_path = BASE_DIR / ".secrets.baseline"

    if baseline_path.exists():
        print(f"‚úÖ .secrets.baseline found at {baseline_path}")
        try:
            result = subprocess.run(
                ["detect-secrets", "scan", "--baseline", str(baseline_path)],
                capture_output=True,
                text=True,
                check=True
            )
            print("‚úÖ Scan completed. No new secrets detected.")
        except subprocess.CalledProcessError as e:
            print("‚ùå Secret scan failed or secrets detected.")
            print(e.stdout or e.stderr)
    else:
        print("‚ö†Ô∏è .secrets.baseline not found. Generating new baseline...")
        try:
            result = subprocess.run(
                ["detect-secrets", "scan", "--all-files", "--output", str(baseline_path)],
                capture_output=True,
                text=True,
                check=True
            )
            print("‚úÖ New .secrets.baseline created.")
        except subprocess.CalledProcessError as e:
            print("‚ùå Failed to create .secrets.baseline.")
            print(e.stdout or e.stderr)
            
if __name__ == "__main__":
    main()

===== END OF FILE: check_env.py =====



===== START OF FILE: companies.json =====

//  companies.json
    {
  "ats_domains" : [
    "myworkday.com",
    "jobvite.com",
    "greenhouse-mail.io",
    "smartrecruiters.com",
    "pageuppeople.com",
    "icims.com"
  ],
"known": [
    "Claroty",
    "Red Hat",
    "Google",
    "Meta",
    "Amazon",
    "Microsoft",
    "Stripe",
    "PartnerForces",
    "Apple"
  ],
 "domain_to_company": {
    "ecs-federal.com": "ECS",
    "partnerforces.com": "PartnerForce",
    "claroty.com": "Claroty",
    "rapid7.com": "Rapid7",
    "simventions.com": "SimVentions",
    "caci.com": "CACI", 
    "cloudflare.com": "Cloudflare",
    "armis.com": "Armis",
    "venturegloballng.com": "Venture Global LNG",
    "nm.com": "Northwestern Mutual",
    "me.com": "Apple",
    "gmail.com": "Google",
    "delta.org": "Delta Dental",
    "comeet-notifications.com": "Claroty",
    "talent.icims.com": "Simventions",
    "pageuppeople.com": "Virginia Tech",
    "greenhouse-mail.io": "Greenhouse",
    "email.figure.com": "Figure"
  },
  "aliases": {
    "CVS Health has been received": "CVS Health",
    "Complete Your SimVentions": "SimVentions",
    "Farr Shepherd & Kelly Shaw about Available Position": "Farr Shepherd",
    "CrowdStrike Job": "CrowdStrike",
    "Follow-up on Dragos": "Dragos",
    "Security Architect at Tyler Technologies": "Tyler Technologies",
    "Dragos Application": "Dragos",
    "Google Application": "Google",
    "SEI Application Update": "SEI",
    "donotreply_Leidos_HR": "Leidos",
    "DeeDee Price from Palo Alto Networks": "Palo Alto Networks",
    "PartnerForce": "Partner Forces"
  }
}

===== END OF FILE: companies.json =====



===== START OF FILE: dashboard/__init__.py =====


===== END OF FILE: dashboard/__init__.py =====



===== START OF FILE: dashboard/asgi.py =====

"""
ASGI config for dashboard project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'dashboard.settings')

application = get_asgi_application()

===== END OF FILE: dashboard/asgi.py =====



===== START OF FILE: dashboard/settings.py =====

"""
Django settings for dashboard project.

Generated by 'django-admin startproject' using Django 5.2.6.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.2/ref/settings/
"""

from pathlib import Path

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'django-insecure-s0z-j1e2!!eu6@&g5h1pn##w7nraq#^4b^i23nmu*oz+d5a(gh'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []

LOGIN_URL = '/accounts/login/'
LOGIN_REDIRECT_URL = '/'

# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'tracker.apps.TrackerConfig'
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'dashboard.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [BASE_DIR / "templates"],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'dashboard.wsgi.application'


# Database
# https://docs.djangoproject.com/en/5.2/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'job_tracker.db',
    }
}


# Password validation
# https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/5.2/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'America/New_York'

USE_I18N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/5.2/howto/static-files/

STATIC_URL = 'static/'
STATIC_ROOT = BASE_DIR / "staticfiles"


# Default primary key field type
# https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
        },
    },
    'root': {
        'handlers': ['console'],
        'level': 'INFO',
    },
}

===== END OF FILE: dashboard/settings.py =====



===== START OF FILE: dashboard/urls.py =====

# dashboard/urls.py
"""
URL configuration for dashboard project.

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/5.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
"""
from tracker.admin import custom_admin_site
from django.urls import path, include
# urls.py
from django.contrib.auth import views as auth_views

urlpatterns = [
    path('accounts/login/', auth_views.LoginView.as_view(), name='login'),
    path("admin/", custom_admin_site.urls),
    path('', include('tracker.urls')),
]

===== END OF FILE: dashboard/urls.py =====



===== START OF FILE: dashboard/wsgi.py =====

"""
WSGI config for dashboard project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'dashboard.settings')

application = get_wsgi_application()

===== END OF FILE: dashboard/wsgi.py =====



===== START OF FILE: db.py =====

# db.py
#
import sqlite3
import json
import pandas as pd
from datetime import datetime, date
import time
import re
import os
import sys
from pathlib import Path

DB_PATH = os.getenv("JOB_TRACKER_DB", "job_tracker.db")
PATTERNS_PATH = Path(__file__).parent / "patterns.json"
COMPANIES_PATH = Path(__file__).parent / "companies.json"
SCHEMA_VERSION = '2.0.0'

# --- Apply is_valid_company() filter globally ---
def is_valid_company(name):
    name = name.strip()
    if not name or len(name.split()) > 8:
        return False
    if re.search(r'\b(application|interview|position|role|job|resume|thank you|your)\b', name, re.I):
        return False
    return True

def get_db_connection(retries=3, delay=2):
    """
    Safely open a SQLite connection with retry logic for locked databases.
    Exits gracefully if the database is missing or inaccessible.
    """
    for attempt in range(retries):
        try:
            conn = sqlite3.connect(DB_PATH)
            return conn
        except sqlite3.OperationalError as e:
            error_msg = str(e).lower()
            if "database is locked" in error_msg:
                print(f"‚ö†Ô∏è Attempt {attempt+1}: Database is locked. Retrying in {delay} seconds...")
                time.sleep(delay)
            elif "unable to open database file" in error_msg:
                print("‚ùå Database file not found or inaccessible. Check DB_PATH and permissions.")
                sys.exit(1)
            else:
                print(f"‚ùå Unexpected SQLite error: {e}")
                sys.exit(1)
        except Exception as e:
            print(f"‚ùå Unexpected error while opening database: {e}")
            sys.exit(1)

    print("‚ùå Failed to acquire database lock after multiple attempts.")
    sys.exit(1)

def init_db():
    conn = get_db_connection()

    c = conn.cursor()

    # Main applications table
    c.execute('''
        CREATE TABLE IF NOT EXISTS applications (
            thread_id TEXT PRIMARY KEY,
            company TEXT,
            predicted_company TEXT,
            job_title TEXT,
            job_id TEXT,
            first_sent TEXT,
            response_date TEXT,
            follow_up_dates TEXT,
            rejection_date TEXT,
            interview_date TEXT,
            status TEXT,
            labels TEXT,
            subject TEXT,
            sender TEXT,
            sender_domain TEXT,
            company_job_index TEXT,
            last_updated TEXT
        )
    ''')

    # Indexes for performance
    c.execute('CREATE INDEX IF NOT EXISTS idx_status ON applications(status)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_company ON applications(company)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_company_job_index ON applications(company_job_index)')
    
    # Meta table for schema versioning
    c.execute('''
    CREATE TABLE IF NOT EXISTS meta (
        key TEXT PRIMARY KEY,
        value TEXT
    )
    ''')
    c.execute('INSERT OR REPLACE INTO meta (key, value) VALUES (?, ?)', ('schema_version', SCHEMA_VERSION))

    # Optional: normalized follow-up tracking (modular rollout)
    c.execute('''
        CREATE TABLE IF NOT EXISTS follow_ups (
            thread_id TEXT,
            follow_up_date TEXT,
            FOREIGN KEY(thread_id) REFERENCES applications(thread_id)
        )
    ''')
    
    # ML training table for subject + body
    c.execute('''
        CREATE TABLE IF NOT EXISTS email_text (
            message_id TEXT PRIMARY KEY,
            subject TEXT,
            body TEXT
        )
    ''')

    # Company
    c.execute('''
        CREATE TABLE IF NOT EXISTS tracker_company (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            domain TEXT,
            first_contact TEXT NOT NULL,
            last_contact TEXT NOT NULL
        )
    ''')

    # Application
    c.execute('''
        CREATE TABLE IF NOT EXISTS tracker_application (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            thread_id TEXT UNIQUE NOT NULL,
            company_source TEXT,
            company_id INTEGER NOT NULL REFERENCES tracker_company(id) ON DELETE CASCADE,
            job_title TEXT NOT NULL,
            status TEXT NOT NULL,
            sent_date TEXT NOT NULL,
            rejection_date TEXT,
            interview_date TEXT,
            ml_label TEXT,
            ml_confidence REAL,
            reviewed INTEGER DEFAULT 0
        )
    ''')

    # Message
    c.execute('''
        CREATE TABLE IF NOT EXISTS tracker_message (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            company_id INTEGER REFERENCES tracker_company(id) ON DELETE SET NULL,
            sender TEXT NOT NULL,
            subject TEXT NOT NULL,
            body TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            msg_id TEXT UNIQUE NOT NULL,
            thread_id TEXT NOT NULL,
            ml_label TEXT,
            confidence REAL,
            reviewed INTEGER DEFAULT 0
        )
    ''')

    # IgnoredMessage
    c.execute('''
        CREATE TABLE IF NOT EXISTS tracker_ignoredmessage (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            msg_id TEXT UNIQUE NOT NULL,
            subject TEXT NOT NULL,
            body TEXT NOT NULL,
            sender TEXT NOT NULL,
            sender_domain TEXT NOT NULL,
            date TEXT NOT NULL,
            reason TEXT NOT NULL,
            logged_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # IngestionStats
    c.execute('''
        CREATE TABLE IF NOT EXISTS tracker_ingestionstats (
            date TEXT PRIMARY KEY,
            total_fetched INTEGER DEFAULT 0,
            total_inserted INTEGER DEFAULT 0,
            total_ignored INTEGER DEFAULT 0,
            total_skipped INTEGER DEFAULT 0
        )
    ''')

    # Ensure today's row exists
    today = date.today().isoformat()
    c.execute('INSERT OR IGNORE INTO tracker_ingestionstats (date) VALUES (?)', (today,))

    conn.commit()
    conn.close()

def insert_email_text(message_id, subject, body):
    
    conn = get_db_connection()
            
    c = conn.cursor()
    c.execute('''
        INSERT OR REPLACE INTO email_text (message_id, subject, body)
        VALUES (?, ?, ?)
    ''', (message_id, subject, body))
    conn.commit()
    conn.close()

def load_training_data():
    conn = sqlite3.connect(DB_PATH)
    query = '''
        SELECT e.message_id, e.subject, e.body, a.company
        FROM email_text e
        JOIN applications a ON e.message_id = a.thread_id
        WHERE a.company IS NOT NULL AND a.company != ''
    '''
    df = pd.read_sql_query(query, conn)
    conn.close()

    if df.empty:
        raise ValueError("üö´ No training data found in applications/email_text")

    # --- Normalize company names ---
    def normalize_company(name):
        name = name.strip().rstrip(",.")
        match = re.search(r'\b(?:at|@)\s+(.+)$', name, flags=re.IGNORECASE)
        if match:
            name = match.group(1)
        return name.strip()

    df['company'] = df['company'].apply(normalize_company)

    # --- Optional: Apply alias mappings and ignore patterns ---
    if PATTERNS_PATH.exists() and COMPANIES_PATH.exist():
        with open(COMPANIES_PATH, "r", encoding="utf-8") as e:
            patterns = json.load(e)

        alias_map = patterns.get("aliases", {})
        if alias_map:
            print(f"üîÑ Applying {len(alias_map)} company alias mappings from companies.json")
            df['company'] = df['company'].replace(alias_map)

        with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
            patterns = json.load(f)

        ignore_patterns = [p.lower() for p in patterns.get("ignore", [])]
        if ignore_patterns:
            mask_ignore = df['subject'].str.lower().apply(
                lambda subj: any(pat in subj for pat in ignore_patterns)
            )
            df = df[~mask_ignore]

    # --- Remove obvious non-company noise ---
    df = df[df['company'].str.len() > 3]
    df = df[~df['company'].str.contains(
        r'thank you|evaluate|job|sr|intelligence|lead engineer',
        case=False
    )]

    # --- Remove likely personal names ---
    personal_name_regex = re.compile(r'^[A-Z][a-z]+ [A-Z][a-z]+$')
    corp_suffix_regex = re.compile(
        r'(Inc|LLC|Ltd|Technologies|Systems|Group|Corp|Company|Co\.|PLC)$', re.I
    )

    def is_personal_name(name):
        return bool(personal_name_regex.match(name)) and not corp_suffix_regex.search(name)

    df = df[~df['company'].apply(is_personal_name)]

    # --- Global validity filter ---
    invalids = df[~df['company'].apply(is_valid_company)]
    if not invalids.empty:
        print("üßπ Dropped invalid company labels (global filter):")
        print(invalids['company'].value_counts())

    df = df[df['company'].apply(is_valid_company)]

    # --- Final safeguard ---
    unique_companies = df['company'].nunique()
    if unique_companies < 2:
        raise ValueError(
            f"üö´ Not enough unique companies after cleaning ({unique_companies} found) ‚Äî aborting training."
        )

    return df

def insert_or_update_application(data):
    conn = get_db_connection()
    c = conn.cursor()

    # Add last_updated timestamp
    data['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Normalize follow_up_dates and labels to strings
    follow_up_raw = data.get('follow_up_dates', [])
    data['follow_up_dates'] = ", ".join(follow_up_raw) if isinstance(follow_up_raw, list) else str(follow_up_raw)

    labels_raw = data.get('labels', [])
    data['labels'] = ", ".join(labels_raw) if isinstance(labels_raw, list) else str(labels_raw)

    c.execute('''
        INSERT OR REPLACE INTO applications (
            thread_id, company, predicted_company, job_title, job_id, first_sent,
            response_date, follow_up_dates, rejection_date,
            interview_date, status, labels, subject, sender, sender_domain,
            company_job_index, last_updated
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        data['thread_id'],
        data.get('company', ''),
        data.get('predicted_company', ''),
        data.get('job_title', ''),
        data.get('job_id', ''),
        data.get('first_sent', ''),
        data.get('response_date', ''),
        data.get('follow_up_dates', ''),
        data.get('rejection_date', ''),
        data.get('interview_date', ''),
        data.get('status', ''),
        data.get('labels', ''),
        data.get('subject', ''),
        data.get('sender', ''),
        data.get('sender_domain', ''),
        data.get('company_job_index', ''),
        data['last_updated']
    ))

    conn.commit()
    conn.close()

===== END OF FILE: db.py =====



===== START OF FILE: db_helpers.py =====

# db_helpers.py
import sqlite3
from datetime import datetime, timedelta
import re
from pathlib import Path

def build_company_job_index(company, job_title, job_id):
    import re
    def normalize(text):
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip().lower())
    return f"{normalize(company)}::{normalize(job_title)}::{normalize(job_id)}"

def get_application_by_sender(sender_email, sender_domain):
    """
    Look up the most recent application for this sender email or domain.
    Returns a dict with application fields, or None if not found.
    """
    conn = sqlite3.connect("job_tracker.db")
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()

    # First try exact email match
    cur.execute("""
        SELECT * FROM applications
        WHERE sender = ?
        ORDER BY first_sent DESC
        LIMIT 1
    """, (sender_email,))
    row = cur.fetchone()

    # If no exact match, try domain match
    if not row:
        cur.execute("""
            SELECT * FROM applications
            WHERE sender_domain = ?
            ORDER BY first_sent DESC
            LIMIT 1
        """, (sender_domain,))
        row = cur.fetchone()

    conn.close()
    return dict(row) if row else None

===== END OF FILE: db_helpers.py =====



===== START OF FILE: domain_to_company.json =====

{
    "verizon.com": "Verizon",
    "google.com": "Google",
    "mitre.org": "MITRE",
    "tiktok.com": "TikTok",
    "cisco.com": "Cisco",
    "simventions.com": "SimVentions",
    "amazon.com": "Amazon",
    "amazon.jobs": "Amazon",
    "amazon.co.uk": "Amazon",
    "amazon.in": "Amazon",
    "facebook.com": "Meta",
    "fb.com": "Meta",
    "careers.simventions.com": "Simventions",
    "partnerforces.com": "Partner Forces",
    "uicalaska.com": "Bowhead",
    "peraton.com": "Peraton"
}

===== END OF FILE: domain_to_company.json =====



===== START OF FILE: get_imports.py =====

import subprocess
import os
import re

def get_repo_root():
    return subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode().strip()

def extract_imports():
    repo_root = get_repo_root()
    os.chdir(repo_root)

    # Get committed .py files from latest commit
    commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()
    files = subprocess.check_output(['git', 'show', '--pretty=', '--name-only', commit_hash]).decode().splitlines()
    py_files = [f for f in files if f.endswith('.py')]

    imports = set()
    pattern = re.compile(r'^\s*(import|from)\s+([a-zA-Z0-9_\.]+)')

    for file in py_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                for line in f:
                    match = pattern.match(line)
                    if match:
                        module = match.group(2).split('.')[0]
                        imports.add(module)
        except FileNotFoundError:
            continue

    print("üì¶ Modules detected in committed .py files:")
    for module in sorted(imports):
        print(f"- {module}")

    return imports

extract_imports()

===== END OF FILE: get_imports.py =====



===== START OF FILE: gmail_auth.py =====

from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
import os
import pickle

SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']

def get_gmail_service():
    creds = None
    token_path = 'token.pickle'

    if os.path.exists(token_path):
        with open(token_path, 'rb') as token:
            creds = pickle.load(token)

    if not creds or not creds.valid:
        flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
        creds = flow.run_local_server(port=0)
        with open(token_path, 'wb') as token:
            pickle.dump(creds, token)

    service = build('gmail', 'v1', credentials=creds)
    return service

===== END OF FILE: gmail_auth.py =====



===== START OF FILE: ignore_tester.py =====

import json
import re
from parser import normalize_text, should_ignore

# Load ignore phrases from patterns.json
with open('patterns.json') as f:
    PATTERNS = json.load(f)
IGNORE_PHRASES = [normalize_text(p) for p in PATTERNS.get('ignore', [])]

# Sample subjects to test
TEST_SUBJECTS = [
    "üõéÔ∏è Your job alert for: Cybersecurity (IT/OT) Senior Leader, United States",
    "New job opportunities at Viasat!",
    "Your Top Job matches for ‚ÄúSr. Engineer, Product Abuse - Product Security (Remote)‚Äù",
    "Saved job is expiring soon!",
    "Looking for a new job? Check out these roles",
    "Interview Invitation from Acme Corp",
    "Application received for Security Analyst"
]

def test_ignore(subject):
    result = should_ignore(subject, "")
    normalized = normalize_text(subject)
    matched_phrase = next((p for p in IGNORE_PHRASES if p in normalized), None)
    print(f"Subject: {subject}")
    print(f"‚Üí Ignored: {result}")
    if matched_phrase:
        print(f"‚Üí Matched phrase: {matched_phrase}")
    else:
        print("‚Üí No match found")
    print("-" * 60)

if __name__ == "__main__":
    for subj in TEST_SUBJECTS:
        test_ignore(subj)

===== END OF FILE: ignore_tester.py =====



===== START OF FILE: known_companies.txt =====

Google
Microsoft
Splunk
Target
ECS
Verizon
Tyler Technologies
VA Tech
Leidos
MITRE
Peraton
SentinelOne
Corelight
Eclypsium
Dragos
Banduri
Patner Forces
SEI
Crowdstrike
Mandiant
Recorded Future
Rapid7
Parsons
MANTECH
Claroty
Nozomi
Simventions

===== END OF FILE: known_companies.txt =====



===== START OF FILE: label_companies.py =====

# label_companies.py
# Script to label companies for job applications with missing or generic company names.


import sqlite3
import csv
from pathlib import Path

DB_PATH = "job_tracker.db"
TRAINING_EXPORT = "labeled_companies.csv"

def label_companies():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    cur.execute("""
        SELECT a.thread_id, a.subject, e.body, e.sender
        FROM applications a
        JOIN email_text e ON a.thread_id = e.msg_id
        WHERE a.company IS NULL
           OR a.company = ''
           OR LOWER(a.company) = 'intel'
    """)
    rows = cur.fetchall()

    if not rows:
        print("‚úÖ No unlabeled companies found.")
        return

    # Prepare CSV export
    export_exists = Path(TRAINING_EXPORT).exists()
    with open(TRAINING_EXPORT, "a", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        if not export_exists:
            writer.writerow(["thread_id", "subject", "sender", "company"])

        for thread_id, subject, body, sender in rows:
            print("\n----------------------------------------")
            print(f"Subject: {subject}")
            print(f"Sender: {sender}")
            label = input("Enter company (or leave blank to skip): ").strip()
            if label:
                # Update DB
                cur.execute("""
                    UPDATE applications
                    SET company = ?
                    WHERE thread_id = ?
                """, (label, thread_id))
                conn.commit()

                # Append to CSV for ML training
                writer.writerow([thread_id, subject, sender, label])

    conn.close()
    print("‚úÖ Labeling complete.")

if __name__ == "__main__":
    label_companies()

===== END OF FILE: label_companies.py =====



===== START OF FILE: main.py =====

# main.py

import os
import sys
from datetime import datetime
import django
from django.utils.timezone import localdate
from django.contrib.auth import get_user_model
import argparse

# --- Initialize Gmail and DB ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "dashboard.settings")
django.setup()

from tracker.models import IngestionStats
from gmail_auth import get_gmail_service
from parser import ingest_message, PATTERNS
from db import init_db

# --- Parse CLI flags ---
parser = argparse.ArgumentParser()
parser.add_argument("--limit-msg", help="Only ingest this single msg_id")
args = parser.parse_args()

# --- Initialize Gmail ---
service = get_gmail_service()

# --- Optional single-message ingest ---
if args.limit_msg:
    try:
        result = ingest_message(service, args.limit_msg)
        print(f"[single] {result or '‚ùì Unknown result'} for {args.limit_msg}")
    except Exception as e:
        print(f"[single] ‚ùå Failed to ingest {args.limit_msg}: {e}")
    sys.exit(0)

# --- Full sync ---
try:
    init_db()
except Exception as e:
    print(f"‚ùå Failed to initialize database: {e}")
    sys.exit(1)

def prompt_superuser():
    User = get_user_model()
    if not User.objects.filter(is_superuser=True).exists():
        print("üîê No superuser found. Create one now:")
        os.system("python manage.py createsuperuser")

print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Database initialized.")

profile = service.users().getProfile(userId="me").execute()
print(f"Connected to Gmail account: {profile['emailAddress']}")


def build_query():
    """Build Gmail search query from static subject terms and patterns.json body terms."""
    subject_terms = ["job", "application", "resume", "interview", "position"]
    subject_query = " OR ".join(subject_terms)

    body_terms = (
        (
            PATTERNS.get("application", [])
            + PATTERNS.get("interview", [])
            + PATTERNS.get("follow_up", [])
            + PATTERNS.get("rejection", [])
        )
        if PATTERNS
        else []
    )

    body_query = " OR ".join(f'"{term}"' for term in body_terms) if body_terms else ""

    if body_query:
        return f"(subject:({subject_query}) OR body:({body_query})) newer_than:365d"
    else:
        return f"(subject:({subject_query})) newer_than:365d"


def fetch_all_messages(service, query):
    """Fetch all Gmail messages matching the query."""
    messages = []
    next_page_token = None

    while True:
        response = (
            service.users()
            .messages()
            .list(userId="me", q=query, maxResults=100, pageToken=next_page_token)
            .execute()
        )

        messages.extend(response.get("messages", []))
        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            break

    return messages

def print_final_stats():
    stats = IngestionStats.objects.get(date=localdate())
    stats.refresh_from_db()
    print(
        f"Summary: {stats.total_inserted} inserted, "
        f"{stats.total_ignored} ignored, "
        f"{stats.total_skipped} skipped"
    )


def sync_messages():
       
    """Run a full sync of Gmail messages into the database."""
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Starting sync...")
    query = build_query()
    print(f"Using Gmail query: {query}")

    messages = fetch_all_messages(service, query)
    print(f"Found {len(messages)} messages matching query.")
   
    
    new_messages = []
    for idx, msg in enumerate(messages, start=1):
        msg_id = msg["id"]
        try:
            result = ingest_message(service, msg_id)
            if result == "ignored":
                print(f"[{idx}/{len(messages)}] ‚ö†Ô∏è Ignored {msg_id}")
            elif result == "inserted":
                new_messages.append(msg_id)
                print(f"[{idx}/{len(messages)}] ‚úÖ Inserted {msg_id}")
            elif result == "skipped":
                print(f"[{idx}/{len(messages)}] ‚è© Skipped duplicate {msg_id}")
            else:
                print(f"[{idx}/{len(messages)}] ‚ùì Unknown result for {msg_id}")
        except Exception as e:
            print(f"[{idx}/{len(messages)}] ‚ùå Failed to ingest {msg_id}: {e}")
            continue
        
    if not new_messages:
        print("No new messages to process. Exiting.")
        return
    
    today = localdate()
    print_final_stats()
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Sync complete.")

    try:
        stats = IngestionStats.objects.get(date=today)
    except IngestionStats.DoesNotExist:
        stats = None

    if stats:
        print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")
    else:
        print("‚ö†Ô∏è No IngestionStats record found for today.")


    return stats

if __name__ == "__main__":
    init_db()
    stats=sync_messages()
    prompt_superuser()

    

===== END OF FILE: main.py =====



===== START OF FILE: main.py.txt =====

# main.py

import os
import sys
from datetime import datetime
import django
from django.utils.timezone import localdate
from django.contrib.auth import get_user_model

import argparse

# --- Initialize Gmail and DB ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "dashboard.settings")
django.setup()

from tracker.models import IngestionStats
from gmail_auth import get_gmail_service
from parser import ingest_message, PATTERNS
from db import init_db

# --- Parse CLI flags ---
parser = argparse.ArgumentParser()
parser.add_argument("--limit-msg", help="Only ingest this single msg_id")
args = parser.parse_args()

# --- Initialize Gmail ---
service = get_gmail_service()

# --- Optional single-message ingest ---
if args.limit_msg:
    try:
        result = ingest_message(service, args.limit_msg)
        print(f"[single] {result or '‚ùì Unknown result'} for {args.limit_msg}")
    except Exception as e:
        print(f"[single] ‚ùå Failed to ingest {args.limit_msg}: {e}")
    sys.exit(0)

# --- Full sync ---
try:
    init_db()
except Exception as e:
    print(f"‚ùå Failed to initialize database: {e}")
    sys.exit(1)

def prompt_superuser():
    User = get_user_model()
    if not User.objects.filter(is_superuser=True).exists():
        print("üîê No superuser found. Create one now:")
        os.system("python manage.py createsuperuser")

print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Database initialized.")

profile = service.users().getProfile(userId="me").execute()
print(f"Connected to Gmail account: {profile['emailAddress']}")


def build_query():
    """Build Gmail search query from static subject terms and patterns.json body terms."""
    subject_terms = ["job", "application", "resume", "interview", "position"]
    subject_query = " OR ".join(subject_terms)

    body_terms = (
        (
            PATTERNS.get("application", [])
            + PATTERNS.get("interview", [])
            + PATTERNS.get("follow_up", [])
            + PATTERNS.get("rejection", [])
        )
        if PATTERNS
        else []
    )

    body_query = " OR ".join(f'"{term}"' for term in body_terms) if body_terms else ""

    if body_query:
        return f"(subject:({subject_query}) OR body:({body_query})) newer_than:365d"
    else:
        return f"(subject:({subject_query})) newer_than:365d"


def fetch_all_messages(service, query):
    """Fetch all Gmail messages matching the query."""
    messages = []
    next_page_token = None

    while True:
        response = (
            service.users()
            .messages()
            .list(userId="me", q=query, maxResults=100, pageToken=next_page_token)
            .execute()
        )

        messages.extend(response.get("messages", []))
        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            break

    return messages


def sync_messages():
       
    """Run a full sync of Gmail messages into the database."""
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Starting sync...")
    query = build_query()
    print(f"Using Gmail query: {query}")

    messages = fetch_all_messages(service, query)
    print(f"Found {len(messages)} messages matching query.")
   
    
    new_messages = []
    for idx, msg in enumerate(messages, start=1):
        msg_id = msg["id"]
        try:
            result = ingest_message(service, msg_id)
            if result == "ignored":
                print(f"[{idx}/{len(messages)}] ‚ö†Ô∏è Ignored {msg_id}")
            elif result == "inserted":
                new_messages.append(msg_id)
                print(f"[{idx}/{len(messages)}] ‚úÖ Inserted {msg_id}")
            elif result == "skipped":
                print(f"[{idx}/{len(messages)}] ‚è© Skipped duplicate {msg_id}")
            else:
                print(f"[{idx}/{len(messages)}] ‚ùì Unknown result for {msg_id}")
        except Exception as e:
            print(f"[{idx}/{len(messages)}] ‚ùå Failed to ingest {msg_id}: {e}")
            continue
        
    if not new_messages:
        print("No new messages to process. Exiting.")
        return
    
    today = localdate()
    stats, _ = IngestionStats.objects.get_or_create(date=today)
    print(
        f"Summary: {stats.total_inserted} inserted, "
        f"{stats.total_ignored} ignored, "
        f"{stats.total_skipped} skipped"
    )

    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] Sync complete.")


if __name__ == "__main__":
    init_db()
    sync_messages()
    prompt_superuser()
    

===== END OF FILE: main.py.txt =====



===== START OF FILE: manage.py =====

#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys


def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'dashboard.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == '__main__':
    main()

===== END OF FILE: manage.py =====



===== START OF FILE: migrations/2025-09-09_add_company_job_index.txt =====

# 2025-09-09_add_company_job_index.py

import sqlite3
import re
import sys
from pathlib import Path

# Add the project root to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from db_helpers import build_company_job_index


DB_PATH = "job_tracker.db"

def migrate():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    # 1. Add column if it doesn't exist
    cur.execute("""
        PRAGMA table_info(applications)
    """)
    cols = [row[1] for row in cur.fetchall()]
    if "company_job_index" not in cols:
        cur.execute("ALTER TABLE applications ADD COLUMN company_job_index TEXT")

    # 2. Populate column for existing rows
    cur.execute("SELECT rowid, company, job_title, job_id FROM applications")
    rows = cur.fetchall()
    for rowid, company, job_title, job_id in rows:
        idx = build_company_job_index(company, job_title, job_id)
        cur.execute("""
            UPDATE applications
            SET company_job_index = ?
            WHERE rowid = ?
        """, (idx, rowid))
    # 3. Create index for fast lookups
    cur.execute("""
        CREATE INDEX IF NOT EXISTS idx_company_job_index
        ON applications(company_job_index)
    """)

    conn.commit()
    conn.close()
    print("‚úÖ Migration complete: company_job_index added and populated.")

if __name__ == "__main__":
    migrate()

===== END OF FILE: migrations/2025-09-09_add_company_job_index.txt =====



===== START OF FILE: migrations/rollback_company_job_index.txt =====


import sqlite3

DB_PATH = "job_tracker.db"

def rollback():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    # Drop index if exists
    cur.execute("DROP INDEX IF EXISTS idx_company_job_index")

    # SQLite can't drop columns easily ‚Äî so we leave the column in place
    # but clear its data if needed
    cur.execute("UPDATE applications SET company_job_index = NULL")

    conn.commit()
    conn.close()
    print("‚ôªÔ∏è Rollback complete: index dropped, column cleared.")

if __name__ == "__main__":
    rollback()

===== END OF FILE: migrations/rollback_company_job_index.txt =====



===== START OF FILE: ml_entity_extraction.py =====

# ml_entity_extraction.py

import spacy

# Load spaCy model (custom or pre-trained)
nlp = spacy.load("en_core_web_sm")

def extract_entities(subject):
    doc = nlp(subject)
    company = ""
    job_title = ""

    for ent in doc.ents:
        if ent.label_ == "ORG":
            company = ent.text
        elif ent.label_ in ["JOB_TITLE", "WORK_OF_ART", "PRODUCT"]:
            job_title = ent.text

    return {
        "company": company,
        "job_title": job_title,
        "location": "",  # Optional: add LOC support
    }

===== END OF FILE: ml_entity_extraction.py =====



===== START OF FILE: ml_prep.py =====

#ml_prep.py
import pandas as pd

import sqlite3


def extract_subject_body(message):
    subject = message.get("subject", "").strip()
    body = message.get("body", "").strip()
    return subject, body

def write_to_sqlite(message_id, subject, body, conn):
    cursor = conn.cursor()
    cursor.execute("""
        INSERT OR REPLACE INTO email_text (message_id, subject, body)
        VALUES (?, ?, ?)
    """, (message_id, subject, body))
    conn.commit()
    
def ensure_email_text_table(conn):
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS email_text (
            message_id TEXT PRIMARY KEY,
            subject TEXT,
            body TEXT
        )
    """)
    conn.commit()
    
def load_training_data(db_path):
    conn = sqlite3.connect(db_path)
    query = """
        SELECT e.message_id, e.subject, e.body, a.company
        FROM email_text e
        JOIN applications a ON e.message_id = a.message_id
        WHERE a.company IS NOT NULL AND a.company != ''
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

===== END OF FILE: ml_prep.py =====



===== START OF FILE: ml_subject_classifier.py =====

# ml_subject_classifier.py

import joblib
import os

# --- Paths ---
MODEL_DIR = os.path.join(os.path.dirname(__file__), "model")
MESSAGE_MODEL_PATH = os.path.join(MODEL_DIR, "message_classifier.pkl")
MESSAGE_VECTORIZER_PATH = os.path.join(MODEL_DIR, "message_vectorizer.pkl")
MESSAGE_LABEL_ENCODER_PATH = os.path.join(MODEL_DIR, "message_label_encoder.pkl")

COMPANY_MODEL_PATH = os.path.join(MODEL_DIR, "company_classifier.pkl")
COMPANY_VECTORIZER_PATH = os.path.join(MODEL_DIR, "vectorizer.pkl")
COMPANY_LABEL_ENCODER_PATH = os.path.join(MODEL_DIR, "label_encoder.pkl")

# --- Load whichever model is available ---
if os.path.exists(MESSAGE_MODEL_PATH):
    model = joblib.load(MESSAGE_MODEL_PATH)
    vectorizer = joblib.load(MESSAGE_VECTORIZER_PATH)
    label_encoder = joblib.load(MESSAGE_LABEL_ENCODER_PATH)
    mode = "message"
    print("ü§ñ Loaded message-level classifier.")
elif os.path.exists(COMPANY_MODEL_PATH):
    model = joblib.load(COMPANY_MODEL_PATH)
    vectorizer = joblib.load(COMPANY_VECTORIZER_PATH)
    label_encoder = joblib.load(COMPANY_LABEL_ENCODER_PATH)
    mode = "company"
    print("ü§ñ Loaded company-level classifier.")
else:
    model = None
    vectorizer = None
    label_encoder = None
    mode = None
    print("‚ö†Ô∏è No classifier found. Predictions will be skipped.")

def predict_subject_type(subject, body=""):
    """
    Predict the label for a given subject/body.
    Uses message-level classifier if available, otherwise company-level.
    Returns dict with label, confidence, and ignore flag.
    """
    if not model or not vectorizer or not label_encoder:
        return {"label": "unknown", "confidence": 0.0, "ignore": False}
    
    text = (subject or "") + " " + (body or "")
    X = vectorizer.transform([text])
    pred_encoded = model.predict(X)[0]
    proba = model.predict_proba(X)[0]
    confidence = float(max(proba))
    label = label_encoder.inverse_transform([pred_encoded])[0]

    return {"label": label, "confidence": confidence, "ignore": False}

===== END OF FILE: ml_subject_classifier.py =====



===== START OF FILE: model/last_cleaned_training_data.csv =====

message_id,subject,body,company,text,company_encoded
196f483fbc884dbd,"Splunk Application Update | Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661)","Hi Adrian, Thank you so much for applying to the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661) position at Splunk. We are very impressed with your background, but after much deliberation, we have decided to not move forward with your application at this time. If you applied to multiple positions at Splunk, note that you may receive a separate update for each position you applied to. While we obviously can‚Äôt hire every applicant, we do strive to give every candidate a positive experience and we would love to hear about yours. Would you mind taking this short survey ? It should only take a couple of minutes to complete. Your responses are confidential. Splunk will not know the identity of the respondents. Data will be used in aggregate to inform improvements to our processes. Again, thank you for your interest in Splunk! Please feel free to apply to any future openings and continue to check our Careers Page for employment opportunities . Best wishes, The Splunk Talent Acquisition Team Splunk‚Äôs Career Site Privacy Policy explains how we collect, use, store and share your information when you apply for a position at Splunk. This message is intended only for the personal, confidential, and authorized use of the recipient(s) named above. If you are not that person, you are not authorized to review, use, copy, forward, distribute or otherwise disclose the information contained in the message. -*_____ Email Preferences | Powered by Jobvite *______*",Splunk,"Splunk Application Update | Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661) Hi Adrian, Thank you so much for applying to the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661) position at Splunk. We are very impressed with your background, but after much deliberation, we have decided to not move forward with your application at this time. If you applied to multiple positions at Splunk, note that you may receive a separate update for each position you applied to. While we obviously can‚Äôt hire every applicant, we do strive to give every candidate a positive experience and we would love to hear about yours. Would you mind taking this short survey ? It should only take a couple of minutes to complete. Your responses are confidential. Splunk will not know the identity of the respondents. Data will be used in aggregate to inform improvements to our processes. Again, thank you for your interest in Splunk! Please feel free to apply to any future openings and continue to check our Careers Page for employment opportunities . Best wishes, The Splunk Talent Acquisition Team Splunk‚Äôs Career Site Privacy Policy explains how we collect, use, store and share your information when you apply for a position at Splunk. This message is intended only for the personal, confidential, and authorized use of the recipient(s) named above. If you are not that person, you are not authorized to review, use, copy, forward, distribute or otherwise disclose the information contained in the message. -*_____ Email Preferences | Powered by Jobvite *______*",0
1967ee3ecd7048d1,"Thanks for applying for the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661) position at Splunk!","Hi Adrian, Thank you for your interest in the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (ID: 32661) role at Splunk! We're always looking for top talent and we are thrilled to receive your application. What happens next? We will review submitted resumes and contact those who we've identified as the best fit for the position. Regrettably, we may not be able to personally reach out to everyone that applies and speak about our roles. Please know that if you do not hear from us, we will keep your resume and contact you if we find a role for your skill-set. Please also keep an eye on our Careers page and apply for any new roles you feel you would be a good fit for. In the meantime, to view the status of your application, please go to the following link: Application Status We welcome candidates with disabilities and strive to create an accessible and inclusive experience for all candidates. If you need an accommodation during the application or the recruiting process, please submit a request via this Interview Accommodation/Adjustment Request Form . Again, thank you for applying and be sure to check out splunk.com to read about all the exciting things Splunk is doing! Sincerely, The Splunk Talent Acquisition Team P.S. How was your application process? We want to hear from you! Please share your feedback in this quick, one minute anonymous survey. MORE ABOUT SPLUNK: Explore what life at Splunk is like Read stories from Splunkers See how we're making an impact Sign up for job alerts Splunk‚Äôs Career Site Privacy Policy explains how we collect, use, store and share your information when you apply for a position at Splunk.  This message is intended only for the personal, confidential, and authorized use of the recipient(s) named above. If you are not that person, you are not authorized to review, use, copy, forward, distribute or otherwise disclose the information contained in the message. -*_____ You can reply directly to this message or click here to reply via your Jobvite user account. Email Preferences | Powered by Jobvite *______*",Splunk,"Thanks for applying for the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (32661) position at Splunk! Hi Adrian, Thank you for your interest in the Senior Machine Learning Engineer, AI (FULLY REMOTE-USA) (ID: 32661) role at Splunk! We're always looking for top talent and we are thrilled to receive your application. What happens next? We will review submitted resumes and contact those who we've identified as the best fit for the position. Regrettably, we may not be able to personally reach out to everyone that applies and speak about our roles. Please know that if you do not hear from us, we will keep your resume and contact you if we find a role for your skill-set. Please also keep an eye on our Careers page and apply for any new roles you feel you would be a good fit for. In the meantime, to view the status of your application, please go to the following link: Application Status We welcome candidates with disabilities and strive to create an accessible and inclusive experience for all candidates. If you need an accommodation during the application or the recruiting process, please submit a request via this Interview Accommodation/Adjustment Request Form . Again, thank you for applying and be sure to check out splunk.com to read about all the exciting things Splunk is doing! Sincerely, The Splunk Talent Acquisition Team P.S. How was your application process? We want to hear from you! Please share your feedback in this quick, one minute anonymous survey. MORE ABOUT SPLUNK: Explore what life at Splunk is like Read stories from Splunkers See how we're making an impact Sign up for job alerts Splunk‚Äôs Career Site Privacy Policy explains how we collect, use, store and share your information when you apply for a position at Splunk.  This message is intended only for the personal, confidential, and authorized use of the recipient(s) named above. If you are not that person, you are not authorized to review, use, copy, forward, distribute or otherwise disclose the information contained in the message. -*_____ You can reply directly to this message or click here to reply via your Jobvite user account. Email Preferences | Powered by Jobvite *______*",0

===== END OF FILE: model/last_cleaned_training_data.csv =====



===== START OF FILE: model/model_audit.json =====

{
  "model_version": "1.2.3",
  "trained_on": "2025-10-12",
  "source_count": 842,
  "label_distribution": {
    "job_application": 412,
    "interview_invite": 97,
    "ghosted": 33,
    "noise": 300
  },
  "scan_passed": true,
  "scan_tool": "detect-secrets v1.4.0"
}

===== END OF FILE: model/model_audit.json =====



===== START OF FILE: model/model_info.json =====

{
  "trained_on": "2025-10-13T00:00:46.087744",
  "labels": [
    "head_hunter",
    "interview_invite",
    "job_application",
    "noise",
    "rejection"
  ],
  "num_samples": 181,
  "features": [
    "00",
    "00 pm",
    "000",
    "000 maximum",
    "04",
    "04 00",
    "09",
    "09 30",
    "0bc4cfefa90c",
    "0bc4cfefa90c 98",
    "0nbj7cyguzlhe1",
    "0nbj7cyguzlhe1 trk",
    "10",
    "10 application",
    "10 http",
    "10 option",
    "100",
    "100 remote",
    "101",
    "102",
    "102 b8ca787b_6c91_45ec_8cc7_8f066e74c560",
    "1081191",
    "11",
    "11 company",
    "11 http",
    "12",
    "12 98",
    "12203",
    "12203 united",
    "13",
    "13 98",
    "14",
    "14 years",
    "1438794",
    "15",
    "150",
    "16",
    "1600",
    "1600 amphitheatre",
    "169",
    "169 microsoft",
    "169 peraton",
    "169 simventions",
    "16px",
    "1758351650266",
    "1758351650266 digest",
    "18",
    "1875",
    "1875 explorer",
    "19",
    "1971",
    "1971 western",
    "1998",
    "1itovf",
    "1itovf m7rvwl6t",
    "1itovf mbz9wcvs",
    "1itovf mf8x79m0",
    "1k",
    "1zwnj000",
    "1zwnj000 west",
    "2007",
    "20190",
    "2024",
    "2024 loved",
    "2024 parents",
    "2024 young",
    "2025",
    "2025 09",
    "2025 11",
    "2025 18",
    "2025 cisco",
    "2025 edition",
    "2025 gartner",
    "2025 linkedin",
    "2025 steps",
    "2025 talk",
    "20speed",
    "20speed 20test",
    "20test",
    "2113",
    "2113 http",
    "22",
    "22 2c",
    "22 3a",
    "22 7d",
    "223",
    "223 albany",
    "22oid",
    "22oid 22",
    "22tid",
    "22tid 22",
    "23686eed04124048ba1218e3a18246ca",
    "25",
    "26",
    "26 2025",
    "26182216",
    "266711b938df",
    "266711b938df 406c100fb5cf",
    "289d4ed4299c",
    "289d4ed4299c 98",
    "29",
    "29 2025",
    "2aa5793b4a98",
    "2aa5793b4a98 98",
    "2b",
    "2b 2bj",
    "2b02iqjykvx9fog",
    "2b02iqjykvx9fog 2bnawd",
    "2b14345792113",
    "2b14345792113 http",
    "2bafnlaz",
    "2bafnlaz 2bscb6vcbsvnss1kjuidns9zph0om17ohb4nmehgerfo6fo73atspnoxc6olbmjbtpxfrbg9h9vffwtumzv5bjkks4q3",
    "2bb0fntmiap0",
    "2bb0fntmiap0 2biqgux",
    "2bbvaxkz6otnqbwlwyxuueqcoqb0rhgdixhdyq20dgeh31k9ngf",
    "2bbvaxkz6otnqbwlwyxuueqcoqb0rhgdixhdyq20dgeh31k9ngf 2b02iqjykvx9fog",
    "2bdydm6w1d9rdcvhdssidc",
    "2bdydm6w1d9rdcvhdssidc 2bafnlaz",
    "2bft9s2eudhva8wsq",
    "2bft9s2eudhva8wsq 3d",
    "2biqgux",
    "2biqgux 2fe",
    "2bj",
    "2bj 2fjp5ttk",
    "2bl3ihpz8dktnokqo3ho10qi3sh5fzxteiivngywgwtgwr",
    "2bl3ihpz8dktnokqo3ho10qi3sh5fzxteiivngywgwtgwr 2fhrfmzxhvevh0xoh8lmfrmt1f1spa0u6qvziz",
    "2bnawd",
    "2bnawd 2bdydm6w1d9rdcvhdssidc",
    "2bogyxj1neubi1dfykg75hu5flphkzd1wswuzgtabmumhlnpxd0cvty6pw23e0bokrtwd3iehslf7i8jaajdz44f07uzc38nrvuzw",
    "2bogyxj1neubi1dfykg75hu5flphkzd1wswuzgtabmumhlnpxd0cvty6pw23e0bokrtwd3iehslf7i8jaajdz44f07uzc38nrvuzw 2breb",
    "2breb",
    "2breb 2bb0fntmiap0",
    "2bs",
    "2bscb6vcbsvnss1kjuidns9zph0om17ohb4nmehgerfo6fo73atspnoxc6olbmjbtpxfrbg9h9vffwtumzv5bjkks4q3",
    "2bscb6vcbsvnss1kjuidns9zph0om17ohb4nmehgerfo6fo73atspnoxc6olbmjbtpxfrbg9h9vffwtumzv5bjkks4q3 2bsr20cnkgyted9bwhgercmese3s0bb4usonssauinc6ieaiyfkq9gyjsbw8oewn6qt",
    "2bsr20cnkgyted9bwhgercmese3s0bb4usonssauinc6ieaiyfkq9gyjsbw8oewn6qt",
    "2bsr20cnkgyted9bwhgercmese3s0bb4usonssauinc6ieaiyfkq9gyjsbw8oewn6qt 2fopx",
    "2budvsxz8qa1a",
    "2budvsxz8qa1a 3d",
    "2bw8fqzmxxoyzfjna1cejee2oiugcxdbmkgqo",
    "2bw8fqzmxxoyzfjna1cejee2oiugcxdbmkgqo 2frfpbdtyqx1nbrnrcbcjpzekv4b3",
    "2c",
    "2c 22oid",
    "2f",
    "2f7p4v4qunyq5mcqddp",
    "2f7p4v4qunyq5mcqddp 2bw8fqzmxxoyzfjna1cejee2oiugcxdbmkgqo",
    "2fe",
    "2fe 2bl3ihpz8dktnokqo3ho10qi3sh5fzxteiivngywgwtgwr",
    "2fevifzsbigw",
    "2fevifzsbigw 3d",
    "2fhrfmzxhvevh0xoh8lmfrmt1f1spa0u6qvziz",
    "2fhrfmzxhvevh0xoh8lmfrmt1f1spa0u6qvziz 2fsd",
    "2fjp5ttk",
    "2fjp5ttk 2fevifzsbigw",
    "2flrv3egjxar",
    "2flrv3egjxar 2fwnklf60hku7krz5eauiesn6hdfbwsvnt4cr2se",
    "2fopx",
    "2fopx 2fujgy4ug",
    "2frfpbdtyqx1nbrnrcbcjpzekv4b3",
    "2fsd",
    "2fsd 2fuqkeqgrivz9dvd4sqf4bqwm",
    "2fujgy4ug",
    "2fujgy4ug 2bogyxj1neubi1dfykg75hu5flphkzd1wswuzgtabmumhlnpxd0cvty6pw23e0bokrtwd3iehslf7i8jaajdz44f07uzc38nrvuzw",
    "2fuqkeqgrivz9dvd4sqf4bqwm",
    "2fuqkeqgrivz9dvd4sqf4bqwm 2fvoeueqiwwxzgyo74u4",
    "2fvoeueqiwwxzgyo74u4",
    "2fvoeueqiwwxzgyo74u4 2f7p4v4qunyq5mcqddp",
    "2fwnklf60hku7krz5eauiesn6hdfbwsvnt4cr2se",
    "2fwnklf60hku7krz5eauiesn6hdfbwsvnt4cr2se 2bbvaxkz6otnqbwlwyxuueqcoqb0rhgdixhdyq20dgeh31k9ngf",
    "30",
    "30 minutes",
    "30 null",
    "30 otptoken",
    "30efd3a7e437",
    "30efd3a7e437 98",
    "31",
    "31815206ca036f99280d2a8051df8935",
    "31815206ca036f99280d2a8051df8935 utm_medium",
    "32661",
    "32661 position",
    "33572",
    "35",
    "36",
    "36939f0cc62e",
    "3a",
    "3aemail_email_application_confirmation_with_nba_01",
    "3aemail_email_application_confirmation_with_nba_01 3be",
    "3aemail_email_application_confirmation_with_nba_01 3bejqdd4bwqv",
    "3aemail_email_jobs_job_application_viewed_01",
    "3aemail_email_jobs_job_application_viewed_01 3bnq0z7x",
    "3ali",
    "3ali 3apage",
    "3apage",
    "3apage 3aemail_email_application_confirmation_with_nba_01",
    "3apage 3aemail_email_jobs_job_application_viewed_01",
    "3be",
    "3be 2b",
    "3bejqdd4bwqv",
    "3bejqdd4bwqv 2budvsxz8qa1a",
    "3bnq0z7x",
    "3bnq0z7x 2bft9s2eudhva8wsq",
    "3d",
    "3d 2025",
    "3d 3d",
    "3d applied",
    "3d email",
    "3d figure",
    "3d https",
    "3d lipi",
    "3d midtoken",
    "3d receiving",
    "3d refid",
    "3d senior",
    "3d5bhtkd8wuru1",
    "3d5bhtkd8wuru1 trk",
    "3nihed8zqo4hq1",
    "3nihed8zqo4hq1 trk",
    "40",
    "40 pm",
    "400",
    "400 000",
    "406c100fb5cf",
    "406c100fb5cf 13",
    "40thread",
    "40thread v2",
    "42e8",
    "42e8 bea6",
    "434",
    "434 579",
    "440100e76000",
    "440100e76000 289d4ed4299c",
    "45",
    "456a00ebf92a",
    "456a00ebf92a type",
    "4644",
    "4644 monday",
    "47",
    "47 47",
    "4788",
    "4788 lang",
    "48",
    "4a43",
    "4a43 83b0",
    "4d0a",
    "4d0a 4fc5",
    "4e01a096a9a0",
    "4f15",
    "4f15 92f2",
    "4fc5",
    "4fc5 ba20",
    "56",
    "56 years",
    "579",
    "579 2113",
    "59",
    "59 b8ca787b_6c91_45ec_8cc7_8f066e74c560",
    "59 pm",
    "5pw_o4y8xooyz09efo",
    "67",
    "67 lang",
    "6930815f1308",
    "70",
    "700",
    "700 font",
    "7031755",
    "7031755 hl",
    "732",
    "732 737",
    "737",
    "737 9236",
    "777674",
    "78073def27b8",
    "78073def27b8 0bc4cfefa90c",
    "78073def27b8 2aa5793b4a98",
    "78d064101951",
    "78d064101951 d151eefd0197",
    "7b",
    "7b 22tid",
    "7d",
    "7d meeting",
    "83b0",
    "83b0 36939f0cc62e",
    "8426",
    "8426 4a43",
    "866",
    "866 950",
    "8993e01dcfd3",
    "8993e01dcfd3 30efd3a7e437",
    "8l8tfvd1td83a_ryro9efhiefh74u8o1j8ojkyf_1a6do6ppimw6ieeqyte_leuw0rysvbotw9kbkh7hhnejrihphfherhvjrehplyr6lpcz8d9vnf_r_ou2wilg7sdx7av32lzxfdnuwh2ua9lctc6hp660bgs3x172yjzzk3y5vrlx0os0tptuiwzd8tyyyenqu8ejoeg0jvbgz7b2bpgs5ifq1zwytm0kcv7ywohgh1k",
    "8th",
    "9236",
    "92f2",
    "92f2 9b97f26c1d3a",
    "94043",
    "94085",
    "94085 linkedin",
    "950",
    "950 4644",
    "954",
    "98",
    "98 b8ca787b_6c91_45ec_8cc7_8f066e74c560",
    "98052",
    "98052 usa",
    "99",
    "9b97f26c1d3a",
    "_____",
    "_____ email",
    "_____ reply",
    "______",
    "________________________________",
    "________________________________________________________________________________",
    "________________________________________________________________________________ microsoft",
    "a3aeaf9b7c1a",
    "a3aeaf9b7c1a 12",
    "aaib5ha",
    "aakib",
    "aakib irionline",
    "abdur",
    "abdur rahman",
    "abdur rahman12",
    "abilities",
    "ability",
    "able",
    "able provide",
    "ablink",
    "ablink email",
    "abruptly",
    "absolutely",
    "access",
    "access control",
    "access private",
    "access talk",
    "accessibility",
    "accessibility answer",
    "accessible",
    "accommodation",
    "accommodation request",
    "accommodations",
    "account",
    "account email",
    "accountability",
    "accurate",
    "accurate extensive",
    "accused",
    "accusing",
    "achieve",
    "acquisition",
    "acquisition business",
    "acquisition https",
    "acquisition team",
    "action",
    "action center",
    "action time",
    "active",
    "active know",
    "activities",
    "actual",
    "actually",
    "actually happening",
    "add",
    "add cloudflare",
    "add donotreply",
    "adding",
    "adding notification",
    "additional",
    "additional draw",
    "address",
    "address book",
    "address unattended",
    "administration",
    "administrator",
    "admitting",
    "adopts",
    "adopts joe",
    "adrian",
    "adrian application",
    "adrian electronically",
    "adrian kelly",
    "adrian received",
    "adrian shaw",
    "adrian substack",
    "adrian thank",
    "adrian thanks",
    "adrianshaw_83379",
    "adrianshaw_83379 source",
    "advanced",
    "advanced expertise",
    "adversarial",
    "adversarial ai",
    "advisor",
    "advisory",
    "advisory services",
    "affiliates",
    "affiliates jobs",
    "age",
    "agency",
    "agenda",
    "ah",
    "ah 23686eed04124048ba1218e3a18246ca",
    "ai",
    "ai data",
    "ai fully",
    "ai interview",
    "ai ml",
    "ai red",
    "ai researcher",
    "ai using",
    "aint",
    "aint easy",
    "air",
    "ak",
    "aka",
    "aka ms",
    "albany",
    "albany new",
    "alert",
    "alert matches",
    "alert page",
    "alerts",
    "alexis",
    "alexis claroty",
    "alexis todero",
    "align",
    "align current",
    "aligned",
    "alignment",
    "alto",
    "alto networks",
    "alumni",
    "alumni view",
    "amazon",
    "amazon gift",
    "america",
    "american",
    "american systems",
    "amp",
    "amp amp",
    "amp cybersecurity",
    "amp mathematics",
    "amphitheatre",
    "amphitheatre parkway",
    "analysis",
    "analysis intersection",
    "analysis management",
    "analyst",
    "analyst position",
    "analyst progression",
    "analyst responsible",
    "analysts",
    "analytical",
    "analytical skills",
    "analytics",
    "analytics amp",
    "andres",
    "andres vourakis",
    "andrew",
    "andrew enter",
    "aniao5o",
    "announced",
    "answer",
    "answer 4788",
    "answer 67",
    "answer 7031755",
    "answered",
    "anti",
    "anti malware",
    "anti money",
    "anti phishing",
    "anti virus",
    "antisyphon",
    "antisyphon training",
    "antisyphontraining",
    "antisyphontraining com",
    "antivirus",
    "apollo",
    "apollo beach",
    "apologies",
    "app",
    "app hosting",
    "app2",
    "app2 greenhouse",
    "app7",
    "app7 greenhouse",
    "appliance",
    "applicability",
    "applicability ot",
    "applicant",
    "applicants",
    "applicat",
    "applicat https",
    "application",
    "application 29",
    "application adrian",
    "application check",
    "application confirmation",
    "application contact",
    "application course",
    "application dear",
    "application directly",
    "application excited",
    "application following",
    "application google",
    "application hi",
    "application https",
    "application icf",
    "application incomplete",
    "application job",
    "application mantech",
    "application order",
    "application partner",
    "application process",
    "application received",
    "application recorded",
    "application role",
    "application security",
    "application senior",
    "application sent",
    "application sentinelone",
    "application separately",
    "application sr",
    "application status",
    "application successfully",
    "application thank",
    "application update",
    "applications",
    "applications dashboard",
    "applied",
    "applied archived",
    "applied leidos",
    "applied positions",
    "apply",
    "apply job",
    "apply position",
    "apply roles",
    "apply tyler",
    "applying",
    "applying cloudflare",
    "applying job",
    "applying lead",
    "applying role",
    "applying senior",
    "appointed",
    "appointment",
    "appointment scheduled",
    "appreciate",
    "appreciate opportunity",
    "appreciate recorded",
    "appreciate time",
    "appreciate wish",
    "approach",
    "approach option",
    "appropriately",
    "apps",
    "aqfg4l_y5e50qw",
    "aqfg4l_y5e50qw midsig",
    "arc",
    "arc interview",
    "architect",
    "architect tyler",
    "architecture",
    "archived",
    "archived state",
    "area",
    "areas",
    "areas information",
    "aren",
    "arise",
    "arise especially",
    "armis",
    "article",
    "artificial",
    "asia",
    "ask",
    "ask patience",
    "asked",
    "aspx",
    "assembly",
    "assess",
    "assessment",
    "assessment help",
    "assessment span",
    "asset",
    "assets",
    "assistant",
    "assistant associate",
    "assistant professor",
    "associate",
    "associate degree",
    "associate professor",
    "associated",
    "associated data",
    "assure",
    "assure receive",
    "atleast",
    "ats",
    "attach",
    "attach resume",
    "attachments",
    "attack",
    "audits",
    "austin",
    "authentication",
    "authentication protocols",
    "authentication systems",
    "authorized",
    "authorized review",
    "authorized use",
    "auto",
    "auto notifications",
    "automated",
    "automated email",
    "automation",
    "availability",
    "availability 31815206ca036f99280d2a8051df8935",
    "availability availability",
    "availability confirmed",
    "availability cyber",
    "availability e17031b67272371fb4010b4969cf47cf",
    "availability https",
    "availabilityrequest",
    "available",
    "available 30",
    "available selecting",
    "avature",
    "avature net",
    "ave",
    "ave 223",
    "avenue",
    "avenue sunnyvale",
    "avoid",
    "awards",
    "awesome",
    "b386",
    "b386 d8ae",
    "b8ca787b_6c91_45ec_8cc7_8f066e74c560",
    "b8ca787b_6c91_45ec_8cc7_8f066e74c560 inpython",
    "b8ca787b_6c91_45ec_8cc7_8f066e74c560 python",
    "b9ocbzlij7gl40fjm5jfz8jupdcsm8x",
    "b9ocbzlij7gl40fjm5jfz8jupdcsm8x t8yub4yhcjscaa_sk7m4zs9op2npmen5hjinvtx6vug4nkjaieg4y7t31ojs8k6bszdswh9tmnadvtfl2oqxrtzcqra",
    "ba20",
    "ba20 4e01a096a9a0",
    "bachelor",
    "bachelor degree",
    "background",
    "background experience",
    "background financial",
    "background syncing",
    "backups",
    "bae",
    "bae systems",
    "balance",
    "baltimore",
    "bank",
    "banner",
    "based",
    "based roles",
    "bash",
    "bash powershell",
    "be6d",
    "be6d 4f15",
    "bea6",
    "bea6 6930815f1308",
    "beach",
    "beach fl",
    "beauty",
    "becca",
    "begin",
    "begin retirement",
    "behalf",
    "behalf claroty",
    "believe",
    "belvoir",
    "belvoir va",
    "benefits",
    "benefits figure",
    "best",
    "best job",
    "best luck",
    "best practices",
    "best regards",
    "better",
    "big",
    "blair",
    "blair like",
    "blanchard",
    "block",
    "blog",
    "blog cloudflare",
    "blog https",
    "blue",
    "book",
    "book follow",
    "borrower",
    "bought",
    "bowhead",
    "bowhead springfield",
    "br",
    "br br",
    "br google",
    "br https",
    "br span",
    "br thanks",
    "brand",
    "brief",
    "brown",
    "browser",
    "browser url",
    "build",
    "build confidence",
    "build profile",
    "built",
    "business",
    "business partner",
    "button",
    "button scheduled",
    "buy",
    "buying",
    "c172f109bdb1",
    "ca",
    "ca 94043",
    "ca 94085",
    "calendar",
    "calendar link",
    "calendars",
    "calendars quickly",
    "called",
    "calls",
    "calls nc",
    "cam",
    "cam seeking",
    "cams",
    "cams highly",
    "candidacy",
    "candidacy way",
    "candidate",
    "candidate time",
    "candidates",
    "capable",
    "card",
    "cards",
    "care",
    "care insurance",
    "career",
    "career mantech",
    "career microsoft",
    "career search",
    "career site",
    "career tyler",
    "careers",
    "careers applications",
    "careers https",
    "careers lifeatclaroty",
    "careers linkedin",
    "careers page",
    "careers profile",
    "careful",
    "careful consideration",
    "carefully",
    "caregivers",
    "caregivers 2024",
    "caregivers https",
    "case",
    "case studies",
    "cell",
    "cell 434",
    "center",
    "center https",
    "center job",
    "certification",
    "certifications",
    "certified",
    "certified anti",
    "certified fraud",
    "certified payments",
    "cfe",
    "cfe certified",
    "change",
    "changed",
    "changed mind",
    "changes",
    "changing",
    "channel",
    "channels",
    "cheat",
    "check",
    "check frequently",
    "checking",
    "checking careers",
    "child",
    "choosing",
    "choosing daily",
    "chosen",
    "cip",
    "cip nist",
    "cip sox",
    "circia",
    "circia contract",
    "cisa",
    "cisco",
    "cisco 2025",
    "cisco affiliates",
    "cisco avature",
    "cisco careers",
    "cisco com",
    "cisco recruiting",
    "cissp",
    "city",
    "city missouri",
    "city school",
    "claim",
    "claroty",
    "claroty careers",
    "claroty com",
    "claroty hi",
    "claroty hiring",
    "claroty leading",
    "claroty looking",
    "claroty named",
    "claroty newsroom",
    "claroty reply",
    "class",
    "class code",
    "class role",
    "clec",
    "clecs",
    "click",
    "click dass4gaewho7lxrwviau7fmtagqag8chw_exjvahdqukozi5cjygoq0v8vjxxbpogk6awedmf5msjgzkaxqow4vfreazal5mv8adohc8jvnai8vt_0f9ycqyiimj8xuyjmzlut7dv3t_x7sxyujhuq1zgvyysii",
    "click dxrshaoyd8",
    "click emails",
    "click https",
    "click link",
    "click reply",
    "click s_vjzuylowtfwibo7fx7eomuob0xju3dqp2rvj",
    "click tjl_lksrzcuuzlusc_knewh",
    "click unsubscribe",
    "click upn",
    "click url",
    "clicking",
    "client",
    "clients",
    "closely",
    "closes",
    "closing",
    "cloud",
    "cloud google",
    "cloud https",
    "cloudflare",
    "cloudflare blog",
    "cloudflare com",
    "cloudflare instagram",
    "cloudflare recruiting",
    "cloudflare thanks",
    "cloudflare tv",
    "cloudflare twitter",
    "cloudflare wanted",
    "code",
    "collaboration",
    "collect",
    "collect use",
    "collective",
    "college",
    "com",
    "com abdur",
    "com accessibility",
    "com adrianshaw_83379",
    "com calls",
    "com careers",
    "com claroty",
    "com cloudflare",
    "com comeetreply",
    "com comm",
    "com company",
    "com don",
    "com email",
    "com en",
    "com fh",
    "com help",
    "com http",
    "com https",
    "com icfexternal_career_site",
    "com job",
    "com lifeatcloudflare",
    "com list",
    "com loved",
    "com ls",
    "com medium",
    "com meetingoptions",
    "com meetup",
    "com otg0lvhirs0xmzgaaagdnb9j",
    "com python",
    "com qs",
    "com redirect",
    "com rgovus",
    "com safe",
    "com search",
    "com talks",
    "com thisisicf",
    "com tracking",
    "com tuning",
    "com wrote",
    "comeetreply",
    "coming",
    "comm",
    "comm jobs",
    "comm psettings",
    "commands",
    "committed",
    "committee",
    "common",
    "communication",
    "communication preferences",
    "communication risk",
    "communications",
    "communications add",
    "community",
    "companies",
    "company",
    "company alumni",
    "company cloudflare",
    "company future",
    "company milwaukee",
    "company pricewaterhousecoopers",
    "company tech",
    "company umkc",
    "competitive",
    "complete",
    "complete ai",
    "complete application",
    "complete assessment",
    "complete google",
    "complete guide",
    "complete heloc",
    "complete selected",
    "complete simventions",
    "completed",
    "completed action",
    "completing",
    "complex",
    "compliance",
    "compliance frameworks",
    "compliance regulatory",
    "computer",
    "computer science",
    "computer society",
    "computing",
    "computing analytics",
    "concept",
    "concept level",
    "condition",
    "conference",
    "confidence",
    "confidence nail",
    "confidential",
    "confidential authorized",
    "confidentiality",
    "confidently",
    "configurations",
    "confirm",
    "confirm time",
    "confirmation",
    "confirmation email",
    "confirmation remote",
    "confirmation rtr",
    "confirmation view_job",
    "confirmed",
    "confirmed note",
    "confirmed receive",
    "confirmed thank",
    "conflict",
    "connect",
    "connection",
    "connections",
    "connections 11",
    "connections company",
    "connectivity",
    "consent",
    "consent time",
    "conservation",
    "consider",
    "consider application",
    "consider formal",
    "consider opportunity",
    "considerable",
    "considerable knowledge",
    "consideration",
    "consideration background",
    "consideration selected",
    "considering",
    "considering career",
    "console",
    "constantly",
    "contact",
    "contact good",
    "contact position",
    "contacted",
    "contain",
    "contained",
    "contained message",
    "containerized",
    "containing",
    "content",
    "content filtering",
    "context",
    "context 7b",
    "contexts",
    "continue",
    "continue apply",
    "continue conversation",
    "continue look",
    "continue receiving",
    "continued",
    "continuous",
    "continuous federal",
    "contract",
    "contract excited",
    "contract opportunity",
    "contract recommend",
    "control",
    "control endpoint",
    "control hub",
    "controls",
    "convenience",
    "conversation",
    "conversation alexis",
    "conversation discuss",
    "conversation senior",
    "conversations",
    "cool",
    "coordinate",
    "coordinate team",
    "coordinated",
    "coordinated confirmed",
    "coordinating",
    "copy",
    "copy forward",
    "core",
    "corelight",
    "cornerstone",
    "corporate",
    "corporation",
    "corporation 1zwnj000",
    "corporation microsoft",
    "corporation rights",
    "correctly",
    "cost",
    "countries",
    "countries going",
    "course",
    "course background",
    "cpp",
    "cpp certified",
    "cps",
    "cps risk",
    "credit",
    "cross",
    "cross functional",
    "cross paths",
    "csf",
    "cstone",
    "ct",
    "ctj",
    "cto",
    "cto position",
    "current",
    "current needs",
    "current roles",
    "customers",
    "cyber",
    "cyber ai",
    "cyber defense",
    "cyber job",
    "cyber machine",
    "cyber practice",
    "cyber program",
    "cyber security",
    "cyber systems",
    "cyber threat",
    "cybersecurity",
    "cybersecurity engineer",
    "cybersecurity industrial",
    "cybersecurity initiatives",
    "cybersecurity location",
    "cybersecurity maintenance",
    "cybersecurity pm",
    "cybersecurity pricewaterhousecoopers",
    "cybersecurity program",
    "cybersecurity programs",
    "cybersecurity risk",
    "cybersecurity sme",
    "cycle",
    "d151eefd0197",
    "d151eefd0197 98",
    "d6f1",
    "d6f1 42e8",
    "d8ae",
    "d8ae c172f109bdb1",
    "dahlgren",
    "daily",
    "daily weekly",
    "dashboard",
    "dass4gaewho7lxrwviau7fmtagqag8chw_exjvahdqukozi5cjygoq0v8vjxxbpogk6awedmf5msjgzkaxqow4vfreazal5mv8adohc8jvnai8vt_0f9ycqyiimj8xuyjmzlut7dv3t_x7sxyujhuq1zgvyysii",
    "data",
    "data governance",
    "data management",
    "data privacy",
    "data science",
    "data usage",
    "database",
    "date",
    "date time",
    "date working",
    "daughter",
    "day",
    "day day",
    "days",
    "days helpful",
    "days time",
    "db",
    "dba",
    "dc",
    "deactivate",
    "deadline",
    "deadline tomorrow",
    "dear",
    "dear adrian",
    "deception",
    "deception engineer",
    "decided",
    "decided forward",
    "decided pursue",
    "declared",
    "decyphertech",
    "deep",
    "defense",
    "defense engineer",
    "definitely",
    "definitely let",
    "degree",
    "degree level",
    "degree years",
    "delighted",
    "delighted considering",
    "delivered",
    "delivery",
    "deloitte",
    "deloitte openings",
    "delta",
    "delta dental",
    "demand",
    "demanding",
    "democrats",
    "demonstrated",
    "demonstrated ability",
    "dental",
    "dental continue",
    "department",
    "deploy",
    "deployment",
    "depth",
    "depth knowledge",
    "description",
    "description senior",
    "description university",
    "design",
    "desired",
    "despite",
    "details",
    "details remote",
    "detained",
    "detect",
    "detection",
    "detection prevention",
    "determine",
    "develop",
    "developing",
    "development",
    "development team",
    "device",
    "df3e28e4ae5d",
    "df3e28e4ae5d 1758351650266",
    "df3e28e4ae5d 456a00ebf92a",
    "dfir",
    "dfir engineer",
    "dial",
    "dial customers",
    "dial modems",
    "dialin",
    "dialin teams",
    "did",
    "didn",
    "difference",
    "difference happy",
    "different",
    "different days",
    "digest",
    "digest reader",
    "digital",
    "digital forensics",
    "directly",
    "directly know",
    "directly message",
    "director",
    "director engineering",
    "director interview",
    "disability",
    "disclose",
    "disclose information",
    "discord",
    "discover",
    "discover splunk",
    "discovered",
    "discuss",
    "discuss position",
    "disney",
    "display",
    "display properly",
    "distribute",
    "distribute disclose",
    "div",
    "division",
    "division computing",
    "dlp",
    "doctor",
    "document",
    "documentation",
    "documentation workflows",
    "documented",
    "documents",
    "does",
    "does negatively",
    "doing",
    "domains",
    "domains claroty",
    "don",
    "don want",
    "donald",
    "donald trump",
    "donotreply",
    "donotreply cisco",
    "download",
    "download speeds",
    "downtown",
    "dozens",
    "dr",
    "dr shaw",
    "dragos",
    "dramatic",
    "draw",
    "drive",
    "dsc",
    "dxrshaoyd8",
    "dxrshaoyd8 zp5ho2o_eezfg69jzi2du",
    "dynamic",
    "e17031b67272371fb4010b4969cf47cf",
    "e17031b67272371fb4010b4969cf47cf utm_medium",
    "e4397a0d",
    "e4397a0d ef4f",
    "earlier",
    "earliest",
    "earliest convenience",
    "early",
    "east",
    "east asia",
    "easy",
    "easy just",
    "eb8e1bd46a6c",
    "eb8e1bd46a6c a3aeaf9b7c1a",
    "ecs",
    "ecs regarding",
    "edition",
    "edition jason",
    "education",
    "ef4f",
    "ef4f b386",
    "effectively",
    "effort",
    "efforts",
    "eid",
    "eid 1itovf",
    "ej",
    "electronically",
    "electronically sign",
    "elon",
    "elon musk",
    "email",
    "email address",
    "email assessment",
    "email date",
    "email df3e28e4ae5d",
    "email figure",
    "email intended",
    "email preferences",
    "email recruiting",
    "email regards",
    "email response",
    "email sent",
    "email settings",
    "email software",
    "email unsubscribe",
    "email utm_source",
    "email_application_confirmation_with_nba_01",
    "email_application_confirmation_with_nba_01 application",
    "email_application_confirmation_with_nba_01 help",
    "email_application_confirmation_with_nba_01 job_card",
    "email_application_confirmation_with_nba_01 securityhelp",
    "email_application_confirmation_with_nba_01 unsubscribe",
    "email_jobs_job_application_viewed_01",
    "email_jobs_job_application_viewed_01 help",
    "email_jobs_job_application_viewed_01 securityhelp",
    "email_jobs_job_application_viewed_01 unsubscribe",
    "emails",
    "emails choosing",
    "emails click",
    "emails company",
    "emails monster",
    "emails unsubscribe",
    "emails update",
    "eml",
    "eml email_application_confirmation_with_nba_01",
    "eml email_jobs_job_application_viewed_01",
    "emphasis",
    "emphasis tcp",
    "employment",
    "employment opportunities",
    "en",
    "en join",
    "en lipi",
    "en windows",
    "encourage",
    "encourage check",
    "encourage continue",
    "encourage monitor",
    "end",
    "endangered",
    "ended",
    "endpoint",
    "endpoint security",
    "engagement",
    "engine",
    "engineer",
    "engineer 777674",
    "engineer ai",
    "engineer depth",
    "engineer ft",
    "engineer kelly",
    "engineer senior",
    "engineer threat",
    "engineer tyler",
    "engineering",
    "engineering adversarial",
    "engineering division",
    "english",
    "english https",
    "english source",
    "enjoyed",
    "enjoyed getting",
    "ensure",
    "ensure continue",
    "ensures",
    "ensuring",
    "ensuring alignment",
    "enter",
    "enter availability",
    "enterprise",
    "entities",
    "environments",
    "equal",
    "equal housing",
    "equity",
    "equity line",
    "error",
    "escalator",
    "especially",
    "especially approach",
    "espn",
    "et",
    "european",
    "evaluate",
    "evaluate ot",
    "evaluated",
    "evaluated ll",
    "evaluating",
    "evaluating profile",
    "event",
    "event view",
    "eventual",
    "evolution",
    "exact",
    "examiner",
    "examiner cfe",
    "excellent",
    "excited",
    "excited forward",
    "exciting",
    "executive",
    "executive recruiter",
    "expected",
    "experience",
    "experience areas",
    "experience assure",
    "experience cybersecurity",
    "experience like",
    "experience substitute",
    "experience web",
    "experience won",
    "experienced",
    "experienced senior",
    "expert",
    "expert staff",
    "expertise",
    "expertise security",
    "explains",
    "explains collect",
    "explore",
    "explore app",
    "explore google",
    "explorer",
    "explorer street",
    "exploring",
    "exposure",
    "express",
    "extensive",
    "extensive step",
    "external",
    "external ecs",
    "eyj0exaioijkv1qilcjhbgcioijiuzi1nij9",
    "eyj1ijoim2nvd3cyin0",
    "eyj1ijoim2nvd3cyin0 gbvayq",
    "ezgzmtdnv5ar",
    "facebook",
    "facebook com",
    "facebook https",
    "facebook instagram",
    "factors",
    "faculty",
    "faculty assistant",
    "fair",
    "fairly",
    "fairly quickly",
    "familiarity",
    "family",
    "faqs",
    "faqs br",
    "fast",
    "father",
    "february",
    "february 26",
    "federal",
    "federal service",
    "feedback",
    "feedback application",
    "feel",
    "feel free",
    "feels",
    "fh",
    "fh files",
    "field",
    "field cto",
    "fields",
    "figure",
    "figure com",
    "figure heloc",
    "figure home",
    "figure lending",
    "file",
    "files",
    "files helpcenter",
    "filling",
    "filling shift",
    "filtering",
    "finally",
    "financial",
    "financial controls",
    "financial goals",
    "financial representative",
    "findings",
    "finish",
    "finley",
    "finley mary",
    "finley partnerforces",
    "finra",
    "firebase",
    "firewalls",
    "firewalls anti",
    "firewalls ids",
    "fit",
    "fit ll",
    "fit receive",
    "fit thanks",
    "fixed",
    "fixed rate",
    "fl",
    "fl 33572",
    "floor",
    "floor unfortunately",
    "florida",
    "focus",
    "focus don",
    "focus filling",
    "folder",
    "follow",
    "follow cisco",
    "follow link",
    "follow linkedin",
    "following",
    "following link",
    "following linkedin",
    "font",
    "font size",
    "font style",
    "font weight",
    "forces",
    "forces help",
    "forces interview",
    "forces support",
    "forensic",
    "forensic investigation",
    "forensics",
    "forever",
    "forgot",
    "form",
    "formal",
    "formal request",
    "forward",
    "forward application",
    "forward candidate",
    "forward distribute",
    "forward hearing",
    "forward interview",
    "forward process",
    "forward time",
    "forwarded",
    "framed",
    "frameworks",
    "frameworks nerc",
    "fraud",
    "fraud examiner",
    "fraud schemes",
    "fredericksburg",
    "fredericksburg virginia",
    "free",
    "free receive",
    "frequently",
    "frequently continue",
    "fri",
    "friday",
    "friday shifting",
    "friends",
    "ft",
    "ft belvoir",
    "fully",
    "fully remote",
    "fun",
    "functional",
    "functions",
    "funded",
    "funding",
    "funds",
    "future",
    "future emails",
    "future hi",
    "future https",
    "future mary",
    "future openings",
    "future taking",
    "game",
    "gartner",
    "gartner claroty",
    "gateways",
    "gateways network",
    "gbvayq",
    "gbvayq jwuxrycpqsaq7s4dn",
    "gc",
    "general",
    "general assembly",
    "generative",
    "generative ai",
    "george",
    "getting",
    "getting know",
    "ghafaqs",
    "ghafaqs pdf",
    "giac",
    "gift",
    "gift card",
    "gift cards",
    "girl",
    "github",
    "glad",
    "glad interested",
    "gle",
    "gle aniao5o",
    "global",
    "global lng",
    "gmail",
    "gmail com",
    "goals",
    "going",
    "going hell",
    "good",
    "good luck",
    "good match",
    "google",
    "google cloud",
    "google com",
    "google hiring",
    "google llc",
    "gop",
    "gop leaders",
    "goswami",
    "got",
    "got thanks",
    "governance",
    "governance amp",
    "government",
    "government using",
    "graduate",
    "great",
    "great learning",
    "great recruiting",
    "green",
    "greenhouse",
    "greenhouse io",
    "greg",
    "greg raaberg",
    "group",
    "groups",
    "grumman",
    "gsa",
    "gsa schedule",
    "guidance",
    "guide",
    "guide narrator",
    "hacker",
    "hacker 2025",
    "halifax",
    "halifax virginia",
    "hand",
    "hand reach",
    "handle",
    "hands",
    "hands experience",
    "happen",
    "happen times",
    "happening",
    "happens",
    "happens review",
    "happy",
    "happy contact",
    "hard",
    "hardware",
    "haven",
    "head",
    "health",
    "healthcare",
    "healthcare domains",
    "hear",
    "hear soon",
    "heard",
    "hearing",
    "hegseth",
    "held",
    "hell",
    "hell actually",
    "hello",
    "hello adrian",
    "hello kelly",
    "heloc",
    "heloc application",
    "help",
    "help develop",
    "help google",
    "help https",
    "help linkedin",
    "help perfect",
    "help questions",
    "help schedule",
    "help textfooterglimmer",
    "helpcenter",
    "helpcenter ghafaqs",
    "helped",
    "helpful",
    "helpful moving",
    "helping",
    "heysummit",
    "heysummit com",
    "hi",
    "hi adrian",
    "hi blair",
    "hi kelly",
    "hi mary",
    "hi reminder",
    "hidden",
    "hide",
    "high",
    "highlights",
    "highly",
    "highly desired",
    "highly required",
    "hire",
    "hire recruit",
    "hiring",
    "hiring assessment",
    "hiring manager",
    "hiring process",
    "hiring team",
    "hit",
    "hit reply",
    "hl",
    "hl en",
    "hokie",
    "home",
    "home equity",
    "home value",
    "honest",
    "honorvet",
    "honorvet technologies",
    "hope",
    "hope cross",
    "hosting",
    "hosting options",
    "hour",
    "hour instructions",
    "hours",
    "house",
    "housing",
    "housing opportunity",
    "hr",
    "hr teams",
    "href",
    "href https",
    "http",
    "http cisco",
    "http mantech",
    "http voice",
    "http www",
    "https",
    "https ablink",
    "https aka",
    "https app2",
    "https app7",
    "https blog",
    "https click",
    "https cloudflare",
    "https dialin",
    "https discover",
    "https gle",
    "https icf",
    "https instagram",
    "https intranet",
    "https link",
    "https medium",
    "https mostlovedworkplace",
    "https policy",
    "https services",
    "https standoutsummit",
    "https substack",
    "https support",
    "https teams",
    "https tracking",
    "https twitter",
    "https www",
    "hub",
    "human",
    "human resources",
    "hunt",
    "hunt like",
    "hunter",
    "hunter lose",
    "hunting",
    "hunting aint",
    "hybrid",
    "hybrid company",
    "hybrid option",
    "ice",
    "icf",
    "icf com",
    "icf wd5",
    "icfexternal_career_site",
    "icims",
    "icims com",
    "id",
    "id 1438794",
    "idea",
    "ideal",
    "ideal candidate",
    "identify",
    "ids",
    "ids ips",
    "ieee",
    "ieee computer",
    "ii",
    "imagine",
    "immediate",
    "immediate focus",
    "immigration",
    "impact",
    "impact candidacy",
    "implementation",
    "implementing",
    "import",
    "import attach",
    "import linkedin",
    "important",
    "important information",
    "important thing",
    "improve",
    "improving",
    "incident",
    "incident responder",
    "incidents",
    "incidents related",
    "include",
    "included",
    "included https",
    "includes",
    "including",
    "income",
    "incomplete",
    "incomplete dear",
    "increasing",
    "industrial",
    "industrial automation",
    "industrial healthcare",
    "industry",
    "industry standards",
    "influence",
    "inform",
    "information",
    "information application",
    "information apply",
    "information contained",
    "information hand",
    "information provide",
    "information security",
    "information systems",
    "infrastructure",
    "initial",
    "initial conversation",
    "initiatives",
    "inpython",
    "inpython plain",
    "insider",
    "insider threat",
    "instagram",
    "instagram com",
    "instagram https",
    "instagram youtube",
    "instead",
    "instructions",
    "instructions access",
    "insurance",
    "insurance application",
    "insurance company",
    "integrated",
    "integrated resources",
    "integrating",
    "integrity",
    "intel",
    "intel 1081191",
    "intel research",
    "intelligence",
    "intelligence analyst",
    "intelligence cycle",
    "intelligence job",
    "intelligence role",
    "intended",
    "intended kelly",
    "intended personal",
    "interested",
    "interested career",
    "interested joining",
    "interested network",
    "interests",
    "interests new",
    "internal",
    "international",
    "international corporation",
    "internet",
    "internet 20speed",
    "internet access",
    "internet connection",
    "internet explorer",
    "intersection",
    "intersection cyber",
    "interview",
    "interview availability",
    "interview claroty",
    "interview confirmed",
    "interview feedback",
    "interview guide",
    "interview kelly",
    "interview let",
    "interview ll",
    "interview network",
    "interview prep",
    "interview process",
    "interview provide",
    "interview receive",
    "interview scheduled",
    "interviewing",
    "interviewing actually",
    "interviews",
    "intranet",
    "intranet radgov",
    "intrusion",
    "intrusion detection",
    "investigate",
    "investigation",
    "investment",
    "investment advisory",
    "io",
    "io availability",
    "ip",
    "ips",
    "ips siem",
    "irionline",
    "irionline com",
    "isdn",
    "isdn cards",
    "isn",
    "iso",
    "isp",
    "isp business",
    "ispcon",
    "isps",
    "isso",
    "isso systems",
    "issue",
    "issues",
    "issues just",
    "italic",
    "jacob",
    "jacob louis",
    "jason",
    "jason blanchard",
    "jimmy",
    "jimmy kimmel",
    "jo",
    "jo enter",
    "jo finley",
    "jo thank",
    "job",
    "job alert",
    "job alerts",
    "job application",
    "job applied",
    "job becca",
    "job description",
    "job expert",
    "job https",
    "job hunt",
    "job hunting",
    "job id",
    "job listed",
    "job market",
    "job moved",
    "job number",
    "job opening",
    "job opportunity",
    "job requirements",
    "job search",
    "job security",
    "job senior",
    "job soc",
    "job submission",
    "job title",
    "job_card",
    "job_card view_job",
    "jobs",
    "jobs help",
    "jobs https",
    "jobs interested",
    "jobs posted",
    "jobs view",
    "jobscan",
    "jobscan partner",
    "jobscan team",
    "jobvite",
    "jobvite ______",
    "jobvite com",
    "jobvite user",
    "joe",
    "john",
    "join",
    "join 19",
    "join links",
    "join meeting",
    "join talk",
    "join training",
    "joining",
    "joining team",
    "jointeamsmeeting",
    "jointeamsmeeting omkt",
    "js",
    "junior",
    "just",
    "just click",
    "just hit",
    "jwuxrycpqsaq7s4dn",
    "jwuxrycpqsaq7s4dn 5pw_o4y8xooyz09efo",
    "kansas",
    "kansas city",
    "kaver68",
    "kaver68 gmail",
    "kelly",
    "kelly application",
    "kelly pure",
    "kelly shaw",
    "kelly thank",
    "kelly thanks",
    "kernel",
    "key",
    "key responsibilities",
    "key review",
    "keys",
    "killer",
    "kimmel",
    "king",
    "king george",
    "know",
    "know appreciate",
    "know available",
    "know build",
    "know evaluated",
    "know learning",
    "know normal",
    "know received",
    "know steps",
    "knowledge",
    "knowledge forensic",
    "knowledge hands",
    "knowledge os",
    "knowledge skills",
    "kopmsj78ak3qwiojafujtd1uqruxbpjq",
    "kopmsj78ak3qwiojafujtd1uqruxbpjq b9ocbzlij7gl40fjm5jfz8jupdcsm8x",
    "kubernetes",
    "lab",
    "lab tool",
    "land",
    "lang",
    "lang en",
    "language",
    "language en",
    "later",
    "latest",
    "launch",
    "laundering",
    "laundering specialist",
    "layers",
    "layers osi",
    "lead",
    "lead analyst",
    "lead cyber",
    "lead engineer",
    "lead machine",
    "leader",
    "leader 2025",
    "leaders",
    "leadership",
    "leading",
    "leading cybersecurity",
    "learn",
    "learn experience",
    "learn included",
    "learn read",
    "learn workshop",
    "learned",
    "learning",
    "learning background",
    "learning engineer",
    "left",
    "left easy",
    "legal",
    "leidos",
    "leidos cybersecurity",
    "leidos position",
    "lending",
    "lending llc",
    "let",
    "let know",
    "lets",
    "level",
    "level cyber",
    "level years",
    "levels",
    "lgbtq",
    "lgbtq 2024",
    "lgbtq https",
    "li",
    "li browser",
    "li href",
    "li li",
    "li ol",
    "li ul",
    "library",
    "life",
    "life insurance",
    "lifeatclaroty",
    "lifeatclaroty awards",
    "lifeatcloudflare",
    "lifeatcloudflare 2024",
    "like",
    "like begin",
    "like fit",
    "like hacker",
    "like import",
    "like match",
    "like receive",
    "limited",
    "limited time",
    "line",
    "lines",
    "link",
    "link application",
    "link calendars",
    "link ll",
    "link return",
    "link sbstck",
    "linkedin",
    "linkedin answer",
    "linkedin com",
    "linkedin corporation",
    "linkedin https",
    "linkedin linkedin",
    "linkedin logo",
    "linkedin manually",
    "linkedin notification",
    "linkedin twitter",
    "links",
    "links http",
    "linux",
    "linux unix",
    "lipi",
    "lipi urn",
    "list",
    "list email",
    "list safe",
    "listed",
    "listed active",
    "live",
    "live following",
    "ll",
    "ll able",
    "ll coordinate",
    "ll definitely",
    "ll hear",
    "ll notified",
    "ll receive",
    "ll sure",
    "llc",
    "llc 1600",
    "llc dba",
    "llc description",
    "llc seattle",
    "lng",
    "lng excited",
    "lng interview",
    "lng regards",
    "loan",
    "loan funds",
    "loans",
    "loans figure",
    "loans secured",
    "local",
    "local community",
    "local exchange",
    "local kid",
    "local number",
    "local officials",
    "local phone",
    "local police",
    "local time",
    "located",
    "located ak",
    "located counties",
    "located fl",
    "located nj",
    "located tx",
    "location",
    "location 100",
    "location apollo",
    "location florida",
    "location kansas",
    "location lynchburg",
    "location pleasanton",
    "location questions",
    "location richmond",
    "location seattle",
    "locations",
    "locations database",
    "log",
    "log analysis",
    "log online",
    "log view",
    "logged",
    "logged event",
    "logging",
    "logging console",
    "logging https",
    "login",
    "login account",
    "login banner",
    "login northwesternmutual",
    "logo",
    "logo equal",
    "logo registered",
    "logs",
    "logs networks",
    "loid",
    "loid aqef5unyrcqhfwaaazkhc32",
    "loid aqffdweo33wmeqaaazvx0",
    "loid aqgeiv7frq82naaaazd5cy54obes2utyvav9thiulscd1op2b7bdxjxbv6xiskqvqtsrnwtawyh1lky_7ldl6dzwso8frngdzd5k9y5pu56vc50",
    "long",
    "long described",
    "long hours",
    "long limited",
    "long odds",
    "long patch",
    "long running",
    "long term",
    "longer",
    "longer like",
    "longer loans",
    "longer open",
    "longer wish",
    "longtime",
    "longtime resident",
    "look",
    "look forward",
    "look opportunities",
    "looked",
    "looked integrating",
    "looked resume",
    "looking",
    "looking deeper",
    "looking forward",
    "looking highly",
    "looking notes",
    "looking talent",
    "looking talented",
    "looming",
    "looming avoid",
    "lose",
    "lose track",
    "losing",
    "losing ve",
    "loss",
    "loss principal",
    "lot",
    "lot freebsd",
    "louis",
    "louis executive",
    "louis honorvettech",
    "love",
    "love continue",
    "love excited",
    "love hear",
    "love stay",
    "loved",
    "loved working",
    "loved workplaces",
    "lower",
    "lower 400",
    "lrj",
    "lrj aenpqaihq2nxwo1_mg",
    "lrymjofgp4reslhtonqtd3duo_l03rkbje5udo8yuqihevhcmtg3l8bzzov3t23dpeza3mgwc4g1fo",
    "lrymjofgp4reslhtonqtd3duo_l03rkbje5udo8yuqihevhcmtg3l8bzzov3t23dpeza3mgwc4g1fo 169",
    "ls",
    "ls click",
    "ltrk",
    "ltrk eyj0exaioijkv1qilcjhbgcioijiuzi1nij9",
    "luck",
    "luck career",
    "luck claroty",
    "luck job",
    "luck notice",
    "luck search",
    "luck taylor",
    "lumen",
    "lumen careers",
    "lumen com",
    "lumen job",
    "lumen nbsp",
    "lumen rights",
    "lunch",
    "lunch learn",
    "lunzaldqq5ucssx3ahrxdhqmgtipbfb_swyzvtseu3pwaft",
    "lunzaldqq5ucssx3ahrxdhqmgtipbfb_swyzvtseu3pwaft f8qzt8da1zbmw3lzvxsho3lyqj24orhr7ia5xb6e",
    "lvbh",
    "lvbh g4f7_vbbpy_anna8dcuau3c_udz5n3i_2zmp1s",
    "lvfuwimcnpqyo",
    "lvfuwimcnpqyo evgvye2a",
    "lwf38iavnpdpqajokijtcmfe_h6e3m7uqm7k",
    "lwf38iavnpdpqajokijtcmfe_h6e3m7uqm7k 8z3qhq",
    "lxsc3toe0wcone4vd",
    "lxsc3toe0wcone4vd fwtw",
    "lynchburg",
    "lynchburg virginia",
    "m0s",
    "m0s ybd6fe5gmtvxrju7tmnuuhldf6ebtjbjo_ewvj8oupqyj1khg7ekrdk7nun3yu0ldtsi0lty52dtz51pz5myu7mpa0bdwvs_i",
    "m6rbkm",
    "m6rbkm s3n_mgezo1s6i1zykhysghlrauh",
    "m7rvwl6t",
    "m7rvwl6t q3",
    "m_8laeogu1vhufw6qhyrhmbd2f8xor5rwhzojkrzy3dl4qm646brbmcc44hfarfi5h5m8qttpkd6bnxrkwkt_o3",
    "m_8laeogu1vhufw6qhyrhmbd2f8xor5rwhzojkrzy3dl4qm646brbmcc44hfarfi5h5m8qttpkd6bnxrkwkt_o3 lrymjofgp4reslhtonqtd3duo_l03rkbje5udo8yuqihevhcmtg3l8bzzov3t23dpeza3mgwc4g1fo",
    "ma",
    "ma md",
    "mac",
    "mac application",
    "mac os",
    "machine",
    "machine learning",
    "machine stolen",
    "machines",
    "machines total",
    "maga",
    "maga agenda",
    "mail",
    "mail id",
    "mail peraton",
    "mail sabre",
    "mail sent",
    "mail working",
    "mailbox",
    "mailbox reply",
    "mailbox used",
    "mailto",
    "mailto donotreply",
    "maintain",
    "maintain baselines",
    "maintain improve",
    "maintain records",
    "maintain security",
    "maintained",
    "maintained perform",
    "maintained strategic",
    "maintaining",
    "maintaining continuously",
    "maintaining improving",
    "maintenance",
    "maintenance activities",
    "maintenance procedures",
    "maintenance schedules",
    "make",
    "make additional",
    "make change",
    "make choices",
    "make contact",
    "make difference",
    "make fun",
    "make request",
    "make sure",
    "makeshift",
    "makeshift coffee",
    "making",
    "making changes",
    "making contest",
    "making impact",
    "making perfect",
    "malfunctioning",
    "malfunctioning escalator",
    "mall",
    "mall sharp",
    "malware",
    "malware analysis",
    "malware anti",
    "malware extensive",
    "manage",
    "manage enrollments",
    "manage kubernetes",
    "manage monster",
    "manage multiple",
    "management",
    "management processes",
    "management web",
    "manager",
    "manager data",
    "manager threat",
    "mantech",
    "mantech adrian",
    "mantech avature",
    "mantech international",
    "mantech regards",
    "mantech review",
    "manually",
    "manually update",
    "march",
    "mark",
    "market",
    "market hell",
    "marketing",
    "mary",
    "mary finley",
    "mary jo",
    "master",
    "match",
    "match interests",
    "match meantime",
    "match position",
    "match role",
    "match skills",
    "matches",
    "matches https",
    "materials",
    "mathematics",
    "mathematics cam",
    "mathematics division",
    "matter",
    "matter expert",
    "matters",
    "maude",
    "maude avenue",
    "maximum",
    "maybe",
    "mbps",
    "mbz9wcvs",
    "mbz9wcvs 30",
    "mcb",
    "mean",
    "means",
    "means position",
    "means won",
    "meantime",
    "meantime read",
    "meantime stay",
    "meantime view",
    "measurement",
    "measurement lab",
    "media",
    "medium",
    "medium com",
    "medium https",
    "medium membership",
    "medium privacy",
    "medium terms",
    "medium work",
    "meet",
    "meet greg",
    "meet highest",
    "meet requirements",
    "meet teams",
    "meeting",
    "meeting democratic",
    "meeting https",
    "meeting id",
    "meeting options",
    "meeting ukrainian",
    "meetingoptions",
    "meetingoptions organizerid",
    "meets",
    "meets current",
    "meetup",
    "meetup join",
    "mejia",
    "mejia detained",
    "mejia longtime",
    "mejia surrendering",
    "melania",
    "melania stepped",
    "member",
    "member finra",
    "member https",
    "members",
    "members personal",
    "membership",
    "membership use",
    "mention",
    "mention honest",
    "mention specific",
    "mentor",
    "mentor provide",
    "mentored",
    "mentored young",
    "mentoring",
    "mentoring junior",
    "merchant",
    "merchant leader",
    "mess",
    "mess room",
    "message",
    "message _____",
    "message attachment",
    "message attachments",
    "message click",
    "message contacted",
    "message error",
    "message got",
    "message icf",
    "message intended",
    "message mailbox",
    "message sent",
    "messageid",
    "messageid language",
    "messages",
    "messages past",
    "met",
    "met adaptability",
    "meta",
    "meta washington",
    "method",
    "method beats",
    "meticulous",
    "meticulous network",
    "metrics",
    "metrics hide",
    "mf8x79m0",
    "mf8x79m0 n8",
    "mfaverify",
    "mfaverify insession",
    "mi",
    "mi 48310",
    "mi mn",
    "miami",
    "miami dade",
    "miami site",
    "microsoft",
    "microsoft com",
    "microsoft corporation",
    "microsoft data",
    "microsoft edge",
    "microsoft glad",
    "microsoft help",
    "microsoft internet",
    "microsoft introduced",
    "microsoft narrator",
    "microsoft recruiting",
    "microsoft respects",
    "microsoft reston",
    "microsoft teams",
    "microsoft way",
    "mid",
    "midsig",
    "midsig 0nbj7cyguzlhe1",
    "midsig 3d5bhtkd8wuru1",
    "midsig 3nihed8zqo4hq1",
    "midtoken",
    "midtoken aqfg4l_y5e50qw",
    "mile",
    "military",
    "milwaukee",
    "milwaukee wi",
    "min",
    "min read",
    "mind",
    "mind opportunities",
    "mind taking",
    "mind unsubscribe",
    "minimum",
    "minimum 15",
    "minimum loan",
    "minus",
    "minus origination",
    "minute",
    "minute anonymous",
    "minute general",
    "minute google",
    "minutes",
    "minutes br",
    "minutes complete",
    "minutes corelight",
    "minutes meantime",
    "minutes selecting",
    "minutes ultimately",
    "misconduct",
    "misconduct dating",
    "miss",
    "miss beat",
    "missed",
    "missed significant",
    "missile",
    "missile launch",
    "missiles",
    "missiles idea",
    "missing",
    "missing experience",
    "missing particular",
    "missing resume",
    "missioncontrol",
    "missioncontrol source",
    "missouri",
    "missouri company",
    "missouri kansas",
    "mitigate",
    "mitigate risks",
    "mitre",
    "mitre thank",
    "mixed",
    "mixed don",
    "mkt_tok",
    "mkt_tok otg0lvhirs0xmzgaaagdnb9j",
    "mkt_unsubscribe",
    "mkt_unsubscribe mkt_tok",
    "ml",
    "ml consultant",
    "ml learn",
    "mn",
    "mn mo",
    "mn6pc6jyk8k9aff8r62l71tvhxcxzash3nln7kgbwavbkpvmlyfxadbuprhstzqm_4fkhjibayn1pjovqmwfilte",
    "mn6pc6jyk8k9aff8r62l71tvhxcxzash3nln7kgbwavbkpvmlyfxadbuprhstzqm_4fkhjibayn1pjovqmwfilte message",
    "mnimi6och6_evwkuv7w32ix0lsx21uoxtruzlglacgjl50o5wjx6jw8j",
    "mnimi6och6_evwkuv7w32ix0lsx21uoxtruzlglacgjl50o5wjx6jw8j 4ecmfqg521b5r8djoyc2yuso3qbcj8tplw",
    "mns98axqpcah0gd5vqvo5clrnjwsbwpl50srjl0yw5deo7r5zxc5htqpvx",
    "mns98axqpcah0gd5vqvo5clrnjwsbwpl50srjl0yw5deo7r5zxc5htqpvx ckcqxix_jvahhtxxd5bf_mwys2zsfl19agglt9n9havykwu1",
    "mo",
    "mo ms",
    "mocked",
    "mocked ineffective",
    "model",
    "model emphasis",
    "model ideal",
    "models",
    "models figure",
    "models including",
    "modem",
    "modem appliance",
    "modem card",
    "modem modem",
    "modem phone",
    "modem screaming",
    "modem use",
    "modem using",
    "modems",
    "modems hard",
    "modems isdn",
    "modems like",
    "modems microsoft",
    "modems missiles",
    "modems need",
    "modems purchased",
    "modems shelf",
    "moderating",
    "moderating stop",
    "modern",
    "modern interviews",
    "modular",
    "modular ai",
    "module",
    "module share",
    "moisture",
    "moisture air",
    "moment",
    "moment justice",
    "mon",
    "mon jun",
    "monday",
    "monday april",
    "monday friday",
    "monday march",
    "money",
    "money laundering",
    "monitor",
    "monitor assess",
    "monitor careers",
    "monitor cve",
    "monitoring",
    "monitoring ot",
    "monitoring proficiency",
    "monitors",
    "monitors log",
    "monkey",
    "monkey population",
    "monster",
    "monster click",
    "monster com",
    "monster email",
    "monster privacy",
    "monster worldwide",
    "month",
    "monthly",
    "monthly notifications",
    "months",
    "months continuous",
    "mostlovedworkplace",
    "mostlovedworkplace com",
    "motivated",
    "motivated people",
    "mountain",
    "mountain view",
    "moved",
    "moved archived",
    "moved forward",
    "moving",
    "moving forward",
    "mra",
    "mra 10",
    "ms",
    "ms jointeamsmeeting",
    "ms word",
    "mt",
    "mt nc",
    "multi",
    "multi sided",
    "multi state",
    "multi year",
    "multilateral",
    "multilateral cooperation",
    "multiple",
    "multiple concurrent",
    "multiple firearms",
    "multiple hr",
    "multiple investigations",
    "multiple positions",
    "multiple times",
    "musk",
    "musk 79",
    "musk accused",
    "mutual",
    "mutual life",
    "mutual marketing",
    "mutual privacy",
    "mutual representatives",
    "mutual wealth",
    "mvbwdbxocafboxlf_qlt",
    "mvbwdbxocafboxlf_qlt 1b8ftappbhh38fzuy6ruaolxo_uqv46yyokmdgneuwuhomnto5e6sbswtemvrknsi5rgwa4w3uwojsh1ymmra4ai2pawvt8dfvdciubhdmbxeic0mvz2x",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvlndyxn2u0yje4zgm4zde0otllytc4nty5nzhjmta5njc0njvintrmymzjywnkmdg5ngvmmwm4zjgwmthhztu5zmzknmeymgiwnzuzzdhmnjrhmzyxnjjlmwm3ldesmq",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvlndyxn2u0yje4zgm4zde0otllytc4nty5nzhjmta5njc0njvintrmymzjywnkmdg5ngvmmwm4zjgwmthhztu5zmzknmeymgiwnzuzzdhmnjrhmzyxnjjlmwm3ldesmq 3d",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndexy2u3yjy4n2nhzde0mtlkywm4nty5nzhjmta5njc0njvintrmymzjognimgjin2finwjmztm3ztzhnjm4mgyzmmy2ztvimmnizjhlzjk3nme3ogjkzjg3ldesmq",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndexy2u3yjy4n2nhzde0mtlkywm4nty5nzhjmta5njc0njvintrmymzjognimgjin2finwjmztm3ztzhnjm4mgyzmmy2ztvimmnizjhlzjk3nme3ogjkzjg3ldesmq 3d",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndyxzwvmymq4zwm5zdq0nzllywy4nty5nzhjmta5njc0njvintrmymzjodi4odllnzjimmm0yzm3nzrjmjjmyznlzwqyn2niywmwmgi3ndgznza4zwm2ymqwldesmq",
    "mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndyxzwvmymq4zwm5zdq0nzllywy4nty5nzhjmta5njc0njvintrmymzjodi4odllnzjimmm0yzm3nzrjmjjmyznlzwqyn2niywmwmgi3ndgznza4zwm2ymqwldesmq 3d",
    "mxnms46utjg1pnpkv8jinh6d6ruwbgep5bo2f2o7qx",
    "mxnms46utjg1pnpkv8jinh6d6ruwbgep5bo2f2o7qx c2xvnlnwdtnpgseef8_wfo",
    "mxui0lyw _xq8lkjrdaaaw8eg4rop",
    "myaccount",
    "myaccount google",
    "mycareersource",
    "mycareersource com",
    "myers",
    "myers content",
    "mypassword",
    "mypassword service",
    "myworkday",
    "myworkday com",
    "myworkdayjobs",
    "myworkdayjobs com",
    "n3wlkx",
    "n3wlkx q3nbheueffzbwk6dbr",
    "n6evgbmbzbibn3imbahystkqo648h0uwcea",
    "n6evgbmbzbibn3imbahystkqo648h0uwcea 169",
    "n8",
    "n8 loid",
    "n8 null",
    "n8 otptoken",
    "nail",
    "nail interview",
    "named",
    "named leader",
    "named person",
    "names",
    "names product",
    "names trademarks",
    "narrator",
    "narrator e4397a0d",
    "narrator ul",
    "narrow",
    "narrow search",
    "natesnewsletter",
    "natesnewsletter substack",
    "nation",
    "nation bonobo",
    "nationally",
    "nationally station",
    "nations",
    "nations going",
    "native",
    "native dialer",
    "nato",
    "nato continued",
    "nato support",
    "nature",
    "nature reason",
    "nazeer",
    "nazeer https",
    "nbsp",
    "nbsp adrian",
    "nbsp available",
    "nbsp background",
    "nbsp global",
    "nbsp nbsp",
    "nbsp openings",
    "nbsp palo",
    "nbsp platform",
    "nbsp senior",
    "nbsp talent",
    "nbu4ls3ampwy058u7tcmnhj_ewjz7aege_s_lu2nvslrqdhwpidw9gkhomr90vc9xljauporj",
    "nbu4ls3ampwy058u7tcmnhj_ewjz7aege_s_lu2nvslrqdhwpidw9gkhomr90vc9xljauporj 169",
    "nc",
    "nc 28202",
    "nc 2b14345792113",
    "nc nd",
    "nc onsite",
    "ncsa",
    "ncsa httpd",
    "nd",
    "nd ne",
    "ne",
    "ne nh",
    "near",
    "near historic",
    "nearly",
    "nearly 000",
    "nearly discipline",
    "nearly impossible",
    "necessarily",
    "necessarily entities",
    "necessary",
    "necessary end",
    "neck",
    "neck pen",
    "need",
    "need accommodation",
    "need change",
    "need complete",
    "need finish",
    "need help",
    "need join",
    "need logged",
    "need pick",
    "need provide",
    "need remediation",
    "need respectful",
    "need t1",
    "need troubleshooting",
    "need ve",
    "needed",
    "needed bipartisan",
    "needed complete",
    "needed personal",
    "needed thank",
    "needing",
    "needing robust",
    "needs",
    "needs contact",
    "needs engaging",
    "needs project",
    "needs thanks",
    "needs things",
    "negatively",
    "negatively impact",
    "negotiate",
    "negotiate changed",
    "nerc",
    "nerc cip",
    "nests",
    "nests afloat",
    "net",
    "net address",
    "net don",
    "net http",
    "net ltrk",
    "net unsubscribe",
    "netflix",
    "netflix united",
    "netscape",
    "netscape sold",
    "netscape web",
    "network",
    "network access",
    "network appliances",
    "network architecture",
    "network communications",
    "network competitive",
    "network financial",
    "network information",
    "network learned",
    "network long",
    "network operations",
    "network projects",
    "network protocols",
    "network scans",
    "network security",
    "network threat",
    "networking",
    "networking associate",
    "networking cybersecurity",
    "networking device",
    "networking distributed",
    "networking linux",
    "networking professionals",
    "networking protocols",
    "networking routing",
    "networks",
    "networks cstone",
    "networks dear",
    "networks good",
    "networks nbsp",
    "networks processing",
    "networks recruiting",
    "networks systems",
    "new",
    "new job",
    "new jobs",
    "new opportunities",
    "new positions",
    "new posts",
    "new roles",
    "new york",
    "news",
    "news donald",
    "newsroom",
    "newsroom follow",
    "nist",
    "nist csf",
    "nj",
    "nm",
    "nmis",
    "nmwmc",
    "non",
    "non technical",
    "normal",
    "normal does",
    "normal recruiting",
    "north",
    "northrop",
    "northrop grumman",
    "northwestern",
    "northwestern mutual",
    "northwesternmutual",
    "northwesternmutual com",
    "note",
    "note ensure",
    "note need",
    "note request",
    "notes",
    "notice",
    "notice application",
    "notice message",
    "notices",
    "notification",
    "notification emails",
    "notification jobvite",
    "notifications",
    "notifications add",
    "notifications job",
    "notified",
    "notified recruiting",
    "notify",
    "notify new",
    "ntelos",
    "null",
    "null 1itovf",
    "null eid",
    "null null",
    "number",
    "numerous",
    "october",
    "offer",
    "offered",
    "offered sold",
    "offers",
    "officials",
    "ol",
    "ol li",
    "old",
    "old 14",
    "omkt",
    "omkt en",
    "online",
    "online account",
    "online calendar",
    "online https",
    "onsite",
    "open",
    "open position",
    "open withdrew",
    "opening",
    "opening role",
    "openings",
    "openings tied",
    "operate",
    "operating",
    "operation",
    "operation video",
    "operational",
    "operational development",
    "operational models",
    "operational systems",
    "operational technology",
    "operations",
    "operations center",
    "operations engineer",
    "operations group",
    "operations king",
    "operations role",
    "opportunities",
    "opportunities arise",
    "opportunities best",
    "opportunities encourage",
    "opportunities friends",
    "opportunities longer",
    "opportunities match",
    "opportunities position",
    "opportunities road",
    "opportunities visit",
    "opportunities www",
    "opportunity",
    "opportunity aligns",
    "opportunity clients",
    "opportunity cyber",
    "opportunity direct",
    "opportunity figure",
    "opportunity logo",
    "opportunity potential",
    "opportunity present",
    "opportunity program",
    "opportunity resume",
    "opportunity review",
    "opportunity shape",
    "opportunity state",
    "opportunity submit",
    "opportunity thank",
    "opportunity unfortunately",
    "opportunity visit",
    "opportunity willing",
    "opposed",
    "opposed strictly",
    "opt",
    "opt types",
    "optimal",
    "optimal experience",
    "optimize",
    "optimize data",
    "option",
    "option 56",
    "option year",
    "optional",
    "optional module",
    "options",
    "options available",
    "options google",
    "options https",
    "options span",
    "options try",
    "optoutemail",
    "optoutemail a422d9xuc3di168d17cd98633a",
    "opuntvddxvzfvz2tpsglx9gmxclmgqv",
    "opuntvddxvzfvz2tpsglx9gmxclmgqv rdyj1znutju",
    "oracle",
    "oracle united",
    "orange",
    "orange green",
    "orange white",
    "orchestration",
    "orchestration google",
    "orchestration leverage",
    "order",
    "order assess",
    "order forward",
    "order received",
    "org",
    "org don",
    "org https",
    "org northwestern",
    "org sipc",
    "organization",
    "organization using",
    "organizerid",
    "organizerid 05b017b8",
    "organizerid 2b5205c6",
    "organizers",
    "organizers meeting",
    "origination",
    "origination based",
    "origination fee",
    "origination initial",
    "ortiz",
    "ortiz flores",
    "os",
    "os familiarity",
    "os security",
    "os web",
    "os83dp5ukhtm5fkbhjkvxcfrcr_ngnk3pqkvmaihj6yu_es2z59n1jt4tvtmknv4",
    "os83dp5ukhtm5fkbhjkvxcfrcr_ngnk3pqkvmaihj6yu_es2z59n1jt4tvtmknv4 nsmjqcuslgdf0igctxwshefidvksm_zb_f_sfys9f0",
    "oscp",
    "oscp giac",
    "osi",
    "osi model",
    "ot",
    "ot assets",
    "ot awareness",
    "ot collaboration",
    "ot cyber",
    "ot cybersecurity",
    "ot detection",
    "ot environments",
    "ot industrial",
    "ot network",
    "ot systems",
    "ot vulnerabilities",
    "otg0lvhirs0xmzgaaagdnb9j",
    "otg0lvhirs0xmzgaaagdnb9j 56aq_",
    "otg0lvhirs0xmzgaaagdnb9j 5ro4mfplgculjhcrn7a4ikqjfaqwcj6ry02lltvcya6ivrylp5s2urdgcvcy",
    "otg0lvhirs0xmzgaaagdnb9j 6yzp7xryuoxxykjxkd5lb5a8faykh9oy2pvv5drsyw3hqbulc90vc0rlgurjb",
    "otg0lvhirs0xmzgaaagdnb9j 7na94spdsbbjyhaa56yaw5rombck7tf0cvtxlpulececptf672d3uirmmp2fxvt84c",
    "otg0lvhirs0xmzgaaagdnb9j 9kkgch2srnxoqixkg4gkl6cji4zofmr0siqnwub",
    "otg0lvhirs0xmzgaaagdnb9j 9royg2jvflmla54soxfcpcmvwqznk43c7pxf7zhjzk4okouytovsfd6rsgqj1qmrkkvbbhndu9z9gn35qqwa5tuuccs9sl0gplgmb8_k7n6",
    "otg0lvhirs0xmzgaaagdnb9j mxnms46utjg1pnpkv8jinh6d6ruwbgep5bo2f2o7qx",
    "otg0lvhirs0xmzgaaagdnb9j w3ya82nzmkzho9hqmcufdkcf1natqqqey511qiabc_68by_7ks7a8cka5c_4",
    "otg0lvhirs0xmzgaaagdnb9j wphmvg4bowfqpkvo8v4bgop0eppwvfdm2fzhayhc_pmzodnbmbdxfdl",
    "otg0lvhirs0xmzgaaagdnb9j zmxfkgfbvgk6vig8vmrq9a6izo",
    "otg0lvhirs0xmzgaaagdnb9j zmxymtro8jnnas1vc8lnvko5rgu2b_rlcjpxrnqglmveyqylrrwlwnsk1migb3mw4q",
    "otptoken",
    "otptoken mwiwnzflztgxnjjiy2jjn2jlmmywmmvlndyxn2u0yje4zgm4zde0otllytc4nty5nzhjmta5njc0njvintrmymzjywnkmdg5ngvmmwm4zjgwmthhztu5zmzknmeymgiwnzuzzdhmnjrhmzyxnjjlmwm3ldesmq",
    "otptoken mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndexy2u3yjy4n2nhzde0mtlkywm4nty5nzhjmta5njc0njvintrmymzjognimgjin2finwjmztm3ztzhnjm4mgyzmmy2ztvimmnizjhlzjk3nme3ogjkzjg3ldesmq",
    "otptoken mwiwnzflztgxnjjiy2jjn2jlmmywmmvmndyxzwvmymq4zwm5zdq0nzllywy4nty5nzhjmta5njc0njvintrmymzjodi4odllnzjimmm0yzm3nzrjmjjmyznlzwqyn2niywmwmgi3ndgznza4zwm2ymqwldesmq",
    "ouch",
    "ouch held",
    "outbound",
    "outbound emails",
    "outcomes",
    "outcomes drive",
    "outside",
    "outside home",
    "outside leominster",
    "outside vendors",
    "overall",
    "overall security",
    "overall technology",
    "oversee",
    "oversee cybersecurity",
    "overseeing",
    "overseeing compliance",
    "oversees",
    "oversees cyber",
    "overview",
    "overview cyber",
    "overview vdot",
    "owcx",
    "owcx amlnfl1xg60ft6uvceg6p",
    "owned",
    "owned miami",
    "owned uva",
    "owner",
    "owner ntelos",
    "owners",
    "owners 2024",
    "owners like",
    "ownership",
    "ownership deliverables",
    "owning",
    "owning corvette",
    "p4zcz5yai4ol9h3fvgfyzolid8my_awitdszgljr3wy_r",
    "p4zcz5yai4ol9h3fvgfyzolid8my_awitdszgljr3wy_r yxzr4nax6l7f5dlvaywuuowka5f_lfxtvj",
    "pa",
    "pa ri",
    "paced",
    "paced development",
    "packet",
    "packet analysis",
    "page",
    "page apply",
    "page current",
    "page days",
    "page employment",
    "page https",
    "page join",
    "page logging",
    "page nbsp",
    "paid",
    "paid ad",
    "paired",
    "paired development",
    "palm",
    "palm beach",
    "palo",
    "palo alto",
    "parents",
    "parents caregivers",
    "parents unfinished",
    "parkway",
    "parkway mountain",
    "parsons",
    "parsons application",
    "parsons job",
    "participant",
    "participant support",
    "participate",
    "participate agency",
    "participates",
    "participates design",
    "particular",
    "particular role",
    "partner",
    "partner alexis",
    "partner deloitte",
    "partner employability",
    "partner forces",
    "partner job",
    "partner monday",
    "partner technical",
    "partnerforces",
    "partnerforces com",
    "passcode",
    "passcode e6uj92sw",
    "passcode jr2cr3gg",
    "passes",
    "passes forgot",
    "passionate",
    "passionate solve",
    "password",
    "password mypassword",
    "passwords",
    "passwords popular",
    "passwords used",
    "past",
    "past months",
    "past police",
    "paste",
    "paste link",
    "patch",
    "patch patching",
    "patches",
    "patches antivirus",
    "patches update",
    "patching",
    "patching closes",
    "patching keeping",
    "path",
    "path cisco",
    "path includes",
    "path level",
    "paths",
    "paths future",
    "patience",
    "patience regarding",
    "patience work",
    "patronas",
    "patronas https",
    "pattern",
    "pattern emerged",
    "pay",
    "pay range",
    "pay use",
    "paying",
    "paying 30k",
    "payment",
    "payment fraud",
    "payment services",
    "payments",
    "payments professional",
    "pbk6",
    "pbk6 jc3qx7jxz3kj6l18th",
    "pcaob",
    "pcaob job",
    "pcfh92gyd7coupgixtql73kcmgzvxykgcx",
    "pcfh92gyd7coupgixtql73kcmgzvxykgcx jlpvxci4",
    "pci",
    "pci excellent",
    "pci strong",
    "pdf",
    "pdf best",
    "pdf br",
    "pdf google",
    "pen",
    "pen verdict",
    "pending",
    "pending unanimous",
    "penetration",
    "penetration testing",
    "pentagon",
    "pentagon long",
    "pentester",
    "pentester know",
    "people",
    "people best",
    "people did",
    "people got",
    "people job",
    "people seek",
    "peraton",
    "peraton 1875",
    "peraton application",
    "peraton careers",
    "peraton click",
    "peraton com",
    "peraton positions",
    "peraton recently",
    "peraton talent",
    "percent",
    "percent don",
    "perfect",
    "perfect containerized",
    "perfect fast",
    "perfect fit",
    "perfect star",
    "perfectly",
    "perfectly good",
    "perform",
    "perform backups",
    "perform day",
    "perform network",
    "performed",
    "performed total",
    "performing",
    "performing backups",
    "performing tech",
    "performing threat",
    "perimeter",
    "perimeter security",
    "period",
    "period borrower",
    "period prior",
    "permanent",
    "permanent time",
    "permission",
    "permission state",
    "permit",
    "permit recording",
    "person",
    "person addressed",
    "person authorized",
    "person closing",
    "person internet",
    "person interview",
    "person regarding",
    "personal",
    "personal confidential",
    "personal correspondence",
    "personal data",
    "personal join",
    "personal life",
    "personal note",
    "personal relationship",
    "personal zoom",
    "personally",
    "personally reach",
    "personnel",
    "personnel tech",
    "pertinent",
    "pertinent network",
    "pete",
    "pete hegseth",
    "petition",
    "petition force",
    "pey1yovfalu2cewbedxicsqg0hfa5_dun0",
    "pey1yovfalu2cewbedxicsqg0hfa5_dun0 http",
    "ph",
    "ph https",
    "philosophy",
    "philosophy hiring",
    "phishing",
    "phishing authentication",
    "phone",
    "phone 540",
    "phone 571",
    "phone 954",
    "phone company",
    "phone conference",
    "phone details",
    "phone interview",
    "phone learn",
    "phone lines",
    "php",
    "php uid",
    "phrase",
    "phrase certifications",
    "pick",
    "pick configure",
    "pick left",
    "pick technology",
    "pictures",
    "pictures binaries",
    "piece",
    "piece hardware",
    "piled",
    "piled reilly",
    "pin",
    "pin https",
    "pinching",
    "pinching promise",
    "pipeline",
    "pipeline lets",
    "pivot",
    "pivot role",
    "pjik4",
    "pjik4 linkedin",
    "place",
    "place saw",
    "place work",
    "plain",
    "plain english",
    "plainsboro",
    "plainsboro nj",
    "plan",
    "plan deploy",
    "plan evil",
    "plan government",
    "plan items",
    "plan northwesternmutual",
    "planned",
    "planned meeting",
    "planning",
    "planning ai",
    "plans",
    "plans role",
    "plans ssp",
    "plans timing",
    "plant",
    "plant relationships",
    "platform",
    "platform display",
    "platform https",
    "platform nbsp",
    "platform perfect",
    "platform simplifies",
    "platforms",
    "platforms disney",
    "platforms initiatives",
    "playing",
    "playing new",
    "playlist",
    "playlist https",
    "plaza",
    "plaza ii",
    "pleasanton",
    "pleasanton ca",
    "pleasanton california",
    "pledge",
    "pledge new",
    "pledged",
    "pledged sign",
    "plus",
    "plus fixed",
    "pm",
    "pm et",
    "pm https",
    "pm kaver68",
    "pm mary",
    "pm pt",
    "pm shivangi",
    "pm utc",
    "podcast",
    "podcast interview",
    "point",
    "point advocacy",
    "point effort",
    "point radius",
    "point salt",
    "point time",
    "pointed",
    "pointed russia",
    "police",
    "police department",
    "police inquiries",
    "policies",
    "policies procedures",
    "policies women",
    "policy",
    "policy click",
    "policy explains",
    "policy f03bf92035c9",
    "policy https",
    "policy intrusion",
    "policy mcb",
    "policy medium",
    "policy performed",
    "policy required",
    "politicization",
    "politicization massachusetts",
    "pollinator",
    "pollinator plant",
    "poly",
    "poly microsoft",
    "pop",
    "pop server",
    "pop time",
    "pop3",
    "pop3 email",
    "pop3 sure",
    "popped",
    "popped worked",
    "popular",
    "port",
    "position",
    "position 2007",
    "position applied",
    "position claroty",
    "position concept",
    "position dear",
    "position hiring",
    "position learn",
    "position longer",
    "position match",
    "position microsoft",
    "position partner",
    "position reach",
    "position simventions",
    "position spectrum",
    "position splunk",
    "positions",
    "positions great",
    "possible",
    "post",
    "post web",
    "posted",
    "posted regularly",
    "posts",
    "posts support",
    "potential",
    "power",
    "powered",
    "powered jobvite",
    "powershell",
    "practical",
    "practice",
    "practice partner",
    "practices",
    "precheck",
    "precheck telos",
    "preferences",
    "preferences click",
    "preferences powered",
    "preferred",
    "preferred skills",
    "prep",
    "prep sheet",
    "prepare",
    "president",
    "president trump",
    "presidential",
    "presidential library",
    "prevent",
    "prevention",
    "prevention systems",
    "pricewaterhousecoopers",
    "pricewaterhousecoopers advisory",
    "prime",
    "prime friday",
    "principal",
    "principal cybersecurity",
    "principal threat",
    "prior",
    "privacy",
    "privacy learn",
    "privacy notice",
    "privacy notices",
    "privacy policy",
    "privacy statement",
    "private",
    "probably",
    "problem",
    "problem solving",
    "problems",
    "procedures",
    "process",
    "process complete",
    "process completed",
    "process evaluating",
    "process follow",
    "process government",
    "process job",
    "process meet",
    "process schedule",
    "process thank",
    "processes",
    "processes mitigate",
    "product",
    "products",
    "professional",
    "professional cpp",
    "professionals",
    "professionals https",
    "professor",
    "professor location",
    "proficiency",
    "profile",
    "profile accurate",
    "profile key",
    "profile like",
    "profile relation",
    "profile tells",
    "program",
    "program director",
    "program management",
    "program manager",
    "programs",
    "progression",
    "progression path",
    "project",
    "projects",
    "prompts",
    "prompts help",
    "proofpoint",
    "proofpoint received",
    "properly",
    "properly recommend",
    "properties",
    "properties located",
    "property",
    "protect",
    "protecting",
    "protocol",
    "protocols",
    "protocols layers",
    "protocols security",
    "proudly",
    "proudly supported",
    "proven",
    "provide",
    "provide accommodation",
    "provide days",
    "provided",
    "provider",
    "providing",
    "providing multiple",
    "psettings",
    "psettings email",
    "pt",
    "public",
    "puerto",
    "puerto rico",
    "pure",
    "pure net",
    "pursue",
    "pursue candidates",
    "python",
    "python bash",
    "python plain",
    "q3",
    "q3 null",
    "q3 otptoken",
    "qs",
    "qualifications",
    "question",
    "questions",
    "questions contact",
    "questions reply",
    "questions requests",
    "quickly",
    "quickly providing",
    "raaberg",
    "raaberg oversees",
    "rack",
    "radgov",
    "radgov com",
    "radius",
    "rahman",
    "rahman https",
    "rahman12",
    "rahman12 source",
    "randolph",
    "randolph college",
    "range",
    "rapid7",
    "rate",
    "rate confirmation",
    "reach",
    "reach new",
    "reach schedule",
    "reaching",
    "read",
    "read microsoft",
    "reader",
    "reader 266711b938df",
    "reader 440100e76000",
    "reader 78073def27b8",
    "reader 78d064101951",
    "reader 8993e01dcfd3",
    "reader b8ca787b_6c91_45ec_8cc7_8f066e74c560",
    "reader eb8e1bd46a6c",
    "readers",
    "reading",
    "reading adrian",
    "real",
    "really",
    "really enjoyed",
    "recall",
    "recall correctly",
    "receive",
    "receive auto",
    "receive communications",
    "receive confirmation",
    "receive emails",
    "receive feedback",
    "receive mail",
    "receive new",
    "received",
    "received application",
    "received ask",
    "received message",
    "received senior",
    "receiving",
    "receiving email",
    "receiving linkedin",
    "recent",
    "recent application",
    "recipient",
    "recipient named",
    "recommend",
    "recommend adding",
    "recommend checking",
    "recommendations",
    "record",
    "recorded",
    "recorded future",
    "recruit",
    "recruit behalf",
    "recruiter",
    "recruiting",
    "recruiting application",
    "recruiting mail",
    "recruiting platform",
    "recruiting process",
    "recruiting team",
    "recruitment",
    "red",
    "red team",
    "redirect",
    "redmond",
    "redmond wa",
    "references",
    "references visible",
    "refid",
    "regarding",
    "regarding application",
    "regarding job",
    "regards",
    "regards andrew",
    "regards claroty",
    "regards mary",
    "regards recorded",
    "regards talent",
    "register",
    "registered",
    "registered starts",
    "registered trademarks",
    "regret",
    "regret inform",
    "regularly",
    "regularly thank",
    "regulatory",
    "related",
    "related experience",
    "related field",
    "related security",
    "relation",
    "relation job",
    "releases",
    "relevant",
    "remarks",
    "remediating",
    "remediating security",
    "remediation",
    "remember",
    "reminder",
    "reminder standout",
    "remote",
    "remote job",
    "remote usa",
    "reply",
    "reply continue",
    "reply directly",
    "reply email",
    "reply jobvite",
    "reply message",
    "reply microsoft",
    "report",
    "reportedly",
    "representative",
    "representatives",
    "request",
    "request availability",
    "request let",
    "requests",
    "requests receive",
    "require",
    "required",
    "required atleast",
    "required knowledge",
    "required levels",
    "required years",
    "requirements",
    "requirements make",
    "requiring",
    "research",
    "researcher",
    "researcher east",
    "researcher job",
    "reserved",
    "reserved email",
    "reserved references",
    "resolution",
    "resolution incidents",
    "resolve",
    "resources",
    "respective",
    "respects",
    "respects privacy",
    "respond",
    "responder",
    "response",
    "response interview",
    "response reply",
    "responses",
    "responsibilities",
    "responsibility",
    "responsible",
    "reston",
    "reston va",
    "results",
    "results specify",
    "resume",
    "resume important",
    "resume reviewed",
    "retirement",
    "retirement process",
    "return",
    "return application",
    "return job",
    "reversal",
    "review",
    "review application",
    "review process",
    "review use",
    "reviewed",
    "reviews",
    "reviews application",
    "rgovus",
    "richmond",
    "richmond va",
    "rico",
    "rights",
    "rights reserved",
    "risk",
    "risk assessment",
    "risk claroty",
    "risks",
    "risks associated",
    "risks enterprise",
    "road",
    "robotics",
    "robotics total",
    "role",
    "role appreciate",
    "role google",
    "role happen",
    "role lead",
    "role ll",
    "role notice",
    "roles",
    "roles fit",
    "roles like",
    "roles match",
    "roles watch",
    "room",
    "routh",
    "routine",
    "rtr",
    "rtr rate",
    "rtr remote",
    "run",
    "running",
    "rural",
    "rural virginia",
    "russia",
    "s3n_mgezo1s6i1zykhysghlrauh",
    "s3n_mgezo1s6i1zykhysghlrauh db",
    "s_vjzuylowtfwibo7fx7eomuob0xju3dqp2rvj",
    "s_vjzuylowtfwibo7fx7eomuob0xju3dqp2rvj m6rbkm",
    "sabre",
    "sabre systems",
    "safe",
    "safe sender",
    "safe senders",
    "said",
    "save",
    "saw",
    "say",
    "saying",
    "sbstck",
    "sbstck com",
    "scans",
    "schedule",
    "schedule initial",
    "schedule interview",
    "schedule phone",
    "scheduled",
    "scheduled time",
    "scheduled tsa",
    "schemes",
    "school",
    "school alumni",
    "school science",
    "science",
    "science collective",
    "science engineering",
    "science job",
    "science tenure",
    "scientists",
    "screen",
    "screen reader",
    "scribble",
    "scribble 1971",
    "scripting",
    "scripting python",
    "search",
    "search internet",
    "search jobs",
    "search regards",
    "search results",
    "seattle",
    "seattle wa",
    "seattle washington",
    "secretary",
    "secure",
    "security",
    "security analyst",
    "security architect",
    "security authentication",
    "security controls",
    "security deception",
    "security engineer",
    "security gateways",
    "security operations",
    "security perimeter",
    "security policy",
    "security researcher",
    "security risks",
    "security security",
    "security specialist",
    "security technologies",
    "security tools",
    "security windows",
    "securityhelp",
    "securityhelp textfooterglimmer",
    "seeing",
    "seek",
    "seek make",
    "seeking",
    "seeking applicat",
    "seeking experienced",
    "sei",
    "sei application",
    "selected",
    "selected interview",
    "selected role",
    "selecting",
    "selecting online",
    "selling",
    "send",
    "send email",
    "sender",
    "sender list",
    "senders",
    "senders _____",
    "sending",
    "sendmail",
    "senior",
    "senior cyber",
    "senior cybersecurity",
    "senior lead",
    "senior machine",
    "senior manager",
    "senior security",
    "senior threat",
    "sent",
    "sent kaver68",
    "sent kelly",
    "sent spark",
    "sent tiktok",
    "sent unmonitored",
    "sent virginia",
    "sentinelone",
    "sep",
    "separately",
    "separately think",
    "september",
    "september 2025",
    "serial",
    "server",
    "server room",
    "servers",
    "service",
    "service consider",
    "services",
    "services google",
    "services llc",
    "services subsidiary",
    "set",
    "set job",
    "settings",
    "settings df3e28e4ae5d",
    "share",
    "share information",
    "share updated",
    "shared",
    "sharing",
    "sharp",
    "shaw",
    "shaw 434",
    "shaw cell",
    "shaw dsc",
    "shaw job",
    "shaw mra",
    "shaw thank",
    "sheet",
    "shift",
    "shift based",
    "shifting",
    "shifting immediate",
    "shivangi",
    "shivangi goswami",
    "shivangi tekintegral",
    "short",
    "showing",
    "shown",
    "shutdown",
    "siem",
    "sign",
    "sign document",
    "sign job",
    "signal",
    "signature",
    "signature needed",
    "significant",
    "signing",
    "similar",
    "similar jobs",
    "simventions",
    "simventions application",
    "simventions fredericksburg",
    "simventions recruiting",
    "simventions thrilled",
    "sincerely",
    "sincerely antisyphon",
    "sincerely ieee",
    "sincerely kelly",
    "sincerely simventions",
    "sincerely tyler",
    "sipc",
    "site",
    "site hybrid",
    "site privacy",
    "size",
    "size 16px",
    "skill",
    "skills",
    "skills abilities",
    "skills experience",
    "skills preferred",
    "slots",
    "slots available",
    "smart",
    "sme",
    "sme position",
    "soaftpfvfm6pl9mnho7vfhath0fcuhmarkff3f",
    "soc",
    "social",
    "social media",
    "society",
    "society web",
    "software",
    "software anti",
    "software meantime",
    "sold",
    "sold appropriately",
    "sold isp",
    "solutions",
    "solving",
    "soon",
    "soon review",
    "source",
    "source email",
    "sox",
    "sox pci",
    "spam",
    "span",
    "span br",
    "span style",
    "spark",
    "spark hire",
    "speak",
    "speak really",
    "special",
    "special limited",
    "specialist",
    "specialist cams",
    "specialist tekintegral",
    "species",
    "specific",
    "specific application",
    "specific step",
    "specify",
    "specify want",
    "spectrum",
    "spectrum cyber",
    "speech",
    "speed",
    "speeds",
    "splunk",
    "splunk career",
    "splunk com",
    "splunk message",
    "splunk talent",
    "springfield",
    "springfield va",
    "sr",
    "sr dfir",
    "sr talent",
    "sse",
    "sse description",
    "stack",
    "staff",
    "staff cyber",
    "stakeholders",
    "standards",
    "standards develop",
    "standards nerc",
    "standout",
    "standout summit",
    "standoutsummit",
    "standoutsummit heysummit",
    "star",
    "start",
    "start application",
    "started",
    "started job",
    "starting",
    "starting application",
    "starting hour",
    "starts",
    "starts hour",
    "state",
    "state means",
    "state new",
    "state va",
    "statement",
    "states",
    "states connections",
    "states https",
    "status",
    "status application",
    "status cyber",
    "status senior",
    "status sincerely",
    "status update",
    "status updates",
    "stay",
    "stay date",
    "stay touch",
    "step",
    "step getting",
    "step intelligence",
    "steps",
    "steps sincerely",
    "steps success",
    "stolen",
    "stolen keys",
    "stop",
    "store",
    "store share",
    "stories",
    "story",
    "story encourage",
    "strategic",
    "streaming",
    "street",
    "street reston",
    "strive",
    "strong",
    "strong analytical",
    "strong background",
    "strong build",
    "strong knowledge",
    "strongly",
    "studies",
    "style",
    "style font",
    "style italic",
    "subject",
    "subject matter",
    "submission",
    "submission details",
    "submission lead",
    "submit",
    "submitted",
    "submitted senior",
    "subscribe",
    "subscribe free",
    "subscribers",
    "subscribing",
    "subscribing cloudflare",
    "subsidiary",
    "subsidiary nm",
    "substack",
    "substack com",
    "substack subscribe",
    "substitute",
    "substitute years",
    "success",
    "success career",
    "success thank",
    "success view",
    "successfully",
    "successfully submitted",
    "suite",
    "summit",
    "summit 2025",
    "sunnyvale",
    "sunnyvale ca",
    "support",
    "support antisyphontraining",
    "support circia",
    "support google",
    "support microsoft",
    "support work",
    "supported",
    "supported changed",
    "sure",
    "sure information",
    "sure profile",
    "survey",
    "switch",
    "syncing",
    "syncing prime",
    "systems",
    "systems automated",
    "systems log",
    "systems security",
    "t1",
    "t8yub4yhcjscaa_sk7m4zs9op2npmen5hjinvtx6vug4nkjaieg4y7t31ojs8k6bszdswh9tmnadvtfl2oqxrtzcqra",
    "t8yub4yhcjscaa_sk7m4zs9op2npmen5hjinvtx6vug4nkjaieg4y7t31ojs8k6bszdswh9tmnadvtfl2oqxrtzcqra ezgzmtdnv5ar",
    "taken",
    "taking",
    "taking time",
    "talent",
    "talent acquisition",
    "talent community",
    "talented",
    "talented motivated",
    "talk",
    "talk click",
    "talk issues",
    "talk note",
    "talk page",
    "talk registered",
    "talks",
    "target",
    "tb",
    "tcp",
    "tcp ip",
    "team",
    "team _____",
    "team automated",
    "team blog",
    "team confirm",
    "team kelly",
    "team mantech",
    "team need",
    "team note",
    "team proudly",
    "team review",
    "team reviews",
    "team sent",
    "team threat",
    "team view",
    "teaming",
    "teaming partner",
    "teams",
    "teams microsoft",
    "teams need",
    "tech",
    "tech application",
    "tech wizards",
    "technical",
    "technical non",
    "technical program",
    "technician",
    "techniques",
    "technologies",
    "technologies careful",
    "technologies hi",
    "technologies received",
    "technologies recruiting",
    "technologies senior",
    "technology",
    "tekintegral",
    "tekintegral com",
    "tell",
    "telling",
    "tells",
    "tells story",
    "telos",
    "telos application",
    "tenantid",
    "tenure",
    "tenure track",
    "term",
    "term care",
    "terms",
    "terms service",
    "testing",
    "testing red",
    "testing tools",
    "text",
    "textfooterglimmer",
    "textfooterglimmer null",
    "textfooterglimmer trkemail",
    "thank",
    "thank application",
    "thank applying",
    "thank career",
    "thank director",
    "thank microsoft",
    "thank peraton",
    "thank recent",
    "thank starting",
    "thank taking",
    "thank tyler",
    "thank venture",
    "thank working",
    "thanks",
    "thanks applying",
    "thanks best",
    "thanks cloudflare",
    "thanks hope",
    "thanks jobscan",
    "thanks patience",
    "thanks reading",
    "thanks starting",
    "thanks venture",
    "thanks yesterday",
    "thing",
    "thing profile",
    "things",
    "think",
    "think match",
    "thinking",
    "thisisicf",
    "thought",
    "thread",
    "thread v2",
    "threadid",
    "threat",
    "threat hunter",
    "threat intel",
    "threat intelligence",
    "threat researcher",
    "threat vulnerability",
    "threats",
    "thrilled",
    "thrilled interested",
    "thrives",
    "tied",
    "tied contract",
    "ties",
    "tiktok",
    "time",
    "time apply",
    "time consider",
    "time coordinated",
    "time encourage",
    "time https",
    "time interview",
    "time job",
    "time offer",
    "time regards",
    "time slots",
    "time speak",
    "time teaming",
    "time ve",
    "timelines",
    "times",
    "times different",
    "times normal",
    "timing",
    "tips",
    "title",
    "title computer",
    "title senior",
    "tjl_lksrzcuuzlusc_knewh",
    "tjl_lksrzcuuzlusc_knewh 8l8tfvd1td83a_ryro9efhiefh74u8o1j8ojkyf_1a6do6ppimw6ieeqyte_leuw0rysvbotw9kbkh7hhnejrihphfherhvjrehplyr6lpcz8d9vnf_r_ou2wilg7sdx7av32lzxfdnuwh2ua9lctc6hp660bgs3x172yjzzk3y5vrlx0os0tptuiwzd8tyyyenqu8ejoeg0jvbgz7b2bpgs5ifq1zwytm0kcv7ywohgh1k",
    "today",
    "todero",
    "todero sent",
    "todero sr",
    "told",
    "tomorrow",
    "took",
    "tool",
    "tools",
    "tools firewalls",
    "tools help",
    "total",
    "total control",
    "touch",
    "tower",
    "track",
    "track faculty",
    "track job",
    "track record",
    "tracking",
    "tracking click",
    "tracking icims",
    "tracking mycareersource",
    "tracking unsubscribe",
    "trackingid",
    "trademarks",
    "trademarks linkedin",
    "training",
    "training support",
    "trial",
    "tried",
    "trk",
    "trk eml",
    "trkemail",
    "trkemail eml",
    "troubleshooting",
    "trump",
    "trump adopts",
    "trumpet",
    "trumpet winsock",
    "trust",
    "trust services",
    "trying",
    "tsa",
    "tsa precheck",
    "tuesday",
    "tuesday interview",
    "tuning",
    "tuning cloudflare",
    "tv",
    "tv https",
    "tv live",
    "twitter",
    "twitter com",
    "twitter facebook",
    "twitter https",
    "tx",
    "tyler",
    "tyler technologies",
    "type",
    "type social",
    "u001",
    "u001 soaftpfvfm6pl9mnho7vfhath0fcuhmarkff3f",
    "ukraine",
    "ul",
    "ul li",
    "ultimately",
    "umkc",
    "umkc sse",
    "unattended",
    "unattended help",
    "understanding",
    "unfortunately",
    "unfortunately means",
    "unfortunately moved",
    "united",
    "united states",
    "university",
    "university missouri",
    "unix",
    "unix mac",
    "unmonitored",
    "unmonitored mailbox",
    "unsubscribe",
    "unsubscribe future",
    "unsubscribe https",
    "unsubscribe lipi",
    "unsubscribe textfooterglimmer",
    "unsubscribe time",
    "update",
    "update email",
    "update import",
    "update leidos",
    "update senior",
    "updated",
    "updated resume",
    "updates",
    "updates action",
    "upn",
    "upn u001",
    "urgent",
    "url",
    "url br",
    "url button",
    "urn",
    "urn 3ali",
    "usa",
    "usa 32661",
    "usage",
    "usage focus",
    "use",
    "use ai",
    "use copy",
    "use information",
    "use recipient",
    "use store",
    "used",
    "usenet",
    "usenet feed",
    "usenet pop3",
    "usenet time",
    "user",
    "user account",
    "user framed",
    "user set",
    "user setup",
    "usernames",
    "usernames started",
    "users",
    "users file",
    "users request_unsubscribe",
    "using",
    "using 568b",
    "using cleverly",
    "using commands",
    "using data",
    "using email",
    "using gui",
    "using href",
    "using information",
    "using isn",
    "using makeshift",
    "using mra",
    "using personal",
    "using specific",
    "using weapons",
    "usp",
    "usp pstnconferencing",
    "usp sharing",
    "ut",
    "ut va",
    "utc",
    "utc view",
    "utm_medium",
    "utm_medium email",
    "utm_source",
    "utm_source availabilityrequest",
    "utm_source gm",
    "uva",
    "uva graduate",
    "uvfxc",
    "uvfxc youtube",
    "uyffof4lktqgx_ybrzq",
    "uyffof4lktqgx_ybrzq build",
    "uzzzaw",
    "v0_vfyynn",
    "v0_vfyynn 2flrv3egjxar",
    "v2",
    "v2 context",
    "v2 messageid",
    "v2ishycxgyoetmqhsa0h65xgskjx3zds",
    "v2ishycxgyoetmqhsa0h65xgskjx3zds wgenzycoj7kqran0afly3q1ygcwcxj8cidv5hm8ld_s52yfmju_kll2q4mkhzahva_v4dop7erfes4cejrhrmrfguo2mu",
    "va",
    "va 20190",
    "va 20194",
    "va 22485",
    "va 23219",
    "va connections",
    "va cybersecurity",
    "va richmond",
    "va site",
    "va usa",
    "va view",
    "va vt",
    "va6zr7jv8d2r2s15lrlt3nscv0xi1wm3gbcqqdinqgviij7lgoeallc1ygqd78r51rsfcwtqvg",
    "va6zr7jv8d2r2s15lrlt3nscv0xi1wm3gbcqqdinqgviij7lgoeallc1ygqd78r51rsfcwtqvg 169",
    "vacation",
    "vacation complete",
    "valuable",
    "valuable game",
    "valuation",
    "valuation models",
    "value",
    "value fearless",
    "value inclusiveness",
    "value lien",
    "value resulting",
    "vanish",
    "vanish bank",
    "various",
    "various fields",
    "various functions",
    "various stakeholders",
    "vast",
    "vast say",
    "vb_jfobvclx14p0g2zhbrqpm",
    "vb_jfobvclx14p0g2zhbrqpm pzadidqaoemovipsmebt_qetgiawsoc_spb_l78lzuhow",
    "vdapegjipb6mrmr1b4lsh0cy9qoowdssuxosmzof0iuykpcy6ulou2ti5vzaimqhjacfyys63jhvkroqtwpp4ok2ug8rrqj2xlgpdc9x",
    "vdapegjipb6mrmr1b4lsh0cy9qoowdssuxosmzof0iuykpcy6ulou2ti5vzaimqhjacfyys63jhvkroqtwpp4ok2ug8rrqj2xlgpdc9x thank",
    "vdot",
    "vdot seeking",
    "ve",
    "ve applied",
    "ve coached",
    "ve completed",
    "ve enjoyed",
    "ve figured",
    "ve got",
    "ve government",
    "ve held",
    "ve identified",
    "ve job",
    "ve looked",
    "ve received",
    "ve sat",
    "ve spent",
    "ve started",
    "ve submitted",
    "ve time",
    "ve wondered",
    "ve work",
    "veered",
    "veered denial",
    "vendor",
    "vendor manuals",
    "vendors",
    "vendors cisco",
    "vendors effectively",
    "vendors selling",
    "venture",
    "venture global",
    "venturegloballng",
    "venturegloballng com",
    "verbal",
    "verbal communication",
    "verdict",
    "verdict prosecutors",
    "verification",
    "verification income",
    "verification property",
    "verified",
    "verified income",
    "verizon",
    "verizon application",
    "version",
    "version 98",
    "version browsers",
    "versus",
    "versus assisting",
    "verylazytech",
    "verylazytech source",
    "vf2st6c3uuvync4udymarcvi6j6xow",
    "vf2st6c3uuvync4udymarcvi6j6xow 12",
    "vfduuhx2nfdthm9cuuzcxfex4fu5pojsfucfcwdbb1u",
    "vfduuhx2nfdthm9cuuzcxfex4fu5pojsfucfcwdbb1u http",
    "vice",
    "vice president",
    "victory",
    "victory notable",
    "video",
    "video interview",
    "video playlist",
    "video shows",
    "view",
    "view 4099138096",
    "view 4137400442",
    "view 4168212582",
    "view 4169006422",
    "view 4283296317",
    "view 4285785299",
    "view 4295224223",
    "view application",
    "view ca",
    "view job",
    "view local",
    "view post",
    "view sign",
    "view similar",
    "view status",
    "view talk",
    "view_job",
    "view_job null",
    "view_job trkemail",
    "virginia",
    "virginia story",
    "virginia tech",
    "virginia usa",
    "virus",
    "virus software",
    "visibility",
    "visibility analysis",
    "visible",
    "visible links",
    "visit",
    "voice",
    "voice google",
    "vote",
    "vourakis",
    "vulnerabilities",
    "vulnerability",
    "vulnerability management",
    "vulnerability visibility",
    "w2",
    "wa",
    "wa 98052",
    "wa optimize",
    "wait",
    "want",
    "want help",
    "want receive",
    "wanted",
    "wanted let",
    "war",
    "washington",
    "washington dc",
    "washington hybrid",
    "washington local",
    "watch",
    "watch floor",
    "watching",
    "watching disaster",
    "water",
    "water reservoirs",
    "way",
    "way discovered",
    "way ispcon",
    "way profile",
    "way redmond",
    "way saying",
    "ways",
    "ways process",
    "ways signal",
    "wd5",
    "wd5 myworkdayjobs",
    "wdbtwmyydvk9jcf5v2uyvxhxid1jdylae_de",
    "wdbtwmyydvk9jcf5v2uyvxhxid1jdylae_de http",
    "wealth",
    "wealth management",
    "weapons",
    "weapons everybody",
    "weapons white",
    "wearecisco",
    "wearecisco community",
    "web",
    "web app",
    "web applications",
    "web browser",
    "web cam",
    "web content",
    "web frameworks",
    "web https",
    "web network",
    "web rare",
    "web related",
    "web scribble",
    "web security",
    "web server",
    "web servers",
    "web www",
    "website",
    "website twitter",
    "wednesday",
    "wednesday like",
    "week",
    "week calendar",
    "week today",
    "weekend",
    "weekend solopreneurs",
    "weekends",
    "weekends vanish",
    "weekly",
    "weekly digest",
    "weekly monthly",
    "weeks",
    "weeks talent",
    "weighs",
    "weighs factors",
    "weight",
    "weight 700",
    "welcome",
    "welcome candidates",
    "went confidently",
    "wesley",
    "wesley routh",
    "west",
    "west 6th",
    "west early",
    "west maude",
    "west palm",
    "west set",
    "western",
    "western ave",
    "white",
    "white house",
    "wi",
    "wide inch",
    "wild",
    "wild west",
    "willing",
    "willing consider",
    "willing make",
    "willing relocate",
    "win",
    "win handy",
    "winback",
    "winback email",
    "windows",
    "windows 95",
    "windows cmd",
    "windows complete",
    "windows linux",
    "windows support",
    "winners",
    "winners aren",
    "winsock",
    "winsock floppy",
    "winsock used",
    "wire",
    "wire rj",
    "wireless",
    "wireless broadband",
    "wisconsin",
    "wisconsin ave",
    "wish",
    "wish best",
    "wish opt",
    "wish receive",
    "wish say",
    "wish success",
    "wish unsubscribe",
    "wish used",
    "wishes",
    "wishes splunk",
    "withdraw",
    "withdraw consent",
    "withdrew",
    "withdrew consideration",
    "wizards",
    "wizards applicantstack",
    "wizards com",
    "wizards company",
    "wizards incorporated",
    "wizards job",
    "women",
    "won",
    "won moving",
    "word",
    "word pdf",
    "word phrase",
    "words",
    "words local",
    "words report",
    "work",
    "work agency",
    "work excited",
    "work foreign",
    "work hours",
    "work instead",
    "work medium",
    "work possible",
    "work process",
    "work professor",
    "work related",
    "work set",
    "work styles",
    "work unsubscribe",
    "workday",
    "workday description",
    "workday opening",
    "worked",
    "worked cloud",
    "worked isp",
    "worked prep",
    "workflows",
    "workflows charters",
    "workflows delivery",
    "workflows structured",
    "working",
    "working claroty",
    "working knowledge",
    "working subscribing",
    "working thousand",
    "working time",
    "working tough",
    "workloads",
    "workloads like",
    "workplaces",
    "workplaces lgbtq",
    "workplaces parents",
    "workplaces young",
    "works",
    "works 2025",
    "works available",
    "works closely",
    "works don",
    "works interview",
    "workshop",
    "workshop job",
    "workshop jobscan",
    "world",
    "world coming",
    "world dial",
    "worth",
    "writing",
    "written",
    "wrote",
    "www",
    "www cloudflare",
    "www facebook",
    "www google",
    "www icf",
    "www linkedin",
    "www northwesternmutual",
    "www radgov",
    "xtvtwipcbpcs ff",
    "xtvuivllykem",
    "xtvuivllykem 00",
    "yang",
    "yang jingyuan",
    "ybd6fe5gmtvxrju7tmnuuhldf6ebtjbjo_ewvj8oupqyj1khg7ekrdk7nun3yu0ldtsi0lty52dtz51pz5myu7mpa0bdwvs_i",
    "ybd6fe5gmtvxrju7tmnuuhldf6ebtjbjo_ewvj8oupqyj1khg7ekrdk7nun3yu0ldtsi0lty52dtz51pz5myu7mpa0bdwvs_i https",
    "year",
    "year bachelor",
    "year benefits",
    "year engagement",
    "year hire",
    "year master",
    "year old",
    "year probably",
    "year thanks",
    "years",
    "years associate",
    "years bachelor",
    "years certified",
    "years cissp",
    "years considerable",
    "years demonstrated",
    "years experience",
    "years https",
    "years later",
    "years months",
    "years old",
    "years progressive",
    "years related",
    "years solid",
    "years ve",
    "yes",
    "yes asked",
    "yesterday",
    "yesterday love",
    "yfuukr6212tcd2lv36pqdq",
    "yfuukr6212tcd2lv36pqdq aaib5ha",
    "yggdrasil",
    "yggdrasil https",
    "yibbyhmexnqbejg8ixmaxxpmstwsiwptnn6p2mwxjbkimcvik6yjg_q",
    "yibbyhmexnqbejg8ixmaxxpmstwsiwptnn6p2mwxjbkimcvik6yjg_q linkedin",
    "yksanual4ofjxg__vryhvxbj12lcxh4_4p4rc41cdnuxqiwbu2nzmcxvijbcxzedog",
    "yksanual4ofjxg__vryhvxbj12lcxh4_4p4rc41cdnuxqiwbu2nzmcxvijbcxzedog mvbwdbxocafboxlf_qlt",
    "ylwersvrrzu0lojz",
    "ylwersvrrzu0lojz 2bmynmw",
    "yom",
    "yom ayom",
    "york",
    "york 12203",
    "york times",
    "york wait",
    "young",
    "young networking",
    "young professionals",
    "youngest",
    "youngest child",
    "youtube",
    "youtube com",
    "youtube details",
    "youtube https",
    "youtube questions",
    "yxzr4nax6l7f5dlvaywuuowka5f_lfxtvj",
    "yxzr4nax6l7f5dlvaywuuowka5f_lfxtvj nrqzgnf089zhd1yb7rac3c5jpxm0",
    "z1frtvwtzjqc4gohvgmk2waoflshd8xcjm2bexouflkicbxzgx1z0ykcaylb_x9jx37bpvmxkihg",
    "z1frtvwtzjqc4gohvgmk2waoflshd8xcjm2bexouflkicbxzgx1z0ykcaylb_x9jx37bpvmxkihg available",
    "z6cipep5onao_qhou58qcaabowsudzll2thsdewv6nsri79nk8ksj10uhssdosxe0",
    "z6cipep5onao_qhou58qcaabowsudzll2thsdewv6nsri79nk8ksj10uhssdosxe0 narrow",
    "zahin",
    "zahin nazeer",
    "zelenskyy",
    "zelenskyy general",
    "zero",
    "zero day",
    "zfah3zwpzxrm",
    "zfah3zwpzxrm hnctxpe7zzf7jkaznilubfangjp9gwn",
    "zippy",
    "zippy 2cchoose",
    "zmxfkgfbvgk6vig8vmrq9a6izo",
    "zmxfkgfbvgk6vig8vmrq9a6izo a_ylgwonzs3ehklppd9nyesycyvc1x3axz6ulwe",
    "zmxymtro8jnnas1vc8lnvko5rgu2b_rlcjpxrnqglmveyqylrrwlwnsk1migb3mw4q",
    "zmxymtro8jnnas1vc8lnvko5rgu2b_rlcjpxrnqglmveyqylrrwlwnsk1migb3mw4q speakers",
    "zmxymtro8jnnas1vc8lnvko5rgu2b_rlcjpxrnqglmveyqylrrwlwnsk1migb3mw4q tech",
    "znah4an3rxcroiufuqr3bw",
    "znah4an3rxcroiufuqr3bw 3d",
    "znh9ybqse",
    "znh9ybqse uzzzaw",
    "znwritesit",
    "znwritesit source",
    "zoo",
    "zoo uk",
    "zoom",
    "zoom 81153702201",
    "zoom 9703199110",
    "zoom ej",
    "zoom google",
    "zoom interview",
    "zoom join",
    "zp5ho2o_eezfg69jzi2du",
    "zp5ho2o_eezfg69jzi2du kopmsj78ak3qwiojafujtd1uqruxbpjq",
    "zt",
    "zt 2jei"
  ]
}

===== END OF FILE: model/model_info.json =====



===== START OF FILE: out.txt =====

Code Problems:
- Add confidence factor to tracker_company table
- Domain value is not being set in tracker_company table
- tracker_companyalias table is empty
- tracker_atsdomain table is empty
- tracker_knowncompany table is empty
- tracker_application table is empty
- in tracker_message no messages  have been review, meaning they all need to be reviewed.
- when running main and just after a successful sync-complete, prompt for superuser username and password.
- main.py starts of ingestion even when there are no knew messages.
Admin Site Web Problems:
- Companies list shows on admin page, with clickable links, but some companies that I have applied to and have messages have no Applications or Messages.  Some of the others may or may not show any or all of its related messages.
- Under Recent Threads, sometimes du-plicate messages are shown in the thread. -- For example, Thread 196cf35eb6fa0b18 2025-05-14 10:32 ‚Äî leidos Update on Leidos Position Full Spectrum Cyber AI Researcher Label: Intel 2025-05-14 10:32 ‚Äî leidos Update on Leidos Position Full Spectrum Cyber AI Researcher Label: Intel
- Some messages under label_messages have "No Company", " No Body" and a non-zero confidence value. -- For example, Application Status for Cyber Machine Learning Engineer, Senior 2025-07-10 14:16 ‚Äî No Company Body: Confidence: 0.51
Admin Site Upgrades:
- Build a new admin card where we can enter, modify, and update ticket status. This
- Move Ingestion section page to its own card
- Move Companies section to its own card
- Recent Mesages section needs its own card.
After hitting the "Save Labels" button, need to add to the top of the page, # messages reviewed,  #Messages Not reviewed, #Messages that were Noise, #No of Applications,  # rejections, #interviews, #valid companies, #invalid companies, average confidence score.
- These Label_Messages should be shown on the front deck of the dashboard to improve UX


===== END OF FILE: out.txt =====



===== START OF FILE: parser.py =====

# parser.py
import os
import re
import json
import base64
import html
import joblib
from joblib import load
import quopri
import django
from django.utils import timezone
from django.utils.timezone import now
from django.db.models import F
from pathlib import Path
from email.utils import parsedate_to_datetime, parseaddr
from bs4 import BeautifulSoup
from db import insert_email_text, insert_or_update_application, is_valid_company
from datetime import datetime, timedelta
from db_helpers import get_application_by_sender, build_company_job_index
from ml_subject_classifier import predict_subject_type
from ml_entity_extraction import extract_entities
from tracker.models import Application, Message, IgnoredMessage, IngestionStats, Company, UnresolvedCompany


os.environ.setdefault("DJANGO_SETTINGS_MODULE", "dashboard.settings")
django.setup()
DEBUG = True

# --- Load patterns.json ---
PATTERNS_PATH = Path(__file__).parent / "patterns.json"
if PATTERNS_PATH.exists():
    with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
        patterns_data = json.load(f)
    PATTERNS = patterns_data
else:
    PATTERNS = {}

COMPANIES_PATH = Path(__file__).parent / "companies.json"
if COMPANIES_PATH.exists():
    with open(COMPANIES_PATH, "r", encoding="utf-8") as f:
        company_data = json.load(f)
    ATS_DOMAINS = [d.lower() for d in company_data.get("ats_domains", [])]  
    KNOWN_COMPANIES = {c.lower() for c in company_data.get("known", [])}
    DOMAIN_TO_COMPANY = {
        k.lower(): v for k, v in company_data.get("domain_to_company", {}).items()
    }
    ALIASES = company_data.get("aliases", {})
else:
    KNOWN_COMPANIES = set()
    DOMAIN_TO_COMPANY = {}
    ALIASES = {}
    
def strip_html_tags(text: str) -> str:
    if not text:
        return ""
    # BeautifulSoup handles nested tags, entities, script/style removal better than regex
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def get_stats():
    today = now().date()
    stats, _ = IngestionStats.objects.get_or_create(date=today)
    return stats


def decode_part(data, encoding):
    if encoding == "base64":
        return base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")
    elif encoding == "quoted-printable":
        return quopri.decodestring(data).decode("utf-8", errors="ignore")
    elif encoding == "7bit":
        return data  # usually already decoded
    else:
        return data
    
def extract_body(payload):
    if 'parts' in payload:
        for part in payload['parts']:
            if part['mimeType'] == 'text/plain':
                return part['body'].get('data', '')
            elif part['mimeType'] == 'text/html':
                html = part['body'].get('data', '')
                return strip_html_tags(html)
    return payload.get('body', {}).get('data', '')

def extract_body_from_parts(parts):
    for part in parts:
        mime_type = part.get("mimeType")
        body_data = part.get("body", {}).get("data")
        if mime_type == "text/html" and body_data:
            decoded = base64.urlsafe_b64decode(body_data).decode("utf-8", errors="ignore")
            return decoded  # preserve full HTML
        elif "parts" in part:
            result = extract_body_from_parts(part["parts"])
            if result:
                return result
    return ""

def log_ignored_message(msg_id, metadata, reason):
    IgnoredMessage.objects.update_or_create(
        msg_id=msg_id,
        defaults={
            "subject": metadata["subject"],
            "body": metadata["body"],
            "sender": metadata["sender"],
            "sender_domain": metadata["sender_domain"],
            "date": metadata["timestamp"],
            "reason": reason,
        },
    )

def is_valid_company_name(name):
    """Reject company names that match known invalid prefixes from patterns.json."""
    if not name:
        return False

    invalid_prefixes = PATTERNS.get("invalid_company_prefixes", [])
    lowered = name.lower()
    return not any(lowered.startswith(prefix.lower()) for prefix in invalid_prefixes)
PARSER_VERSION = "1.0.0"

# --- Load ML model artifacts at startup ---
# --- Load message-level ML model artifacts at startup ---
try:
    CLASSIFIER = joblib.load("model/message_classifier.pkl")
    VECTORIZER = joblib.load("model/message_vectorizer.pkl")
    LABEL_ENCODER = joblib.load("model/message_label_encoder.pkl")
    ml_enabled = True
    if DEBUG:
        print("ü§ñ Loaded message-level classifier.")
except FileNotFoundError:
    CLASSIFIER = None
    ml_enabled = False
    if DEBUG:
        print("‚ö†Ô∏è ML model not found ‚Äî skipping prediction.")

def is_correlated_message(sender_email, sender_domain, msg_date):
    """
    True if sender matches an existing application and msg_date is within 1 year after first_sent.
    """
    app = get_application_by_sender(sender_email, sender_domain)
    if not app:
        return False

    try:
        app_date = datetime.strptime(app["first_sent"], "%Y-%m-%d %H:%M:%S")
        msg_dt = datetime.strptime(msg_date, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        return False

    one_year_later = app_date + timedelta(days=365)
    return app_date <= msg_dt <= one_year_later


def predict_company(subject, body):
    """Predict company name using the trained ML model."""
    if not ml_enabled:
        return None
    text = (subject or "") + " " + (body or "")
    X = VECTORIZER.transform([text])
    pred_encoded = CLASSIFIER.predict(X)[0]
    return LABEL_ENCODER.inverse_transform([pred_encoded])[0]

def should_ignore(subject, body):
    """Return True if subject/body matches ignore patterns."""
    subj_lower = subject.lower()
    ignore_patterns = PATTERNS.get("ignore", [])
    return any(p.lower() in subj_lower for p in ignore_patterns)


def extract_metadata(service, msg_id):
    body_html = []
    """Extract subject, date, thread_id, labels, sender, sender_domain, and body text from a Gmail message."""
    msg = (
        service.users().messages().get(userId="me", id=msg_id, format="full").execute()
    )
    headers = msg["payload"]["headers"]

    subject = next((h["value"] for h in headers if h["name"] == "Subject"), "")
    date_raw = next((h["value"] for h in headers if h["name"] == "Date"), "")
    try:
        date_obj = parsedate_to_datetime(date_raw)
        if timezone.is_naive(date_obj):
            date_obj = timezone.make_aware(date_obj)  # assume settings.TIME_ZONE
        date_str = date_obj.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        date_str = date_raw

    sender = next((h["value"] for h in headers if h["name"].lower() == "from"), "")
    parsed = parseaddr(sender)
    email_addr = parsed[1] if len(parsed) == 2 else ""
    match = re.search(r"@([A-Za-z0-9.-]+)$", email_addr)
    sender_domain = match.group(1).lower() if match else ""

    thread_id = msg["threadId"]
    label_ids = msg.get("labelIds", [])
    labels = ",".join(label_ids)  # raw IDs unless you re-add get_label_map()

    body = ""
    parts = msg["payload"].get("parts", [])
    body = extract_body_from_parts(parts)

    for part in parts:
        mime_type = part.get("mimeType")
        data = part["body"].get("data")
        if not data:
            continue
        #decoded = base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")
        encoding = part.get("body", {}).get("encoding", "base64").lower()
        data = part.get("body", {}).get("data")
        if data:
            decoded = decode_part(data, encoding)

        if mime_type == "text/plain" and not body:
            body = decoded.strip()
        elif mime_type == "text/html" and not body:
            body_html = html.unescape(decoded)
            soup = BeautifulSoup(body_html, "html.parser")
            body_text = soup.get_text(separator=" ", strip=True)

    # Fallback if no parts
    if not body and "body" in msg["payload"]:
        data = msg["payload"]["body"].get("data")
        if data:
            body = base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")

    return {
        "thread_id": thread_id,
        "subject": subject,
        "body": body,
        "body_html": body_html,
        "date": date_str,
        "timestamp": date_obj,
        "labels": labels,
        "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "sender": sender,
        "sender_domain": sender_domain,
        "parser_version": PARSER_VERSION,
    }


def extract_status_dates(body, received_date):
    """Extract key status dates from email body."""
    body_lower = body.lower()
    dates = {
        "response_date": None,
        "rejection_date": None,
        "interview_date": None,
        "follow_up_dates": [],
    }
    if any(p in body_lower for p in PATTERNS.get("response", [])):
        dates["response_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("rejection", [])):
        dates["rejection_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("interview", [])):
        dates["interview_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("follow_up", [])):
        dates["follow_up_dates"] = received_date
    return dates


def classify_message(body):
    """Classify message body into a status category based on patterns.json."""
    body_lower = body.lower()
    if any(p in body_lower for p in PATTERNS.get("rejection", [])):
        return "rejection"
    if any(p in body_lower for p in PATTERNS.get("interview", [])):
        return "interview"
    if any(p in body_lower for p in PATTERNS.get("follow_up", [])):
        return "follow_up"
    if any(p in body_lower for p in PATTERNS.get("application", [])):
        return "application"
    if any(p in body_lower for p in PATTERNS.get("response", [])):
        return "response"
    return ""


def parse_subject(subject, sender=None, sender_domain=None):
    """Extract company, job title, and job ID from subject line, sender, and optionally sender domain."""

    RESUME_NOISE_PATTERNS = [
        r"\bresume\b",
        r"\bcv\b",
        r"\bcover letter\b",
        r"\bmuch more\b",
        r"\bnow available\b",
        r"\bgift card\b",
        r"\bcyberattack\b"
    ]

    # --- ML classification ---
    result = predict_subject_type(subject)
    label = result["label"]
    confidence = result["confidence"]
    ignore = result["ignore"]

    # --- Hard-ignore for resume or known noise patterns ---
    if label == "noise" or should_ignore(subject, "") or any(re.search(p, subject, re.I) for p in RESUME_NOISE_PATTERNS):
        return {
            "company": "",
            "job_title": "",
            "job_id": "",
            "predicted_company": "",
            "label": "noise",
            "confidence": 0.9,
            "ignore": True,
        }

    # --- Entity extraction ---
    entities = extract_entities(subject)
    company = entities.get("company", "")
    job_title = entities.get("job_title", "")
    job_id = ""

    # --- Continue with original logic for fallback or enrichment ---
    subject_clean = subject.strip()
    subj_lower = subject_clean.lower()
    domain_lower = sender_domain.lower() if sender_domain else None

    # Colon-prefix
    if not company:
        m = re.match(r"^([A-Z][A-Za-z0-9&.\- ]+):", subject_clean)
        if m:
            company = m.group(1).strip()

    # Known companies
    if not company and KNOWN_COMPANIES:
        for known in KNOWN_COMPANIES:
            if known in subj_lower:
                company = known.title()
                break

    # Domain mapping
    if not company and domain_lower and domain_lower in DOMAIN_TO_COMPANY:
        company = DOMAIN_TO_COMPANY[domain_lower]

    # ATS domain ‚Üí display name
    if not company and domain_lower in ATS_DOMAINS and sender:
        display_name, _ = parseaddr(sender)
        cleaned = re.sub(
            r"\b(Workday|Recruiting Team|Careers|Talent Acquisition Team|HR|Hiring)\b",
            "",
            display_name,
            flags=re.I,
        ).strip()
        if cleaned:
            company = cleaned

    # Regex patterns
    patterns = [
        (r"application (?:to|for|with)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"(?:from|with|at)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"position\s+@\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),  # catches "position @ Claroty",
        (r"^([A-Z][\w\s&\-]+)\s+(Job|Application|Interview)", 0),
        (r"-\s*([A-Z][\w\s&\-]+)\s*-\s*", 0),
        (r"^([A-Z][\w\s&\-]+)\s+application", 0),
        (r"(?:your application with|application with|interest in|position at)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"update on your ([A-Z][\w\s&\-]+) application", re.IGNORECASE),
        (r"thank you for your application with\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"@\s*([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"^([A-Z][\w\s&\-]+)\s+[-:]", re.IGNORECASE),  # catches "ECS -", "Partner Forces:",
        (r"applying for ([\w\s\-]+) position @ ([A-Z][\w\s&\-]+)", re.IGNORECASE),  # special case
    ]
    # Handle special case: "applying for Field CTO position @ Claroty"
    special_match = re.search(
        r"applying for ([\w\s\-]+) position @ ([A-Z][\w\s&\-]+)", subject_clean
    )
    if special_match:
        job_title = special_match.group(1).strip()
        company = special_match.group(2).strip()
    
    
    for pat, flags in patterns:
        if not company:
            match = re.search(pat, subject_clean, flags)
            if match:
                company = match.group(1).strip()
    
    # üßº Sanity check: reject job titles misclassified as companies
    if company and re.search(r"\b(CTO|Engineer|Manager|Director|Intern|Analyst)\b", company, re.I):
        company = ""
    
    # Job title fallback
    if not job_title:
        title_match = re.search(
            r"job\s+(?:submission\s+for|application\s+for|title\s+is)?\s*([\w\s\-]+)",
            subject_clean,
            re.IGNORECASE,
        )
        job_title = title_match.group(1).strip() if title_match else ""

    # Job ID
    id_match = re.search(
        r"(?:Job\s*#?|Position\s*#?|jobId=)([\w\-]+)", subject_clean, re.IGNORECASE
    )
    job_id = id_match.group(1).strip() if id_match else ""

    return {
        "company": company,
        "job_title": job_title,
        "job_id": job_id,
        "predicted_company": company,
        "label": label,
        "confidence": confidence,
        "ignore": False,
    }

def ingest_message(service, msg_id):
    stats = get_stats()
    
    try:
        metadata = extract_metadata(service, msg_id)
        body = metadata["body"]
        result = None  # ‚úÖ Prevent UnboundLocalError

    except Exception as e:
        if DEBUG:
            print(f"‚ùå Failed to extract data for {msg_id}: {e}")
        return

    parsed_subject = parse_subject(
        metadata["subject"],
        sender=metadata.get("sender"),
        sender_domain=metadata.get("sender_domain"),
    ) or {}

    if parsed_subject.get("ignore"):
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored by ML: {metadata['subject']}")
            log_ignored_message(
                msg_id, metadata, reason=parsed_subject.get("ignore_reason", "ml_ignore")
            )

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    status = classify_message(body)
    status_dates = extract_status_dates(body, metadata["date"])
    def to_date(value):
        try:
            return datetime.strptime(value, "%Y-%m-%d %H:%M:%S").date()
        except Exception:
            return None

    status_dates = {
        "response_date": to_date(status_dates.get("response_date")),
        "rejection_date": to_date(status_dates.get("rejection_date")),
        "interview_date": to_date(status_dates.get("interview_date")),
        "follow_up_dates": status_dates.get("follow_up_dates", []),
    }   
    
    
    # Normalize follow_up_dates and labels to strings
    follow_up_raw = status_dates.get("follow_up_dates", [])
    follow_up_str = ", ".join(follow_up_raw) if isinstance(follow_up_raw, list) else str(follow_up_raw)

    labels_raw = metadata.get("labels", [])
    labels_str = ", ".join(labels_raw) if isinstance(labels_raw, list) else str(labels_raw)
    
    if DEBUG:
        print(f"üì• Inserting message: {metadata['subject']}")

    insert_email_text(msg_id, metadata["subject"], body)

    subject = metadata["subject"]
    result = predict_subject_type(subject, body)

    company = parsed_subject.get("company", "") or ""
    company_norm = company.lower()
    company_source = "subject_parse"
    
    # Reject invalid company names from patterns.json
    if company and not is_valid_company_name(company):
        if DEBUG:
            print(f"üßπ Rejected invalid company name: {company}")
        company = ""
        
    print(f"üß™ company_norm: {company_norm}")
    print(f"üß™ is_valid_company: {is_valid_company(company)}")
    if company_norm not in KNOWN_COMPANIES and not is_valid_company(company):
        company = ""
    
    if not company:
        sender_domain = metadata.get("sender_domain", "").lower()
        is_ats = any(d in sender_domain for d in ATS_DOMAINS)
        if not is_ats:
            mapped = DOMAIN_TO_COMPANY.get(sender_domain, "")
            if mapped:
                company = mapped
                company_source = "domain_mapping"
                if DEBUG:
                    print(f"üß© Domain mapping used: {sender_domain} ‚Üí {company}")

    if not company:
        sender_name = metadata.get("sender", "").split("<")[0].strip().lower()
        for known in KNOWN_COMPANIES:
            if known.lower() in sender_name:
                company = known
                company_source = "sender_name_match"
                if DEBUG:
                    print(f"üîç Sender name match: {sender_name} ‚Üí {company}")
                break

    if not company:
        try:
            predicted = predict_company(subject, body)
            if predicted and predicted.lower() in {"job_application", "job_alert", "noise",}:
                predicted = ""
            
            if predicted:
                company = predicted
                company_source = "ml_prediction"
                if DEBUG:
                    print(f"üß† ML prediction used: {predicted}")
        except NameError:
            if DEBUG:
                print("‚ö†Ô∏è ML prediction function not available.")

    if not company:

       # Allow optional space, punctuation‚Äêagnostic, case‚Äêinsensitive
        at_match = re.search(r"@\s*([A-Za-z][\w\s&\-]+?)(?=[\W]|$)",body,flags=re.IGNORECASE
        )
        if at_match:
            # Normalize casing
            company = at_match.group(1).strip().title()
            company_source = "body_at_symbol"
            if DEBUG:
                print(f"üìß '@' symbol match used: {company}")

    if not company:
        body_match = re.search(
            r"(?:apply(?:ing)? to|application to|interest in|position at|role at|opportunity with)\s+([A-Z][\w\s&\-]+)",
            body,
            re.IGNORECASE
        )
        if body_match:
            company = body_match.group(1).strip()
            company_source = "body_regex"
            if DEBUG:
                print(f"üìÑ Body regex used: {company}")
    if not company:
        company_source = "unresolved"

    company_obj = None

    # Normalize casing for known companies
    if company:
        for known in KNOWN_COMPANIES:
            if company.lower() == known.lower():
                company = known
                break
     # Sanity check: does subject contain a conflicting company name?
    subject_lower = metadata["subject"].lower()
    if company and company.lower() not in subject_lower:
        for known in KNOWN_COMPANIES:
            if known.lower() in subject_lower and known.lower() != company.lower():
                print(f"‚ö†Ô∏è Subject mentions different company: {known} vs resolved {company}")
                break 

    confidence = float(result.get("confidence", 0.0)) if result else 0.0
                    
    if company:
        company_obj, _ = Company.objects.get_or_create(
        name=company,
            defaults={
                "first_contact": metadata["timestamp"],
                "last_contact": metadata["timestamp"],
                "confidence": confidence
                }
            )
        if company_obj and not company_obj.domain:
            sender_domain = metadata.get("sender_domain", "").lower()
            if sender_domain:
                company_obj.domain = sender_domain
                company_obj.save()
                if DEBUG:
                    print(f"üåê Set domain for {company}: {sender_domain}")

    if DEBUG:
        confidence = result.get("confidence", 0.0) if result else 0.0
        print(f"üìé Final company: {company}")
        print(f"üìé company_obj: {company_obj}")
        print(f"üìé ML label: {result.get('label') if result else 'unknown'}")
        print(f"üìé confidence: {confidence}")

    #
    # This is the re-ingest logic
    #
    # ‚úÖ Skip logic (now safe to run after enrichment)
    existing = Message.objects.filter(msg_id=msg_id).first()
    if existing:
        if DEBUG:
            print(f"‚úèÔ∏è Updating existing message: {msg_id}")
            print(f"üß† Re-ingest reviewed={existing.reviewed} (confidence={result['confidence']:.2f})")
        if company_obj:
            existing.company = company_obj
            existing.company_source = company_source
        if result:
            existing.ml_label = result["label"]
            existing.confidence = result["confidence"]

        # üß† Preserve manual review or auto-mark if confidence is high
        if (
            result
            and result.get("confidence", 0.0) >= 0.85
            and result.get("label") not in {"noise", "job_alert"}
            and company_obj is not None
            and is_valid_company(company)
        ):
            existing.reviewed = True

        IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_skipped}, skipped={stats.total_skipped}")

        return "skipped"

    
    reviewed = (
        result
        and result.get("confidence", 0.0) >= 0.85
        and result.get("label") not in {"noise", "job_alert"}
        and company_obj is not None
        and is_valid_company(company)
    )
    # or whatever threshold you trust
    if DEBUG and not reviewed:
        print(f"üõë Not reviewed: confidence={result.get('confidence', 0.0):.2f}, label={result.get('label')}, company={company}")
    
    # ‚úÖ Now safe to insert Message with enriched company
    Message.objects.create(
        msg_id=msg_id,
        thread_id=metadata["thread_id"],
        subject=subject,
        sender=metadata["sender"],
        body=metadata["body"],
        body_html=metadata.get("body_html", ""),
        timestamp=metadata["timestamp"],
        ml_label=result["label"],
        confidence=result["confidence"],
        reviewed=(reviewed),
        company=company_obj,
        company_source=company_source,
    )
    # ‚úÖ Create or update Application record using Django ORM


    try:
        message_obj = Message.objects.get(msg_id=msg_id)
        if company_obj and message_obj:
            application_obj, created = Application.objects.get_or_create(
                thread_id=metadata["thread_id"],
                defaults={
                    "company": company_obj,
                    "company_source": company_source,
                    "job_title": parsed_subject.get("job_title", ""),
                    "job_id": parsed_subject.get("job_id", ""),
                    "status": status,
                    "sent_date": metadata["timestamp"].date(),
                    "rejection_date": status_dates["rejection_date"],
                    "interview_date": status_dates["interview_date"],
                    "ml_label": result.get("label") if result else None,
                    "ml_confidence": float(result.get("confidence", 0.0)) if result else 0.0,
                    "reviewed": reviewed,
                }
            )
            if created:
                IngestionStats.objects.filter(date=stats.date).update(total_inserted=F("total_inserted") + 1)
            else:
                IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)
        else:
            # If company_obj or message_obj is missing, count as skipped
            IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

    except Exception as e:
        if DEBUG:
            print(f"‚ö†Ô∏è Failed to create Application: {e}")
        IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

    # Refresh stats before printing
    if DEBUG:
        stats.refresh_from_db()
        print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")
  
    # Final record assembly for applications table
    record = {
        "thread_id": metadata["thread_id"],
        "company": company,
        "predicted_company": parsed_subject.get("predicted_company", ""),
        "job_title": parsed_subject.get("job_title", ""),
        "job_id": parsed_subject.get("job_id", ""),
        "first_sent": metadata["date"],
        "response_date": status_dates["response_date"],
        "follow_up_dates": follow_up_str,
        "rejection_date": status_dates["rejection_date"],
        "interview_date": status_dates["interview_date"],
        "status": status,
        "labels": labels_str,
        "subject": metadata["subject"],
        "sender": metadata["sender"],
        "sender_domain": metadata["sender_domain"],
        "last_updated": metadata["last_updated"],
        "company_source": company_source,
    }
    if not company and not should_ignore(subject, body):
        UnresolvedCompany.objects.update_or_create(
            msg_id=msg_id,
            defaults={
                "subject": metadata["subject"],
                "body": metadata["body"],
                "sender": metadata["sender"],
                "sender_domain": metadata["sender_domain"],
                "timestamp": metadata["timestamp"],
            }
        )
        if DEBUG:
            print(f"üóÇ Logged unresolved company for manual review: {msg_id}")
        
    if not record["company"] and not record["job_title"] and not record["job_id"]:
        reason = "unclassified"
        if not metadata["body"]:
            reason = "missing_body"
        elif metadata["body"] and not record["company"]:
            reason = "missing_company"
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored due to: {reason} ‚Üí {metadata['subject']}")
        log_ignored_message(msg_id, metadata, reason=reason)

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    record["company_job_index"] = build_company_job_index(
        record.get("company", ""), record.get("job_title", ""), record.get("job_id", "")
    )

    if DEBUG:
        print(f"üîç company: {record['company']}")
        print(f"üîç job_title: {record['job_title']}")
        print(f"üîç job_id: {record['job_id']}")
        print(f"üîç company_source: {record['company_source']}")
        print(f"üîç company_job_index: {record['company_job_index']}")

    if should_ignore(metadata["subject"], metadata["body"]):
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored by pattern: {metadata['subject']}")
        log_ignored_message(msg_id, metadata, reason="pattern_ignore")

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    insert_or_update_application(record)

    if DEBUG:
        print(f"‚úÖ Logged: {metadata['subject']}")

    return "inserted"

===== END OF FILE: parser.py =====



===== START OF FILE: parser.py.txt =====

# parser.py
import os
import re
import json
import base64
import html
import joblib
from joblib import load
import django
from django.utils import timezone
from django.utils.timezone import now
from django.db.models import F
from pathlib import Path
from email.utils import parsedate_to_datetime, parseaddr
from bs4 import BeautifulSoup
from db import insert_email_text, insert_or_update_application, is_valid_company
from datetime import datetime, timedelta
from db_helpers import get_application_by_sender, build_company_job_index
from ml_subject_classifier import predict_subject_type
from ml_entity_extraction import extract_entities
from tracker.models import Application, Message, IgnoredMessage, IngestionStats, Company, UnresolvedCompany


os.environ.setdefault("DJANGO_SETTINGS_MODULE", "dashboard.settings")
django.setup()
DEBUG = True

# --- Load patterns.json ---
PATTERNS_PATH = Path(__file__).parent / "patterns.json"
if PATTERNS_PATH.exists():
    with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
        patterns_data = json.load(f)
    PATTERNS = patterns_data
else:
    PATTERNS = {}

COMPANIES_PATH = Path(__file__).parent / "companies.json"
if COMPANIES_PATH.exists():
    with open(COMPANIES_PATH, "r", encoding="utf-8") as f:
        company_data = json.load(f)
    ATS_DOMAINS = [d.lower() for d in company_data.get("ats_domains", [])]  
    KNOWN_COMPANIES = {c.lower() for c in company_data.get("known", [])}
    DOMAIN_TO_COMPANY = {
        k.lower(): v for k, v in company_data.get("domain_to_company", {}).items()
    }
    ALIASES = company_data.get("aliases", {})
else:
    KNOWN_COMPANIES = set()
    DOMAIN_TO_COMPANY = {}
    ALIASES = {}
    
def strip_html_tags(text: str) -> str:
    if not text:
        return ""
    # BeautifulSoup handles nested tags, entities, script/style removal better than regex
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def get_stats():
    today = now().date()
    stats, _ = IngestionStats.objects.get_or_create(date=today)
    return stats

def extract_body(payload):
    if 'parts' in payload:
        for part in payload['parts']:
            if part['mimeType'] == 'text/plain':
                return part['body'].get('data', '')
            elif part['mimeType'] == 'text/html':
                html = part['body'].get('data', '')
                return strip_html_tags(html)
    return payload.get('body', {}).get('data', '')

def log_ignored_message(msg_id, metadata, reason):
    IgnoredMessage.objects.update_or_create(
        msg_id=msg_id,
        defaults={
            "subject": metadata["subject"],
            "body": metadata["body"],
            "sender": metadata["sender"],
            "sender_domain": metadata["sender_domain"],
            "date": metadata["timestamp"],
            "reason": reason,
        },
    )

def is_valid_company_name(name):
    """Reject company names that match known invalid prefixes from patterns.json."""
    if not name:
        return False

    invalid_prefixes = PATTERNS.get("invalid_company_prefixes", [])
    lowered = name.lower()
    return not any(lowered.startswith(prefix.lower()) for prefix in invalid_prefixes)
PARSER_VERSION = "1.0.0"

# --- Load ML model artifacts at startup ---
# --- Load message-level ML model artifacts at startup ---
try:
    CLASSIFIER = joblib.load("model/message_classifier.pkl")
    VECTORIZER = joblib.load("model/message_vectorizer.pkl")
    LABEL_ENCODER = joblib.load("model/message_label_encoder.pkl")
    ml_enabled = True
    if DEBUG:
        print("ü§ñ Loaded message-level classifier.")
except FileNotFoundError:
    CLASSIFIER = None
    ml_enabled = False
    if DEBUG:
        print("‚ö†Ô∏è ML model not found ‚Äî skipping prediction.")

def is_correlated_message(sender_email, sender_domain, msg_date):
    """
    True if sender matches an existing application and msg_date is within 1 year after first_sent.
    """
    app = get_application_by_sender(sender_email, sender_domain)
    if not app:
        return False

    try:
        app_date = datetime.strptime(app["first_sent"], "%Y-%m-%d %H:%M:%S")
        msg_dt = datetime.strptime(msg_date, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        return False

    one_year_later = app_date + timedelta(days=365)
    return app_date <= msg_dt <= one_year_later


def predict_company(subject, body):
    """Predict company name using the trained ML model."""
    if not ml_enabled:
        return None
    text = (subject or "") + " " + (body or "")
    X = VECTORIZER.transform([text])
    pred_encoded = CLASSIFIER.predict(X)[0]
    return LABEL_ENCODER.inverse_transform([pred_encoded])[0]

def should_ignore(subject, body):
    """Return True if subject/body matches ignore patterns."""
    subj_lower = subject.lower()
    ignore_patterns = PATTERNS.get("ignore", [])
    return any(p.lower() in subj_lower for p in ignore_patterns)


def extract_metadata(service, msg_id):
    """Extract subject, date, thread_id, labels, sender, sender_domain, and body text from a Gmail message."""
    msg = (
        service.users().messages().get(userId="me", id=msg_id, format="full").execute()
    )
    headers = msg["payload"]["headers"]

    subject = next((h["value"] for h in headers if h["name"] == "Subject"), "")
    date_raw = next((h["value"] for h in headers if h["name"] == "Date"), "")
    try:
        date_obj = parsedate_to_datetime(date_raw)
        if timezone.is_naive(date_obj):
            date_obj = timezone.make_aware(date_obj)  # assume settings.TIME_ZONE
        date_str = date_obj.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        date_str = date_raw

    sender = next((h["value"] for h in headers if h["name"].lower() == "from"), "")
    parsed = parseaddr(sender)
    email_addr = parsed[1] if len(parsed) == 2 else ""
    match = re.search(r"@([A-Za-z0-9.-]+)$", email_addr)
    sender_domain = match.group(1).lower() if match else ""

    thread_id = msg["threadId"]
    label_ids = msg.get("labelIds", [])
    labels = ",".join(label_ids)  # raw IDs unless you re-add get_label_map()

    body = ""
    parts = msg["payload"].get("parts", [])
    for part in parts:
        mime_type = part.get("mimeType")
        data = part["body"].get("data")
        if not data:
            continue
        decoded = base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")
        if mime_type == "text/plain" and not body:
            body = decoded.strip()
        elif mime_type == "text/html" and not body:
            soup = BeautifulSoup(decoded, "html.parser")
            body = html.unescape(soup.get_text(separator=" ", strip=True))

    # Fallback if no parts
    if not body and "body" in msg["payload"]:
        data = msg["payload"]["body"].get("data")
        if data:
            decoded = base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")
            body = decoded.strip()

    return {
        "thread_id": thread_id,
        "subject": subject,
        "body": body,
        "date": date_str,
        "timestamp": date_obj,
        "labels": labels,
        "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "sender": sender,
        "sender_domain": sender_domain,
        "parser_version": PARSER_VERSION,
    }


def extract_status_dates(body, received_date):
    """Extract key status dates from email body."""
    body_lower = body.lower()
    dates = {
        "response_date": None,
        "rejection_date": None,
        "interview_date": None,
        "follow_up_dates": [],
    }
    if any(p in body_lower for p in PATTERNS.get("response", [])):
        dates["response_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("rejection", [])):
        dates["rejection_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("interview", [])):
        dates["interview_date"] = received_date
    if any(p in body_lower for p in PATTERNS.get("follow_up", [])):
        dates["follow_up_dates"] = received_date
    return dates


def classify_message(body):
    """Classify message body into a status category based on patterns.json."""
    body_lower = body.lower()
    if any(p in body_lower for p in PATTERNS.get("rejection", [])):
        return "rejection"
    if any(p in body_lower for p in PATTERNS.get("interview", [])):
        return "interview"
    if any(p in body_lower for p in PATTERNS.get("follow_up", [])):
        return "follow_up"
    if any(p in body_lower for p in PATTERNS.get("application", [])):
        return "application"
    if any(p in body_lower for p in PATTERNS.get("response", [])):
        return "response"
    return ""


def parse_subject(subject, sender=None, sender_domain=None):
    """Extract company, job title, and job ID from subject line, sender, and optionally sender domain."""

    RESUME_NOISE_PATTERNS = [
        r"\bresume\b",
        r"\bcv\b",
        r"\bcover letter\b",
        r"\bmuch more\b",
        r"\bnow available\b",
        r"\bgift card\b",
        r"\bcyberattack\b"
    ]

    # --- ML classification ---
    result = predict_subject_type(subject)
    label = result["label"]
    confidence = result["confidence"]
    ignore = result["ignore"]

    # --- Hard-ignore for resume or known noise patterns ---
    if label == "noise" or should_ignore(subject, "") or any(re.search(p, subject, re.I) for p in RESUME_NOISE_PATTERNS):
        return {
            "company": "",
            "job_title": "",
            "job_id": "",
            "predicted_company": "",
            "label": "noise",
            "confidence": 0.9,
            "ignore": True,
        }

    # --- Entity extraction ---
    entities = extract_entities(subject)
    company = entities.get("company", "")
    job_title = entities.get("job_title", "")
    job_id = ""

    # --- Continue with original logic for fallback or enrichment ---
    subject_clean = subject.strip()
    subj_lower = subject_clean.lower()
    domain_lower = sender_domain.lower() if sender_domain else None

    # Colon-prefix
    if not company:
        m = re.match(r"^([A-Z][A-Za-z0-9&.\- ]+):", subject_clean)
        if m:
            company = m.group(1).strip()

    # Known companies
    if not company and KNOWN_COMPANIES:
        for known in KNOWN_COMPANIES:
            if known in subj_lower:
                company = known.title()
                break

    # Domain mapping
    if not company and domain_lower and domain_lower in DOMAIN_TO_COMPANY:
        company = DOMAIN_TO_COMPANY[domain_lower]

    # ATS domain ‚Üí display name
    if not company and domain_lower in ATS_DOMAINS and sender:
        display_name, _ = parseaddr(sender)
        cleaned = re.sub(
            r"\b(Workday|Recruiting Team|Careers|Talent Acquisition Team|HR|Hiring)\b",
            "",
            display_name,
            flags=re.I,
        ).strip()
        if cleaned:
            company = cleaned

    # Regex patterns
    patterns = [
        (r"application (?:to|for|with)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"(?:from|with|at)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"position\s+@\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),  # catches "position @ Claroty",
        (r"^([A-Z][\w\s&\-]+)\s+(Job|Application|Interview)", 0),
        (r"-\s*([A-Z][\w\s&\-]+)\s*-\s*", 0),
        (r"^([A-Z][\w\s&\-]+)\s+application", 0),
        (r"(?:your application with|application with|interest in|position at)\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"update on your ([A-Z][\w\s&\-]+) application", re.IGNORECASE),
        (r"thank you for your application with\s+([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"@\s*([A-Z][\w\s&\-]+)", re.IGNORECASE),
        (r"^([A-Z][\w\s&\-]+)\s+[-:]", re.IGNORECASE),  # catches "ECS -", "Partner Forces:",
        (r"applying for ([\w\s\-]+) position @ ([A-Z][\w\s&\-]+)", re.IGNORECASE),  # special case
    ]
    # Handle special case: "applying for Field CTO position @ Claroty"
    special_match = re.search(
        r"applying for ([\w\s\-]+) position @ ([A-Z][\w\s&\-]+)", subject_clean
    )
    if special_match:
        job_title = special_match.group(1).strip()
        company = special_match.group(2).strip()
    
    
    for pat, flags in patterns:
        if not company:
            match = re.search(pat, subject_clean, flags)
            if match:
                company = match.group(1).strip()
    
    # üßº Sanity check: reject job titles misclassified as companies
    if company and re.search(r"\b(CTO|Engineer|Manager|Director|Intern|Analyst)\b", company, re.I):
        company = ""
    
    # Job title fallback
    if not job_title:
        title_match = re.search(
            r"job\s+(?:submission\s+for|application\s+for|title\s+is)?\s*([\w\s\-]+)",
            subject_clean,
            re.IGNORECASE,
        )
        job_title = title_match.group(1).strip() if title_match else ""

    # Job ID
    id_match = re.search(
        r"(?:Job\s*#?|Position\s*#?|jobId=)([\w\-]+)", subject_clean, re.IGNORECASE
    )
    job_id = id_match.group(1).strip() if id_match else ""

    return {
        "company": company,
        "job_title": job_title,
        "job_id": job_id,
        "predicted_company": company,
        "label": label,
        "confidence": confidence,
        "ignore": False,
    }

def ingest_message(service, msg_id):
    stats = get_stats()
    
    try:
        metadata = extract_metadata(service, msg_id)
        body = metadata["body"]
        result = None  # ‚úÖ Prevent UnboundLocalError

    except Exception as e:
        if DEBUG:
            print(f"‚ùå Failed to extract data for {msg_id}: {e}")
        return

    parsed_subject = parse_subject(
        metadata["subject"],
        sender=metadata.get("sender"),
        sender_domain=metadata.get("sender_domain"),
    ) or {}

    if parsed_subject.get("ignore"):
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored by ML: {metadata['subject']}")
            log_ignored_message(
                msg_id, metadata, reason=parsed_subject.get("ignore_reason", "ml_ignore")
            )

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    status = classify_message(body)
    status_dates = extract_status_dates(body, metadata["date"])
    def to_date(value):
        try:
            return datetime.strptime(value, "%Y-%m-%d %H:%M:%S").date()
        except Exception:
            return None

    status_dates = {
        "response_date": to_date(status_dates.get("response_date")),
        "rejection_date": to_date(status_dates.get("rejection_date")),
        "interview_date": to_date(status_dates.get("interview_date")),
        "follow_up_dates": status_dates.get("follow_up_dates", []),
    }   
    
    
    # Normalize follow_up_dates and labels to strings
    follow_up_raw = status_dates.get("follow_up_dates", [])
    follow_up_str = ", ".join(follow_up_raw) if isinstance(follow_up_raw, list) else str(follow_up_raw)

    labels_raw = metadata.get("labels", [])
    labels_str = ", ".join(labels_raw) if isinstance(labels_raw, list) else str(labels_raw)
    
    if DEBUG:
        print(f"üì• Inserting message: {metadata['subject']}")

    insert_email_text(msg_id, metadata["subject"], body)

    subject = metadata["subject"]
    result = predict_subject_type(subject, body)

    company = parsed_subject.get("company", "") or ""
    company_norm = company.lower()
    company_source = "subject_parse"
    
    # Reject invalid company names from patterns.json
    if company and not is_valid_company_name(company):
        if DEBUG:
            print(f"üßπ Rejected invalid company name: {company}")
        company = ""
        
    print(f"üß™ company_norm: {company_norm}")
    print(f"üß™ is_valid_company: {is_valid_company(company)}")
    if company_norm not in KNOWN_COMPANIES and not is_valid_company(company):
        company = ""
    
    if not company:
        sender_domain = metadata.get("sender_domain", "").lower()
        is_ats = any(d in sender_domain for d in ATS_DOMAINS)
        if not is_ats:
            mapped = DOMAIN_TO_COMPANY.get(sender_domain, "")
            if mapped:
                company = mapped
                company_source = "domain_mapping"
                if DEBUG:
                    print(f"üß© Domain mapping used: {sender_domain} ‚Üí {company}")

    if not company:
        sender_name = metadata.get("sender", "").split("<")[0].strip().lower()
        for known in KNOWN_COMPANIES:
            if known.lower() in sender_name:
                company = known
                company_source = "sender_name_match"
                if DEBUG:
                    print(f"üîç Sender name match: {sender_name} ‚Üí {company}")
                break

    if not company:
        try:
            predicted = predict_company(subject, body)
            if predicted and predicted.lower() in {"job_application", "job_alert", "noise",}:
                predicted = ""
            
            if predicted:
                company = predicted
                company_source = "ml_prediction"
                if DEBUG:
                    print(f"üß† ML prediction used: {predicted}")
        except NameError:
            if DEBUG:
                print("‚ö†Ô∏è ML prediction function not available.")

    if not company:

       # Allow optional space, punctuation‚Äêagnostic, case‚Äêinsensitive
        at_match = re.search(r"@\s*([A-Za-z][\w\s&\-]+?)(?=[\W]|$)",body,flags=re.IGNORECASE
        )
        if at_match:
            # Normalize casing
            company = at_match.group(1).strip().title()
            company_source = "body_at_symbol"
            if DEBUG:
                print(f"üìß '@' symbol match used: {company}")

    if not company:
        body_match = re.search(
            r"(?:apply(?:ing)? to|application to|interest in|position at|role at|opportunity with)\s+([A-Z][\w\s&\-]+)",
            body,
            re.IGNORECASE
        )
        if body_match:
            company = body_match.group(1).strip()
            company_source = "body_regex"
            if DEBUG:
                print(f"üìÑ Body regex used: {company}")
    if not company:
        company_source = "unresolved"

    company_obj = None

    # Normalize casing for known companies
    if company:
        for known in KNOWN_COMPANIES:
            if company.lower() == known.lower():
                company = known
                break
     # Sanity check: does subject contain a conflicting company name?
    subject_lower = metadata["subject"].lower()
    if company and company.lower() not in subject_lower:
        for known in KNOWN_COMPANIES:
            if known.lower() in subject_lower and known.lower() != company.lower():
                print(f"‚ö†Ô∏è Subject mentions different company: {known} vs resolved {company}")
                break 

    confidence = float(result.get("confidence", 0.0)) if result else 0.0
                    
    if company:
        company_obj, _ = Company.objects.get_or_create(
        name=company,
            defaults={
                "first_contact": metadata["timestamp"],
                "last_contact": metadata["timestamp"],
                "confidence": confidence
                }
            )
        if company_obj and not company_obj.domain:
            sender_domain = metadata.get("sender_domain", "").lower()
            if sender_domain:
                company_obj.domain = sender_domain
                company_obj.save()
                if DEBUG:
                    print(f"üåê Set domain for {company}: {sender_domain}")

    if DEBUG:
        confidence = result.get("confidence", 0.0) if result else 0.0
        print(f"üìé Final company: {company}")
        print(f"üìé company_obj: {company_obj}")
        print(f"üìé ML label: {result.get('label') if result else 'unknown'}")
        print(f"üìé confidence: {confidence}")

    #
    # This is the re-ingest logic
    #
    # ‚úÖ Skip logic (now safe to run after enrichment)
    existing = Message.objects.filter(msg_id=msg_id).first()
    if existing:
        if DEBUG:
            print(f"‚úèÔ∏è Updating existing message: {msg_id}")
            print(f"üß† Re-ingest reviewed={existing.reviewed} (confidence={result['confidence']:.2f})")
        if company_obj:
            existing.company = company_obj
            existing.company_source = company_source
        if result:
            existing.ml_label = result["label"]
            existing.confidence = result["confidence"]

        # üß† Preserve manual review or auto-mark if confidence is high
        if (
            result
            and result.get("confidence", 0.0) >= 0.85
            and result.get("label") not in {"noise", "job_alert"}
            and company_obj is not None
            and is_valid_company(company)
        ):
            existing.reviewed = True

        IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_skipped}, skipped={stats.total_skipped}")

        return "skipped"

    
    reviewed = (
        result
        and result.get("confidence", 0.0) >= 0.85
        and result.get("label") not in {"noise", "job_alert"}
        and company_obj is not None
        and is_valid_company(company)
    )
    # or whatever threshold you trust
    if DEBUG and not reviewed:
        print(f"üõë Not reviewed: confidence={result.get('confidence', 0.0):.2f}, label={result.get('label')}, company={company}")
    
    # ‚úÖ Now safe to insert Message with enriched company
    Message.objects.create(
        msg_id=msg_id,
        thread_id=metadata["thread_id"],
        subject=subject,
        sender=metadata["sender"],
        body=metadata["body"],
        timestamp=metadata["timestamp"],
        ml_label=result["label"],
        confidence=result["confidence"],
        reviewed=(reviewed),
        company=company_obj,
        company_source=company_source,
    )
    # ‚úÖ Create or update Application record using Django ORM


    try:
        message_obj = Message.objects.get(msg_id=msg_id)
        if company_obj and message_obj:
            application_obj, created = Application.objects.get_or_create(
                thread_id=metadata["thread_id"],
                defaults={
                    "company": company_obj,
                    "company_source": company_source,
                    "job_title": parsed_subject.get("job_title", ""),
                    "job_id": parsed_subject.get("job_id", ""),
                    "status": status,
                    "sent_date": metadata["timestamp"].date(),
                    "rejection_date": status_dates["rejection_date"],
                    "interview_date": status_dates["interview_date"],
                    "ml_label": result.get("label") if result else None,
                    "ml_confidence": float(result.get("confidence", 0.0)) if result else 0.0,
                    "reviewed": reviewed,
                }
            )
            if created:
                IngestionStats.objects.filter(date=stats.date).update(total_inserted=F("total_inserted") + 1)
            else:
                IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)
        else:
            # If company_obj or message_obj is missing, count as skipped
            IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

    except Exception as e:
        if DEBUG:
            print(f"‚ö†Ô∏è Failed to create Application: {e}")
        IngestionStats.objects.filter(date=stats.date).update(total_skipped=F("total_skipped") + 1)

    # Refresh stats before printing
    if DEBUG:
        stats.refresh_from_db()
        print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")
  
    # Final record assembly for applications table
    record = {
        "thread_id": metadata["thread_id"],
        "company": company,
        "predicted_company": parsed_subject.get("predicted_company", ""),
        "job_title": parsed_subject.get("job_title", ""),
        "job_id": parsed_subject.get("job_id", ""),
        "first_sent": metadata["date"],
        "response_date": status_dates["response_date"],
        "follow_up_dates": follow_up_str,
        "rejection_date": status_dates["rejection_date"],
        "interview_date": status_dates["interview_date"],
        "status": status,
        "labels": labels_str,
        "subject": metadata["subject"],
        "sender": metadata["sender"],
        "sender_domain": metadata["sender_domain"],
        "last_updated": metadata["last_updated"],
        "company_source": company_source,
    }
    if not company and not should_ignore(subject, body):
        UnresolvedCompany.objects.update_or_create(
            msg_id=msg_id,
            defaults={
                "subject": metadata["subject"],
                "body": metadata["body"],
                "sender": metadata["sender"],
                "sender_domain": metadata["sender_domain"],
                "timestamp": metadata["timestamp"],
            }
        )
        if DEBUG:
            print(f"üóÇ Logged unresolved company for manual review: {msg_id}")
        
    if not record["company"] and not record["job_title"] and not record["job_id"]:
        reason = "unclassified"
        if not metadata["body"]:
            reason = "missing_body"
        elif metadata["body"] and not record["company"]:
            reason = "missing_company"
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored due to: {reason} ‚Üí {metadata['subject']}")
        log_ignored_message(msg_id, metadata, reason=reason)

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    record["company_job_index"] = build_company_job_index(
        record.get("company", ""), record.get("job_title", ""), record.get("job_id", "")
    )

    if DEBUG:
        print(f"üîç company: {record['company']}")
        print(f"üîç job_title: {record['job_title']}")
        print(f"üîç job_id: {record['job_id']}")
        print(f"üîç company_source: {record['company_source']}")
        print(f"üîç company_job_index: {record['company_job_index']}")

    if should_ignore(metadata["subject"], metadata["body"]):
        if DEBUG:
            print(f"‚ö†Ô∏è Ignored by pattern: {metadata['subject']}")
        log_ignored_message(msg_id, metadata, reason="pattern_ignore")

        IngestionStats.objects.filter(date=stats.date).update(total_ignored=F("total_ignored") + 1)

        if DEBUG:
            stats.refresh_from_db()
            print(f"üìä Stats updated: inserted={stats.total_inserted}, ignored={stats.total_ignored}, skipped={stats.total_skipped}")

        return "ignored"

    insert_or_update_application(record)

    if DEBUG:
        print(f"‚úÖ Logged: {metadata['subject']}")

    return "inserted"

===== END OF FILE: parser.py.txt =====



===== START OF FILE: patterns.json =====

{
  "application": [
    "for applying",
    "received your application",
    "Your application has been received",
    "thank you for your application",
    "successfully submitted"
  ],
  "rejection": [
    "we regret",
    "we appreciate your interest",
    "other candidates",
    "not moving forward",
    "position closed",
    "no longer available"
  ],
  "interview": [
    "schedule",
    "availability",
    "next steps",
    "interview"
  ],
"ignore": [
  "your job alert for",
  "miss out",
  "Registration Confirmation",
  "how your job search",
  "saved job is expiring",
  "job matches",
  "new job opportunities",
  "looking for a new job",
  "platform",
  "enterprise ",
  "architecture",
  "performance",
  "development",
  "multifunctiona",
  "web",
  "cloud application",
  "application security",
  "Retirement"
],
  "response": [
    "thank you for applying",
    "received your resume",
    "application received"
  ],
  "follow_up": [
    "just checking in",
    "following up",
    "wanted to follow",
    "any update on my application",
    "reaching out again"
  ],
"invalid_company_prefixes": [
    "Cyber Program",
    "Lead Machine Learning Engineer",
    "Detection and Response",
    "Machine Learning Engineer",
    "Senior Manager",
    "Program Manager",
    "Security Architect",
    "Director of Engineering",
    "Lead Cyber Security Engineer",
    "Principal",
    "Submission Form",
    "Remote",
    "Rejection",
    "Interview Invite",
    "Job_Application",
    "Fwd",
    "VA -",
    "Detection and Response",
    "the Lead",
    "the Senior",
    "the Principal",
    "donotreply",
    "Field CTO"
  ]
}

===== END OF FILE: patterns.json =====



===== START OF FILE: pytest.ini =====

# pytest.ini
[pytest]
DJANGO_SETTINGS_MODULE = dashboard.settings
python_files = tests.py test_*.py *_tests.py

===== END OF FILE: pytest.ini =====



===== START OF FILE: requirements.lock.txt =====

asgiref==3.9.1
beautifulsoup4==4.13.5
contourpy==1.3.3
cycler==0.12.1
Django==5.2.6
django-crispy-forms==2.4
fonttools==4.59.2
joblib==1.5.2
kiwisolver==1.4.9
matplotlib==3.10.6
numpy==2.3.3
packaging==25.0
pandas==2.3.2
pillow==11.3.0
pyparsing==3.2.3
python-dateutil==2.9.0.post0
pytz==2025.2
scikit-learn==1.7.2
scipy==1.16.2
six==1.17.0
soupsieve==2.8
sqlparse==0.5.3
threadpoolctl==3.6.0
typing_extensions==4.15.0
tzdata==2025.2

===== END OF FILE: requirements.lock.txt =====



===== START OF FILE: requirements.txt =====

annotated-types==0.7.0
asgiref==3.10.0
beautifulsoup4==4.12.3
blis==0.7.11
cachetools==5.5.2
catalogue==2.0.10
certifi==2025.10.5
cfgv==3.4.0
charset-normalizer==3.4.3
click==8.0.4
cloudpathlib==0.23.0
colorama==0.4.6
confection==0.1.5
cymem==2.0.11
detect-secrets==1.5.0
distlib==0.4.0
Django==5.2.6
en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85
filelock==3.20.0
google-api-core==2.26.0
google-api-python-client==2.125.0
google-auth==2.29.0
google-auth-httplib2==0.2.0
google-auth-oauthlib==1.2.0
googleapis-common-protos==1.70.0
httplib2==0.31.0
identify==2.6.15
idna==3.10
iniconfig==2.1.0
Jinja2==3.1.6
joblib==1.4.0
langcodes==3.5.0
language_data==1.3.0
marisa-trie==1.3.1
MarkupSafe==3.0.3
murmurhash==1.0.13
nodeenv==1.9.1
numpy==1.26.4
oauthlib==3.3.1
packaging==25.0
pandas==2.2.2
platformdirs==4.5.0
pluggy==1.6.0
pre_commit==4.3.0
preshed==3.0.10
proto-plus==1.26.1
protobuf==6.32.1
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.12.0
pydantic_core==2.41.1
pyparsing==3.2.5
pytest==8.1.1
python-dateutil==2.9.0.post0
pytz==2025.2
PyYAML==6.0.3
requests==2.32.5
requests-oauthlib==2.0.0
rsa==4.9.1
scikit-learn==1.7.1
scipy==1.16.2
setuptools==80.9.0
six==1.17.0
smart_open==7.3.1
soupsieve==2.8
spacy==3.8.0
spacy-legacy==3.0.12
spacy-loggers==1.0.5
sqlparse==0.4.4
srsly==2.5.1
thinc==8.2.5
threadpoolctl==3.6.0
tqdm==4.67.1
typer==0.9.0
typing-inspection==0.4.2
typing_extensions==4.15.0
tzdata==2025.2
uritemplate==4.2.0
urllib3==2.5.0
virtualenv==20.35.3
wasabi==1.1.3
weasel==0.4.1
wrapt==1.17.3

===== END OF FILE: requirements.txt =====



===== START OF FILE: requirements.txt.old =====

annotated-types==0.7.0
asgiref==3.9.2
beautifulsoup4==4.14.2
blis==1.3.0
cachetools==6.2.0
catalogue==2.0.10
certifi==2025.8.3
charset-normalizer==3.4.3
click==8.3.0
cloudpathlib==0.22.0
confection==0.1.5
contourpy==1.3.3
cycler==0.12.1
cymem==2.0.11
Django==4.2.25
django-crispy-forms==2.4
en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85
fonttools==4.60.1
google-api-core==2.25.2
google-api-python-client==2.184.0
google-auth==2.41.1
google-auth-httplib2==0.2.0
google-auth-oauthlib==1.2.2
googleapis-common-protos==1.70.0
httplib2==0.31.0
idna==3.10
Jinja2==3.1.6
joblib==1.5.2
kiwisolver==1.4.9
langcodes==3.5.0
language_data==1.3.0
marisa-trie==1.3.1
markdown-it-py==4.0.0
MarkupSafe==3.0.3
matplotlib==3.10.6
mdurl==0.1.2
murmurhash==1.0.13
numpy==2.3.3
oauthlib==3.3.1
packaging==25.0
pandas==2.3.3
pillow==11.3.0
preshed==3.0.10
proto-plus==1.26.1
protobuf==6.32.1
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.11.9
pydantic_core==2.33.2
Pygments==2.19.2
pyparsing==3.2.5
python-dateutil==2.9.0.post0
pytz==2025.2
requests==2.32.5
requests-oauthlib==2.0.0
rich==14.1.0
rsa==4.9.1
scikit-learn==1.7.2
scipy==1.16.2
setuptools==80.9.0
shellingham==1.5.4
six==1.17.0
smart_open==7.3.1
soupsieve==2.8
spacy==3.8.7
spacy-legacy==3.0.12
spacy-loggers==1.0.5
sqlparse==0.5.3
srsly==2.5.1
thinc==8.3.6
threadpoolctl==3.6.0
tqdm==4.67.1
typer==0.19.2
typing-inspection==0.4.2
typing_extensions==4.15.0
tzdata==2025.2
uritemplate==4.2.0
urllib3==2.5.0
wasabi==1.1.3
weasel==0.4.1
wrapt==1.17.3

===== END OF FILE: requirements.txt.old =====



===== START OF FILE: tests/SCHEMA_CHANGELOG.md =====

# SCHEMA_CHANGELOG.md
## 2025-09-09
- Added `company_job_index` TEXT column to `applications` table.
- Populated from normalized company, job_title, job_id.
- Created index `idx_company_job_index` for fast grouping.

===== END OF FILE: tests/SCHEMA_CHANGELOG.md =====



===== START OF FILE: tests/test_company_job_index.py.txt =====

# test_company_job_index.py:2
from db_helpers import build_company_job_index

def test_company_job_index_normalization():
    idx = build_company_job_index("Acme Corp", "Senior Engineer", "12345")
    assert idx == "acme corp::senior engineer::12345"

def test_company_job_index_empty_fields():
    idx = build_company_job_index("Acme Corp", "", None)
    assert idx == "acme corp::::"

===== END OF FILE: tests/test_company_job_index.py.txt =====



===== START OF FILE: tests/test_parse_subject.py.txt =====

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from parser import parse_subject

test_subjects = [
    "Your Google data is ready to download",
    "Lead Cyber Security Analyst (Hybrid) at CareFirst BlueCross BlueShield",
    "Thank you for applying to Bowhead",
    "Interview Confirmation ‚Äì SAIC",
    "Application Confirmation ‚Äì Peraton",
    "New jobs similar to Strategic Engineer at Lenovo",
    "Security Alert: New sign-in from Chrome",
    "Weekly Bulletin ‚Äì Tech Jobs in DC",
    "UMKC- SSE has an open position",
    "Google Verification Code"
]

for subject in test_subjects:
    result = parse_subject(subject)
    print(f"üîç Subject: {subject}")
    print(f"üß™ Parsed: {result}\n")

===== END OF FILE: tests/test_parse_subject.py.txt =====



===== START OF FILE: todo.md =====

### Todo.

Perfect ‚Äî you're designing a hybrid ingestion mode that captures real Gmail data once, then reuses it offline for simulation. Here's how to implement it cleanly and modularly:

---

## ‚úÖ Goal

When `--build-simulation-gmail-box` is passed:
- Authenticate and ingest real Gmail messages
- Save the raw Gmail API responses (or parsed metadata) to a local file (e.g., `simulated_gmail.json`)
- Exit after saving ‚Äî no DB writes yet

Later, during dev:
- Use `--simulate-gmail` to ingest from that local file instead of hitting Gmail

---

## üß± Implementation Plan

### 1. **Extend CLI Flags in `main.py`**
```python
parser.add_argument("--build-simulation-gmail-box", action="store_true", help="Fetch real Gmail messages and save to local file")
parser.add_argument("--simulate-gmail", action="store_true", help="Ingest from local Gmail simulation file instead of live Gmail")
```

---

### 2. **Simulation File Location**
Define a constant:
```python
SIM_FILE = "simulated_gmail.json"
```

---

### 3. **If `--build-simulation-gmail-box` is set**
Replace the full sync logic with:

```python
if args.build_simulation_gmail_box:
    print("üì• Fetching real Gmail messages for simulation...")
    query = build_query()
    messages = fetch_all_messages(service, query)

    # Fetch full message payloads
    full_messages = []
    for idx, msg in enumerate(messages, start=1):
        msg_id = msg["id"]
        try:
            full = service.users().messages().get(userId="me", id=msg_id, format="full").execute()
            full_messages.append(full)
            print(f"[{idx}/{len(messages)}] ‚úÖ Saved {msg_id}")
        except Exception as e:
            print(f"[{idx}/{len(messages)}] ‚ùå Failed to fetch {msg_id}: {e}")

    # Save to file
    import json
    with open(SIM_FILE, "w", encoding="utf-8") as f:
        json.dump(full_messages, f, indent=2)

    print(f"‚úÖ Saved {len(full_messages)} messages to {SIM_FILE}")
    sys.exit(0)
```

---

### 4. **If `--simulate-gmail` is set**
Bypass Gmail auth and load from file:

```python
if args.simulate_gmail:
    print("üß™ Simulation mode: loading Gmail messages from local file")
    import json
    with open(SIM_FILE, "r", encoding="utf-8") as f:
        simulated_messages = json.load(f)

    print(f"üìÑ Loaded {len(simulated_messages)} simulated messages")

    for idx, msg in enumerate(simulated_messages, start=1):
        msg_id = msg["id"]
        try:
            result = ingest_message(None, msg_id, raw_message=msg)
            print(f"[{idx}/{len(simulated_messages)}] ‚úÖ {result} {msg_id}")
        except Exception as e:
            print(f"[{idx}/{len(simulated_messages)}] ‚ùå Failed to ingest {msg_id}: {e}")
    sys.exit(0)
```

You‚Äôll need to modify `ingest_message()` to accept a `raw_message` override and skip the Gmail API call if it‚Äôs provided.

---

### 5. **Patch `ingest_message()` in `parser.py`**
```python
def ingest_message(service, msg_id, raw_message=None):
    if raw_message:
        message = raw_message
    else:
        message = service.users().messages().get(userId="me", id=msg_id, format="full").execute()
    ...
```

---

## üß™ Final CLI Usage

- To build the simulation file:
  ```bash
  python main.py --build-simulation-gmail-box
  ```

- To ingest from it offline:
  ```bash
  python main.py --simulate-gmail
  ```

---

Let me know if you want to version the simulation file, anonymize sensitive fields, or scaffold a `simulate_gmail.py` module. This hybrid mode is a brilliant move ‚Äî it gives you reproducible, real-world test data without hammering the Gmail API.

===== END OF FILE: todo.md =====



===== START OF FILE: tracker/admin.py =====

from django.contrib import admin
from .models import Company, Application, Message, UnresolvedCompany,KnownCompany, ATSDomain, DomainToCompany, CompanyAlias, Ticket

list_display = ("name", "message_count", "application_count")

class UnresolvedCompanyAdmin(admin.ModelAdmin):
    fields = ("msg_id", "subject", "body", "sender", "sender_domain", "timestamp", "notes", "reviewed")
    
    list_display = ("msg_id", "sender_domain", "timestamp", "reviewed")
    list_filter = ("reviewed", "sender_domain")
    search_fields = ("msg_id", "subject", "body", "sender", "sender_domain")
    readonly_fields = ("msg_id", "subject", "body", "sender", "sender_domain", "timestamp")
    actions = ["mark_as_reviewed"]

    def mark_as_reviewed(self, request, queryset):
        updated = queryset.update(reviewed=True)
        self.message_user(request, f"{updated} entries marked as reviewed.")
    mark_as_reviewed.short_description = "Mark selected as reviewed"
    
   
class CustomAdminSite(admin.AdminSite):
    site_header = "Gmail Job Tracker Admin"
    site_title = "Gmail Job Tracker"
    index_title = "Dashboard"

    def each_context(self, request):
        context = super().each_context(request)
        context['message_count'] = Message.objects.count()
        return context

custom_admin_site = CustomAdminSite(name='custom_admin')

def mark_as_reviewed(modeladmin, request, queryset):
    queryset.update(reviewed=True)
mark_as_reviewed.short_description = "Mark selected applications as reviewed"

class ApplicationAdmin(admin.ModelAdmin):
    list_display = ("job_title", "company", "ml_label", "ml_confidence", "reviewed", "sent_date")
    list_filter = ("ml_label", "reviewed", "company_source")
    search_fields = ("job_title", "company__name", "thread_id", "ml_label")
    actions = [mark_as_reviewed]

class CompanyAdmin(admin.ModelAdmin):
    list_display = ("name", "domain")
    search_fields = ("name", "domain")

class MessageAdmin(admin.ModelAdmin):
    list_display = ("thread_id", "timestamp", "sender", "subject")
    search_fields = ("subject", "sender", "body")
from .models import Ticket

class TicketAdmin(admin.ModelAdmin):
    list_display = ("title", "category", "status", "updated_at")
    list_filter = ("category", "status")
    search_fields = ("title", "description")
    
custom_admin_site.register(Application, ApplicationAdmin)
custom_admin_site.register(Company, CompanyAdmin)
custom_admin_site.register(Message, MessageAdmin)
custom_admin_site.register(UnresolvedCompany, UnresolvedCompanyAdmin)
custom_admin_site.register(Ticket,TicketAdmin)

admin.site.register(KnownCompany)
admin.site.register(ATSDomain)
admin.site.register(DomainToCompany)
admin.site.register(CompanyAlias)

===== END OF FILE: tracker/admin.py =====



===== START OF FILE: tracker/apps.py =====

from django.apps import AppConfig


class TrackerConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "tracker"

    def ready(self):
        import tracker.signals  # üëà This loads your signal handlers

class TrackerConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'tracker'

===== END OF FILE: tracker/apps.py =====



===== START OF FILE: tracker/flagged.html =====

<h2>üõ†Ô∏è Flagged Applications for Manual Review</h2>
<table>
  <tr>
    <th>Subject</th>
    <th>Sender</th>
    <th>Company</th>
    <th>Source</th>
    <th>Edit</th>
  </tr>
  {% for app in applications %}
  <tr>
    <td>{{ app.subject }}</td>
    <td>{{ app.sender }}</td>
    <td>{{ app.company }}</td>
    <td>{{ app.company_source }}</td>
    <td><a href="{% url 'edit_application' app.id %}">Edit</a></td>
  </tr>
  {% endfor %}
</table>

===== END OF FILE: tracker/flagged.html =====



===== START OF FILE: tracker/forms.py =====

# tracker/forms.py
from django import forms
from tracker.models import Application

class ApplicationEditForm(forms.ModelForm):
    class Meta:
        model = Application
        fields = ['company', 'status']

===== END OF FILE: tracker/forms.py =====



===== START OF FILE: tracker/management/commands/export_companies.py =====

from django.core.management.base import BaseCommand
from tracker.models import Company
import json
from pathlib import Path

class Command(BaseCommand):
    help = "Export known companies to companies.json"

    def handle(self, *args, **kwargs):
        companies = Company.objects.all().values_list("name", flat=True)
        data = {"known": sorted(set(companies))}
        path = Path("companies.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        self.stdout.write(f"‚úÖ Exported {len(companies)} companies to {path}")

===== END OF FILE: tracker/management/commands/export_companies.py =====



===== START OF FILE: tracker/management/commands/export_labels.py =====

# tracker/management/commands/export_labels.py

import csv
from django.core.management.base import BaseCommand
from tracker.models import Application, Message   # ‚úÖ include Message

class Command(BaseCommand):
    help = "Export labeled Applications and Messages for ML training"

    def handle(self, *args, **kwargs):
        with open("labeled_subjects.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            # ‚úÖ Add a "type" column so you know if the row came from Application or Message
            writer.writerow(["type", "id", "subject", "body", "ml_label"])

            # --- Applications ---
            for app in Application.objects.filter(reviewed=True, ml_label__isnull=False):
                writer.writerow([
                    "application",
                    app.id,
                    getattr(app, "subject", ""),   # some apps may not have subject field
                    "",                            # Applications don‚Äôt have body text
                    app.ml_label
                ])

            # --- Messages ---
            for msg in Message.objects.filter(reviewed=True, ml_label__isnull=False):
                writer.writerow([
                    "message",
                    msg.id,
                    msg.subject,
                    msg.body,
                    msg.ml_label
                ])

===== END OF FILE: tracker/management/commands/export_labels.py =====



===== START OF FILE: tracker/management/commands/import_companies.py =====

from django.core.management.base import BaseCommand
from tracker.models import KnownCompany, ATSDomain, DomainToCompany, CompanyAlias
import json
from pathlib import Path

class Command(BaseCommand):
    help = "Import companies.json into the database"

    def handle(self, *args, **kwargs):
        path = Path(__file__).resolve().parent.parent.parent.parent / "companies.json"
        if not path.exists():
            self.stdout.write(self.style.ERROR("companies.json not found"))
            return

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        for name in data.get("known", []):
            KnownCompany.objects.get_or_create(name=name)

        for domain in data.get("ats_domains", []):
            ATSDomain.objects.get_or_create(domain=domain)

        for domain, company in data.get("domain_to_company", {}).items():
            DomainToCompany.objects.get_or_create(domain=domain, company=company)

        for alias, company in data.get("aliases", {}).items():
            CompanyAlias.objects.get_or_create(alias=alias, company=company)

        self.stdout.write(self.style.SUCCESS("‚úÖ companies.json imported successfully"))

===== END OF FILE: tracker/management/commands/import_companies.py =====



===== START OF FILE: tracker/management/commands/ingest_gmail.py =====

from django.core.management.base import BaseCommand
import os
import sys
from gmail_auth import get_gmail_service  # adjust if needed
from parser import ingest_message, parse_subject  # parse_subject now includes ML
import django
from tracker.models import IngestionStats
from datetime import datetime

# Setup Django environment
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "dashboard.settings")
django.setup()


class Command(BaseCommand):
    help = 'Ingest Gmail messages and populate job applications'

    def handle(self, *args, **kwargs):
        try:
            service = get_gmail_service()
            
            stats, _ = IngestionStats.objects.get_or_create(date=datetime.today().date())
            
            # Fetch Gmail messages
            results = service.users().messages().list(
                userId='me',
                labelIds=['INBOX'],  # or use a custom label like 'JobApps'
                maxResults=1000
            ).execute()
            
            print(f"üì® Fetching Gmail messages...")
            messages = results.get('messages', [])

            if not messages:
                self.stdout.write(self.style.WARNING("No Gmail messages found."))
                return

            for idx, msg in enumerate(messages, start=1):
                msg_id = msg['id']
                try:
                    # Fetch metadata first to inspect subject
                    msg_meta = service.users().messages().get(userId='me', id=msg_id, format='metadata', metadataHeaders=['Subject', 'From']).execute()
                    headers = {h['name']: h['value'] for h in msg_meta.get('payload', {}).get('headers', [])}
                    subject = headers.get('Subject', '')
                    sender = headers.get('From', '')
                    sender_domain = sender.split('@')[-1] if '@' in sender else None

                    parsed = parse_subject(subject, sender, sender_domain)
                    print(f"üì• Processing message: {subject}")
                    
                    if parsed.get('ignore'):
                        self.stdout.write(f"‚ö†Ô∏è Ignored: {subject}")
                        stats.total_fetched += 1
                        stats.save()
                        continue

                    ingest_message(service, msg_id)
                    stats.total_fetched += 1
                    stats.save()
                except Exception as e:
                    self.stderr.write(f"‚ùå Failed to ingest {msg_id}: {e}")

            self.stdout.write(self.style.SUCCESS(
                f"üìä Stats for {stats.date}: Fetched={stats.total_fetched}, Inserted={stats.total_inserted}, Ignored={stats.total_ignored}"
))
        except Exception as e:
            self.stderr.write(self.style.ERROR(f"‚ùå Ingestion failed: {e}"))

===== END OF FILE: tracker/management/commands/ingest_gmail.py =====



===== START OF FILE: tracker/management/commands/mark_ghosted.py =====

# mark_ghosted.py

from django.core.management.base import BaseCommand
from django.utils.timezone import now
from datetime import timedelta
from tracker.models import Message

class Command(BaseCommand):
    help = "Mark messages as ghosted if no follow-up received after X months"

    def handle(self, *args, **kwargs):
        cutoff = now() - timedelta(days=60)  # 2 months
        candidates = Message.objects.filter(
            ml_label__in=["job_application", "interview_invite"],
            timestamp__lte=cutoff,
            reviewed=True,
        ).exclude(company=None)

        ghosted_count = 0
        for msg in candidates:
            newer = Message.objects.filter(
                company=msg.company,
                timestamp__gt=msg.timestamp,
                reviewed=True
            ).exclude(ml_label="ghosted").exists()

            if not newer:
                msg.ml_label = "ghosted"
                msg.reviewed = True
                msg.save()
                ghosted_count += 1
                self.stdout.write(f"üëª Marked ghosted: {msg.subject} ({msg.company.name})")

        self.stdout.write(self.style.SUCCESS(f"‚úÖ {ghosted_count} messages marked as ghosted."))

===== END OF FILE: tracker/management/commands/mark_ghosted.py =====



===== START OF FILE: tracker/migrations/0001_initial.py =====

# Generated by Django 5.2.6 on 2025-10-13 03:33

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='ATSDomain',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('domain', models.CharField(max_length=255, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='Company',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(max_length=255)),
                ('domain', models.CharField(blank=True, max_length=255)),
                ('first_contact', models.DateTimeField()),
                ('last_contact', models.DateTimeField()),
                ('confidence', models.FloatField(blank=True, null=True)),
            ],
        ),
        migrations.CreateModel(
            name='CompanyAlias',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('alias', models.CharField(max_length=255, unique=True)),
                ('company', models.CharField(max_length=255)),
            ],
        ),
        migrations.CreateModel(
            name='DomainToCompany',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('domain', models.CharField(max_length=255, unique=True)),
                ('company', models.CharField(max_length=255)),
            ],
        ),
        migrations.CreateModel(
            name='IgnoredMessage',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('msg_id', models.CharField(max_length=128, unique=True)),
                ('subject', models.TextField()),
                ('body', models.TextField()),
                ('company_source', models.CharField(blank=True, max_length=50)),
                ('sender', models.CharField(max_length=256)),
                ('sender_domain', models.CharField(max_length=256)),
                ('date', models.DateTimeField()),
                ('reason', models.CharField(max_length=128)),
                ('logged_at', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='IngestionStats',
            fields=[
                ('date', models.DateField(primary_key=True, serialize=False)),
                ('total_fetched', models.IntegerField(default=0)),
                ('total_inserted', models.IntegerField(default=0)),
                ('total_ignored', models.IntegerField(default=0)),
                ('total_skipped', models.IntegerField(default=0)),
                ('last_updated', models.DateTimeField(auto_now=True)),
            ],
        ),
        migrations.CreateModel(
            name='KnownCompany',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(max_length=255, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='Ticket',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=255)),
                ('description', models.TextField()),
                ('category', models.CharField(choices=[('code', 'Code Problem'), ('admin_ui', 'Admin Site Web Problem'), ('upgrade', 'Admin Site Upgrade')], max_length=50)),
                ('status', models.CharField(choices=[('open', 'Open'), ('in_progress', 'In Progress'), ('resolved', 'Resolved'), ('wont_fix', "Won't Fix")], default='open', max_length=50)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
        ),
        migrations.CreateModel(
            name='UnresolvedCompany',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('msg_id', models.CharField(max_length=128, unique=True)),
                ('subject', models.TextField()),
                ('body', models.TextField()),
                ('sender', models.CharField(max_length=256)),
                ('sender_domain', models.CharField(max_length=256)),
                ('timestamp', models.DateTimeField()),
                ('notes', models.TextField(blank=True, null=True)),
                ('reviewed', models.BooleanField(default=False)),
            ],
        ),
        migrations.CreateModel(
            name='Application',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('thread_id', models.CharField(max_length=255, unique=True)),
                ('company_source', models.CharField(blank=True, max_length=50)),
                ('job_title', models.CharField(max_length=255)),
                ('job_id', models.CharField(blank=True, max_length=255)),
                ('status', models.CharField(max_length=50)),
                ('sent_date', models.DateField()),
                ('rejection_date', models.DateField(blank=True, null=True)),
                ('interview_date', models.DateField(blank=True, null=True)),
                ('ml_label', models.CharField(blank=True, max_length=50, null=True)),
                ('ml_confidence', models.FloatField(blank=True, null=True)),
                ('reviewed', models.BooleanField(default=False)),
                ('company', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='tracker.company')),
            ],
        ),
        migrations.CreateModel(
            name='Message',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('company_source', models.CharField(blank=True, max_length=50, null=True)),
                ('sender', models.CharField(max_length=255)),
                ('subject', models.TextField()),
                ('body', models.TextField()),
                ('timestamp', models.DateTimeField()),
                ('msg_id', models.CharField(max_length=255, unique=True)),
                ('thread_id', models.CharField(db_index=True, max_length=255)),
                ('ml_label', models.CharField(blank=True, max_length=50, null=True)),
                ('confidence', models.FloatField(blank=True, null=True)),
                ('reviewed', models.BooleanField(default=False)),
                ('company', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='tracker.company')),
            ],
        ),
    ]

===== END OF FILE: tracker/migrations/0001_initial.py =====



===== START OF FILE: tracker/migrations/0002_message_body_html.py =====

# Generated by Django 5.2.6 on 2025-10-13 20:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tracker', '0001_initial'),
    ]

    operations = [
        migrations.AddField(
            model_name='message',
            name='body_html',
            field=models.TextField(blank=True, null=True),
        ),
    ]

===== END OF FILE: tracker/migrations/0002_message_body_html.py =====



===== START OF FILE: tracker/migrations/__init__.py =====


===== END OF FILE: tracker/migrations/__init__.py =====



===== START OF FILE: tracker/models.py =====

# Create models here.
from django.db import models
from django.utils.timezone import now

class Company(models.Model):
    name = models.CharField(max_length=255)
    domain = models.CharField(max_length=255, blank=True)
    first_contact = models.DateTimeField()
    last_contact = models.DateTimeField()
    confidence = models.FloatField(null=True, blank=True)

    def __str__(self):
        return self.name

    def message_count(self):
        return self.message_set.count()

    def application_count(self):
        return self.application_set.count()

class Application(models.Model):
    thread_id = models.CharField(max_length=255, unique=True)
    company_source = models.CharField(max_length=50, blank=True)
    company = models.ForeignKey(Company, on_delete=models.CASCADE)
    job_title = models.CharField(max_length=255)
    job_id = models.CharField(max_length=255, blank=True)
    status = models.CharField(max_length=50)
    sent_date = models.DateField()
    rejection_date = models.DateField(null=True, blank=True)
    interview_date = models.DateField(null=True, blank=True)
    ml_label = models.CharField(max_length=50, blank=True, null=True)  # e.g., job_alert, noise
    ml_confidence = models.FloatField(blank=True, null=True)
    reviewed = models.BooleanField(default=False)
    def __str__(self):
        return f"{self.company.name} - {self.job_title}"

class Message(models.Model):
    company = models.ForeignKey(
        Company, null=True, blank=True, on_delete=models.SET_NULL
    )
    company_source = models.CharField(max_length=50, null=True, blank=True)  # ‚úÖ Add this
    sender = models.CharField(max_length=255)
    subject = models.TextField()
    body = models.TextField()
    body_html = models.TextField(blank=True, null=True)  # new field

    timestamp = models.DateTimeField()

    # Gmail identifiers
    msg_id = models.CharField(max_length=255, unique=True)  # NEW: unique Gmail messageId
    thread_id = models.CharField(max_length=255, db_index=True)   # keep, but index for grouping

    # Manual labeling for ML
    ml_label = models.CharField(max_length=50, null=True, blank=True)  # NEW
    confidence = models.FloatField(null=True, blank=True)   # ‚úÖ NEW
    reviewed = models.BooleanField(default=False)                      # NEW

    def __str__(self):
        company_name = self.company.name if self.company else "No Company"
        return f"{company_name} ‚Äì {self.timestamp.strftime('%Y-%m-%d %H:%M')}"
        
class IgnoredMessage(models.Model):
    msg_id = models.CharField(max_length=128, unique=True)
    subject = models.TextField()
    body = models.TextField()
    company_source=models.CharField(max_length=50, blank=True)
    sender = models.CharField(max_length=256)
    sender_domain = models.CharField(max_length=256)
    date = models.DateTimeField()
    reason = models.CharField(max_length=128)  # e.g., 'ml_ignore', 'low_confidence'
    logged_at = models.DateTimeField(auto_now_add=True)    
    
class IngestionStats(models.Model):
    date = models.DateField(primary_key=True)
    total_fetched = models.IntegerField(default=0)
    total_inserted = models.IntegerField(default=0)
    total_ignored = models.IntegerField(default=0)
    total_skipped = models.IntegerField(default=0)
    last_updated = models.DateTimeField(auto_now=True)
    
class UnresolvedCompany(models.Model):
    msg_id = models.CharField(max_length=128, unique=True)
    subject = models.TextField()
    body = models.TextField()
    sender = models.CharField(max_length=256)
    sender_domain = models.CharField(max_length=256)
    timestamp = models.DateTimeField()
    notes = models.TextField(blank=True, null=True)
    reviewed = models.BooleanField(default=False)
    
    def __str__(self):
        return f"{self.msg_id} ({self.sender_domain})"
    
class KnownCompany(models.Model):
    name = models.CharField(max_length=255, unique=True)

class ATSDomain(models.Model):
    domain = models.CharField(max_length=255, unique=True)

class DomainToCompany(models.Model):
    domain = models.CharField(max_length=255, unique=True)
    company = models.CharField(max_length=255)

class CompanyAlias(models.Model):
    alias = models.CharField(max_length=255, unique=True)
    company = models.CharField(max_length=255) 
    
class Ticket(models.Model):
    CATEGORY_CHOICES = [
        ("code", "Code Problem"),
        ("admin_ui", "Admin Site Web Problem"),
        ("upgrade", "Admin Site Upgrade"),
    ]

    STATUS_CHOICES = [
        ("open", "Open"),
        ("in_progress", "In Progress"),
        ("resolved", "Resolved"),
        ("wont_fix", "Won't Fix"),
    ]

    title = models.CharField(max_length=255)
    description = models.TextField()
    category = models.CharField(max_length=50, choices=CATEGORY_CHOICES)
    status = models.CharField(max_length=50, choices=STATUS_CHOICES, default="open")
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return f"[{self.category}] {self.title}"       

===== END OF FILE: tracker/models.py =====



===== START OF FILE: tracker/signals.py =====

from django.db.models.signals import post_save, post_delete
from django.dispatch import receiver
from .models import KnownCompany, ATSDomain, DomainToCompany, CompanyAlias
import json
from pathlib import Path

@receiver([post_save, post_delete], sender=KnownCompany)
@receiver([post_save, post_delete], sender=ATSDomain)
@receiver([post_save, post_delete], sender=DomainToCompany)
@receiver([post_save, post_delete], sender=CompanyAlias)
def export_companies(sender, **kwargs):
    known = list(KnownCompany.objects.values_list("name", flat=True))
    ats_domains = list(ATSDomain.objects.values_list("domain", flat=True))
    domain_to_company = {
        d["domain"]: d["company"]
        for d in DomainToCompany.objects.values("domain", "company")
    }
    aliases = {
        a["alias"]: a["company"]
        for a in CompanyAlias.objects.values("alias", "company")
    }

    data = {
        "ats_domains": ats_domains,
        "known": known,
        "domain_to_company": domain_to_company,
        "aliases": aliases,
    }

    with open("companies.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

===== END OF FILE: tracker/signals.py =====



===== START OF FILE: tracker/templates/admin/index.html =====

{% extends "admin/index.html" %}

{% block content %}
  <div class="dashboard-widget">
    <h2>Total Messages Ingested</h2>
    <p>{{ message_count }}</p>
  </div>
  {{ block.super }}
{% endblock %}

===== END OF FILE: tracker/templates/admin/index.html =====



===== START OF FILE: tracker/templates/registration/login.html =====

<!-- templates/registration/login.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Login</title>
</head>
<body>
    <h2>Login</h2>
    {% if form.errors %}
        <p style="color: red;">Invalid username or password.</p>
    {% endif %}
    <form method="post">
        {% csrf_token %}
        <label for="id_username">Username:</label>
        {{ form.username }}<br><br>
        <label for="id_password">Password:</label>
        {{ form.password }}<br><br>
        <button type="submit">Log in</button>
    </form>
</body>
</html>

===== END OF FILE: tracker/templates/registration/login.html =====



===== START OF FILE: tracker/templates/tracker/company_detail.html =====

<h1>{{ company.name }}</h1>

<h2>Applications</h2>
<ul>
  {% for app in applications %}
    <li>{{ app.job_title }} ‚Äî {{ app.status }} (Sent: {{ app.sent_date }})</li>
  {% empty %}
    <li>No applications</li>
  {% endfor %}
</ul>

<h2>Messages</h2>
<ul>
  {% for msg in messages %}
    <li><strong>{{ msg.timestamp|date:"Y-m-d H:i" }}</strong>: {{ msg.subject }}<br>{{ msg.body }}</li>
  {% empty %}
    <li>No messages</li>
  {% endfor %}
</ul>

===== END OF FILE: tracker/templates/tracker/company_detail.html =====



===== START OF FILE: tracker/templates/tracker/dashboard.html =====

<!--dashboard.html -->

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>üìä Gmail Job Tracker Dashboard</title>
  <style>
    body {
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif;
      margin: 2rem;
      background-color: #f9fafb;
      color: #333;
      line-height: 1.6;
    }
    h1, h2 {
      color: #1f2937;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }
    .tabs {
  display: flex;
  margin-top: 2rem;
  border-bottom: 2px solid #e5e7eb;
}
.tab {
  padding: 0.75rem 1.25rem;
  cursor: pointer;
  font-weight: 600;
  color: #374151;
  border: 1px solid transparent;
  border-bottom: none;
  background: #f3f4f6;
  margin-right: 0.5rem;
  border-radius: 6px 6px 0 0;
}
.tab:hover {
  background: #e5e7eb;
}
.tab.active {
  background: #fff;
  border-color: #d1d5db;
  border-bottom: 2px solid #fff;
}
.tab-content {
  display: none;
}
.tab-content.active {
  display: block;
}
    ul {
      list-style: none;
      padding: 0;
    }
    li {
      margin-bottom: 0.5rem;
    }
    .card {
      background: #fff;
      border-radius: 8px;
      padding: 1rem 1.5rem;
      margin-bottom: 1.5rem;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .stats li {
      font-weight: 500;
      margin: 0.25rem 0;
    }
    a {
      color: #2563eb;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    details {
      background: #f3f4f6;
      border-radius: 6px;
      margin-bottom: 1rem;
      padding: 0.75rem 1rem;
    }
    details summary {
      cursor: pointer;
      font-weight: 600;
      color: #111827;
    }
    details[open] {
      background: #e5e7eb;
    }
    .message {
      margin-left: 1rem;
      padding: 0.25rem 0;
      border-left: 3px solid #d1d5db;
      padding-left: 0.75rem;
      font-size: 0.95rem;
    }
    .timestamp {
      color: #6b7280;
      font-size: 0.85rem;
    }
    canvas {
      margin-top: 1rem;
    }
  </style>
</head>
<body>

  <h1>üìä Gmail Job Tracker Dashboard</h1>

  <div class="card">
    <ul class="stats">
      <li>Total Companies Contacted: {{ companies }}</li>
      <li>Total Applications Sent: {{ applications }}</li>
      <li>Rejections This Week: {{ rejections_week }}</li>
      <li>Interviews This Week: {{ interviews_week }}</li>
    </ul>
  </div>

  <!-- ‚úÖ Ingestion Stats card -->
  <div class="card">
    <h2>Ingestion Stats (Today)</h2>
    {% if latest_stats %}
      <ul class="stats">
        {% if latest_stats.total_inserted == 0 %}
        <div style="background:#fee2e2; padding:0.75rem; border-radius:6px; margin-top:1rem;">
          ‚ö†Ô∏è No new messages ingested today.</p>
        </div>
          {% endif %}
        <li>üì• Fetched: <strong>{{ latest_stats.total_fetched }}</strong></li>
        <li>‚úÖ Inserted: <strong>{{ latest_stats.total_inserted }}</strong></li>
        <li>üö´ Ignored: <strong>{{ latest_stats.total_ignored }}</strong></li>
        <li>‚è≠Ô∏è Skipped: <strong>{{ latest_stats.total_skipped }}</strong></li>
        <li>üìä Success Rate: 
          <strong>
            {{ latest_stats.total_inserted|floatformat:2 }} /
            {{ latest_stats.total_inserted|add:latest_stats.total_ignored|add:latest_stats.total_skipped }}
          </strong>
        </li>
      </ul>
      <p class="timestamp">Last updated: {{ latest_stats.date }}</p>
      <p class="timestamp">Last sync completed: {{ latest_stats.last_updated|date:"Y-m-d H:i" }}</p>
      <!-- Trend chart -->
      <canvas id="ingestionChart" height="120"></canvas>
    {% else %}
      <p>No ingestion stats available yet.</p>
    {% endif %}
  </div>

  <div class="card">
    <h2>Upcoming Interviews</h2>
    <ul>
      {% for interview in upcoming_interviews %}
        <li>{{ interview.company.name }} ‚Äî {{ interview.interview_date }}</li>
      {% empty %}
        <li>No upcoming interviews</li>
      {% endfor %}
    </ul>
  </div>

  <div class="card">
    <h2>Companies</h2>
    <ul>
      {% for company in companies_list %}
        <li><a href="{% url 'company_detail' company.id %}">{{ company.name }}</a></li>
      {% empty %}
        <li>No companies found</li>
      {% endfor %}
    </ul>
<div class="card">
  <h2>Labeling Tools</h2>
  <div class="tabs">
    <div class="tab active" onclick="showTab('applications')">Label Applications</div>
    <div class="tab" onclick="showTab('messages')">Label Messages</div>
    <div class="tab" onclick="showTab('unresolved')">Label Unresolved Companies</div>
  </div>

  <div id="applications" class="tab-content active">
    <p><a href="{% url 'label_applications' %}">‚û§ Go to Application Labeling</a></p>
  </div>

  <div id="messages" class="tab-content">
    <p><a href="{% url 'label_messages' %}">‚û§ Go to Message Labeling</a></p>
  </div>

  <div id="unresolved" class="tab-content">
    {% if unresolved_companies %}
      <table style="width:100%; border-collapse: collapse;">
        <thead>
          <tr style="background:#f3f4f6;">
            <th style="text-align:left; padding:0.5rem;">Message ID</th>
            <th style="text-align:left; padding:0.5rem;">Sender Domain</th>
            <th style="text-align:left; padding:0.5rem;">Timestamp</th>
            <th style="text-align:left; padding:0.5rem;">Subject</th>
            <th style="text-align:left; padding:0.5rem;">Notes</th>
          </tr>
        </thead>
        <tbody>
          {% for uc in unresolved_companies %}
          <tr>
            <td style="padding:0.5rem;">{{ uc.msg_id }}</td>
            <td style="padding:0.5rem;">{{ uc.sender_domain }}</td>
            <td style="padding:0.5rem;">{{ uc.timestamp|date:"Y-m-d H:i" }}</td>
            <td style="padding:0.5rem;">{{ uc.subject }}</td>
            <td style="padding:0.5rem;">{{ uc.notes|default:"‚Äî" }}</td>
          </tr>
          {% endfor %}
        </tbody>
      </table>
    {% else %}
      <p>No unresolved companies found.</p>
    {% endif %}
  </div>
</div>
</div>

<!-- ‚úÖ Recent Messages -->
<div class="card">
  <h2>Recent Messages</h2>
  <ul>
    {% for msg in messages %}
      <li>
        {% if msg.company %}
          <strong>{{ msg.company.name }}</strong>
        {% else %}
          <strong>No Company</strong>
        {% endif %}
        ‚Äî {{ msg.subject }}
        <span class="timestamp">({{ msg.timestamp|date:"Y-m-d H:i" }})</span>
      </li>
    {% empty %}
      <li>No messages found.</li>
    {% endfor %}
  </ul>
</div>

<h2>üßµ Recent Threads</h2>
{% for thread_id, msgs in threads %}
  <div class="thread" style="margin-bottom: 2em; padding: 1em; border: 1px solid #ddd;">
    <strong>Thread {{ thread_id }}</strong><br>
    {% for msg in msgs %}
      <div style="margin-top: 0.5em; padding-left: 1em;">
        <em>{{ msg.timestamp|date:"Y-m-d H:i" }}</em> ‚Äî
        {% if msg.company %}
          {{ msg.company.name }}
        {% else %}
          No Company
        {% endif %}
        <br>
        <strong>{{ msg.subject }}</strong><br>
        <span style="color: #6b7280;">Label: {{ msg.ml_label|default:"Unlabeled" }}</span><br>

          <div style="font-size: 0.9em; color: #4b5563;">
            <button onclick="toggleBody(this)" style="margin-bottom: 0.5rem; font-size: 0.75rem; background: #e5e7eb; border: none; padding: 0.25rem 0.5rem; border-radius: 4px; cursor: pointer;">
              Toggle View
            </button>
            <div class="rendered-body">
              {{ msg.body_html|default:"<em>No HTML body available.</em>"|safe }}
            </div>
            <pre class="raw-body" style="display: none; white-space: pre-wrap;">{{ msg.body }}</pre>
          </div>

      </div>
    {% endfor %}
  </div>
{% empty %}
  <p>No recent threads found.</p>
{% endfor %}



  <!-- ‚úÖ Chart.js integration -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script>
    {% if chart_labels %}
    const ctx = document.getElementById('ingestionChart').getContext('2d');
    const ingestionChart = new Chart(ctx, {
      type: 'bar',
      data: {
        labels: {{ chart_labels|safe }},
        datasets: [
          {
            label: 'Inserted',
            data: {{ chart_inserted|safe }},
            backgroundColor: '#10b981'
          },
          {
            label: 'Skipped',
            data: {{ chart_skipped|safe }},
            backgroundColor: '#f59e0b'
          },
          {
            label: 'Ignored',
            data: {{ chart_ignored|safe }},
            backgroundColor: '#ef4444'
          }
        ]
      },
      options: {
        responsive: true,
        plugins: {
          legend: { position: 'bottom' },
          title: { display: true, text: 'Ingestion Activity (Last 7 Days)' },
          tooltip: {
            mode: 'index',
            intersect: false
          }
        },
        scales: {
          x: { stacked: true },
          y: { stacked: true, beginAtZero: true }
        }
      }
    });
    {% endif %}
  </script>

<script>
  function showTab(tabId) {
    document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
    document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
    document.getElementById(tabId).classList.add('active');
    document.querySelector(`.tab[onclick="showTab('${tabId}')"]`).classList.add('active');
  }
</script>  
<script>
  function toggleBody(button) {
    const container = button.parentElement;
    const rendered = container.querySelector(".rendered-body");
    const raw = container.querySelector(".raw-body");

    if (rendered.style.display === "none") {
      rendered.style.display = "block";
      raw.style.display = "none";
      button.textContent = "Toggle View";
    } else {
      rendered.style.display = "none";
      raw.style.display = "block";
      button.textContent = "Toggle View";
    }
  }
</script>

</body>
</html>

===== END OF FILE: tracker/templates/tracker/dashboard.html =====



===== START OF FILE: tracker/templates/tracker/label_applications.html =====

<!-- templates/tracker/label_applications.html -->

<h2>Label Unreviewed Applications</h2>
<form method="post">
  {% csrf_token %}
  <table>
    <tr>
      <th>Company</th>
      <th>Job Title</th>
      <th>Status</th>
      <th>Label</th>
    </tr>
    {% for app in applications %}
    <tr>
      <td>{{ app.company.name }}</td>
      <td>{{ app.job_title }}</td>
      <td>{{ app.status }}</td>
      <td>
        <select name="label_{{ app.id }}">
          <option value="">--</option>
          <option value="job_application">Job Application</option>
          <option value="job_alert">Job Alert</option>
          <option value="interview_invite">Interview Invite</option>
          <option value="noise">Noise</option>
        </select>
      </td>
    </tr>
    {% endfor %}
  </table>
  <button type="submit">Save Labels</button>
</form>

===== END OF FILE: tracker/templates/tracker/label_applications.html =====



===== START OF FILE: tracker/templates/tracker/label_messages.html =====

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Label Messages</title>
  <p>
  Reviewed: {{ total_reviewed }} | Remaining: {{ total_unreviewed }}
  </p>
  <style>
    body {
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif;
      font-size: 0.8rem; /* or 0.8rem for a tighter look */
      margin: 2rem;
      background-color: #f9fafb;
      color: #333;
      line-height: 1.6;
    }
    h1 {
      color: #1f2937;
      margin-bottom: 1.5rem;
    }
    form {
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }
    .card {
      clear: both;
      background: #fff;
      border-radius: 8px;
      padding: 1rem 1.25rem;
      margin-bottom: 1.5rem;
      margin-top: 1rem;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .subject {
      font-weight: 600;
      margin-bottom: 0.25rem;
    }
    .body {
      font-size: 0.8rem;
      color: #4b5563;
      margin-bottom: 0.5rem;
    }
    .confidence {
      font-size: 0.8rem;
      color: #6b7280;
      margin-bottom: 0.75rem;
    }
    label {
      font-size: 0.8rem;
      font-weight: 500;
      margin-right: 0.5rem;
    }
    select {
      padding: 0.2rem 0.2rem;
      border-radius: 4px;
      border: 1px solid #d1d5db;
      font-size: 0.8rem;
    }
    button {
      align-self: flex-start;
      background-color: #2563eb;
      color: white;
      font-weight: 600;
      padding: 0.5rem 1rem;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      transition: background-color 0.2s;
    }
    button:hover {
      background-color: #1e40af;
    }
    table {
      font-size: 0.8rem;
      border-spacing: 0;
    }

    th {
      font-weight: 600;
      color: #374151;
    }

    td {
      vertical-align: top;
    }
  </style>
</head>
<body>

  <h1>üìù Label Messages</h1>
  <form method="post" name="label_form">
    {% csrf_token %}
    <table style="width: 100%; border-collapse: collapse; margin-top: 1.5rem;">
      <thead>
        <tr style="background-color: #f3f4f6; text-align: left;">
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">DateTime</th>
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">Company</th>
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">Subject</th>
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">Body</th>
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">Confidence</th>
          <th style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">Assign Label</th>
        </tr>
      </thead>
      <tbody>
        {% for msg in messages %}
        <tr style="vertical-align: top; {% if msg.ml_label == 'ghosted' %}background-color: #fef3c7;{% endif %}">
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">
            {{ msg.timestamp|date:"Y-m-d H:i" }}
          </td>
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">
            {% if msg.company %}
              {{ msg.company.name }}
            {% else %}
              No Company
            {% endif %}
          </td>
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">
            {{ msg.subject }}
          </td>
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb; max-width: 400px;">
            <details>
              <summary style="cursor: pointer; font-weight: 500; color: #2563eb;">
                View message body
              </summary>
              <div style="margin-top: 0.5rem; font-size: 0.8em; color: #4b5563;">
                {{ msg.rendered_body|safe }}
              </div>
            </details>
          </td>
        </td>
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">
            {{ msg.confidence|default:"N/A"|floatformat:2 }}
          </td>
          <td style="padding: 0.5rem; border-bottom: 1px solid #e5e7eb;">
            <select name="label_{{ msg.id }}" id="label_{{ msg.id }}" title="Assign label to message {{ msg.id }}">
              <option value="">--</option>
              <option value="job_application">Application</option>
              <option value="interview_invite">Interview</option>
              <option value="rejection">Rejection</option>
              <option value="head_hunter">Head Hunter</option>
              <option value="job_alert">Job Alert</option>
              <option value="ghosted">Ghosted</option>
              <option value="noise">Noise</option>
            </select>
          </td>
        </tr>
        {% empty %}
        <tr>
          <td colspan="6" style="padding: 1rem; text-align: center;">No unlabeled messages found.</td>
        </tr>
        {% endfor %}
      </tbody>
    </table>
    <button type="submit">Submit Labels</button>
  </form>

  {% if training_output %}
  <div class="card mt-4">
    <div class="card-header">
      <strong>üìä Model Training Summary</strong>
      <button class="btn btn-sm btn-secondary float-end" onclick="toggleTrainingOutput()">Toggle</button>
    </div>
    <div class="card-body" id="training-output" style="display: none;">
      <pre style="white-space: pre-wrap;">{{ training_output }}</pre>
    </div>
  </div>

  <script>
  function toggleTrainingOutput() {
    const block = document.getElementById("training-output");
    block.style.display = block.style.display === "none" ? "block" : "none";
  }
  </script>
  {% endif %}
</body>
</html>

===== END OF FILE: tracker/templates/tracker/label_messages.html =====



===== START OF FILE: tracker/templates/tracker/manage_aliases.html =====

{% extends "base.html" %}
{% block content %}
<h2>Alias Suggestions</h2>
<form method="POST" action="{% url 'approve_bulk_aliases' %}">
  {% csrf_token %}
  <table class="table table-bordered">
    <thead>
      <tr>
        <th>Select</th>
        <th>Alias</th>
        <th>Suggested</th>
        <th>Reason</th>
        <th>Confidence</th>
        <th>Thread ID</th>
        <th>Subject</th>
        <th>Actions</th>
      </tr>
    </thead>
    <tbody>
      {% for s in suggestions %}
      <tr {% if s.confidence >= 0.85 %}class="table-success"{% endif %}>
        <td><input type="checkbox" name="alias" value="{{ s.alias }}">
            <input type="hidden" name="suggested" value="{{ s.suggested }}">
            <input type="hidden" name="timestamp" value="{{ s.timestamp }}">
        </td>
        <td>{{ s.alias }}</td>
        <td>{{ s.suggested }}</td>
        <td>{{ s.reason }}</td>
        <td>{{ s.confidence }}</td>
        <td>{{ s.source_thread_id }}</td>
        <td>{{ s.source_subject }}</td>
        <td>
          <form method="POST" action="{% url 'reject_alias' %}">
            {% csrf_token %}
            <input type="hidden" name="alias" value="{{ s.alias }}">
            <input type="hidden" name="timestamp" value="{{ s.timestamp }}">
            <button class="btn btn-danger btn-sm">Reject</button>
          </form>
        </td>
      </tr>
      {% endfor %}
    </tbody>
  </table>
  <button class="btn btn-primary">Approve Selected</button>
</form>
{% endblock %}

===== END OF FILE: tracker/templates/tracker/manage_aliases.html =====



===== START OF FILE: tracker/tests/__init__.py =====


===== END OF FILE: tracker/tests/__init__.py =====



===== START OF FILE: tracker/tests/conftest.py =====

import pytest
from tracker.tests.test_helpers import FakeManager

@pytest.fixture
def fake_message_model(monkeypatch):
    fake_manager = FakeManager()

    class FakeMessage:
        objects = fake_manager

    monkeypatch.setattr("parser.Message", FakeMessage)
    return fake_manager.queryset, fake_manager

@pytest.fixture
def fake_stats():
    class Stats:
        total_ignored = 0
        total_skipped = 0
        total_inserted = 0
        def save(self): pass
    return Stats()

===== END OF FILE: tracker/tests/conftest.py =====



===== START OF FILE: tracker/tests/test_helpers.py =====

class FakeMessageRecord:
    def __init__(self, data):
        self.__dict__.update(data)
    def save(self): pass

class FakeQuerySet:
    def __init__(self):
        self._first = None
    def first(self): return self._first
    def set_first(self, obj): self._first = obj

class FakeManager:
    def __init__(self):
        self.created = []
        self.queryset = FakeQuerySet()
    def filter(self, **kwargs): return self.queryset
    def create(self, **kwargs):
        self.created.append(kwargs)
        return FakeMessageRecord(kwargs)

===== END OF FILE: tracker/tests/test_helpers.py =====



===== START OF FILE: tracker/tests/test_ingest_message.py =====

# test_ingest_message.py
import pytest
from parser import ingest_message, log_ignored_message, parse_subject
from db import is_valid_company
from datetime import datetime
from django.utils.timezone import make_aware
from tracker.tests.test_helpers import FakeMessageRecord, FakeManager, FakeQuerySet

pytestmark = pytest.mark.django_db
timestamp = make_aware(datetime(2025, 9, 29, 12, 0))

def test_subject_with_job_title_at_company(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model  # ‚úÖ Only if fixture returns a tuple
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))
    subject_line = "We Got It: Thanks for applying for Field CTO position @ Claroty!"
    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": subject_line,
        "body": "Thanks for your interest in Claroty.",
        "date": "2025-10-08",
        "thread_id": "t11",
        "sender": "Claroty Recruiting <jobs@claroty.com>",
        "sender_domain": "claroty.com",
        "timestamp": timestamp,
        "labels": [],
        "last_updated": "now"
    })

    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": "2025-10-08",
        "follow_up_dates": [],
        "rejection_date": None,
        "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    # Let parse_subject run normally to test regex + sanity logic

    monkeypatch.setattr("parser.parse_subject", parse_subject)

    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "claroty_field_cto")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m11")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    # ‚úÖ Confirm correct company extraction
    assert captured_record["company"] == "Claroty"
    assert captured_record["job_title"].lower() == "field cto"
    assert captured_record["company_source"] == "subject_parse"
    
def test_ingest_ignored_reason_logging(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured = {}

    # Patch log_ignored_message to capture its arguments
    monkeypatch.setattr("parser.log_ignored_message", lambda msg_id, metadata, reason: captured.update({
        "msg_id": msg_id,
        "reason": reason,
        "subject": metadata["subject"],
        "sender": metadata["sender"]
    }))

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t8",
        "sender": "x", "sender_domain": "example.com",
        "timestamp": timestamp, "labels": [], "last_updated": "now"
    })
    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    # Parsed subject flags the message as ignored
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": True,
        "ignore_reason": "ml_ignore",
        "company": "", "job_title": "", "job_id": ""
    })

    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m8")
    assert result == "ignored"
    assert fake_stats.total_ignored == 1

    # ‚úÖ Confirm log_ignored_message was called with correct values
    assert captured["msg_id"] == "m8"
    assert captured["reason"] == "ml_ignore"
    assert captured["subject"] == "foo"
    assert captured["sender"] == "x"

@pytest.fixture
def fake_stats():
    class Stats:
        total_ignored = 0
        total_skipped = 0
        total_inserted = 0
        def save(self): pass
    return Stats()
   
def test_ingest_ignored(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {"subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t1", "sender": "x", "sender_domain": "y", "timestamp": timestamp, "labels": [], "last_updated": "now"})
    monkeypatch.setattr("parser.classify_message", lambda b: None)
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {"response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None})
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {"ignore": True})
    monkeypatch.setattr("parser.log_ignored_message", lambda *a, **k: None)
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m1")
    assert result == "ignored"
    assert fake_stats.total_ignored == 1

def test_ingest_skipped(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    timestamp = make_aware(datetime(2025, 9, 29, 12, 0))

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo",
        "body": "bar",
        "date": "2025-09-29",
        "thread_id": "t1",
        "sender": "x",
        "sender_domain": "y",
        "timestamp": timestamp,
        "labels": [],
        "last_updated": "now"
    })

    monkeypatch.setattr("parser.classify_message", lambda b: {"label": "skipped"})
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None,
        "follow_up_dates": [],
        "rejection_date": None,
        "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {"ignore": False})
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    queryset, manager = fake_message_model
    queryset.set_first(FakeMessageRecord({"msg_id": "m2"}))

    result = ingest_message(None, "m2")
    assert result == "skipped"
    assert fake_stats.total_skipped == 1  

def test_ingest_domain_mapping(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))
    monkeypatch.setattr("parser.DOMAIN_TO_COMPANY", {"example.com": "MappedCo"})

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t1",
        "sender": "x", "sender_domain": "example.com", "timestamp":  timestamp, "labels": [], "last_updated": "now"
    })
    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False, "company": "", "job_title": "Engineer", "job_id": "123"
    })
    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "test_index")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m4")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1
    assert captured_record["company"] == "MappedCo"
    assert captured_record["company_source"] == "domain_mapping"

def test_ingest_subject_parse(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t1",
        "sender": "x", "sender_domain": "y", "timestamp": timestamp, "labels": [], "last_updated": "now"
    })
    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False, "company": "TestCo", "job_title": "Engineer", "job_id": "123"
    })
    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "test_index")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m3")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    assert len(manager.created) == 1
    # ‚úÖ Verify the inserted message content
    assert manager.created[0]["subject"] == "foo"
    assert manager.created[0]["thread_id"] == "t1"

    # ‚úÖ Verify the final record
    assert captured_record["company"] == "TestCo"
    assert captured_record["company_source"] == "subject_parse"
    assert captured_record["job_title"] == "Engineer"
    assert captured_record["company_job_index"] == "test_index"
    assert captured_record["subject"] == "foo"
    assert captured_record["status"] == "applied"
    
def test_ingest_sender_name_match(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))

    # Patch known companies
    monkeypatch.setattr("parser.KNOWN_COMPANIES", ["Airbnb"])

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t6",
        "sender": "Airbnb Recruiting <jobs@airbnb.com>", "sender_domain": "airbnb.com",
        "timestamp":  timestamp, "labels": [], "last_updated": "now"
    })
    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    # No company in parsed subject
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False, "company": "", "job_title": "Engineer", "job_id": "123"
    })
    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "test_index")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m6")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    assert captured_record["company"] == "Airbnb"
    assert captured_record["company_source"] == "sender_name_match"
    assert captured_record["job_title"] == "Engineer"
    assert captured_record["company_job_index"] == "test_index"    
    
def test_ingest_company_rejection(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))

    # Patch known companies and validation logic from db
    monkeypatch.setattr("parser.KNOWN_COMPANIES", [])
    monkeypatch.setattr("parser.is_valid_company", lambda name: False)

    # Patch domain mapping to catch fallback
    monkeypatch.setattr("parser.DOMAIN_TO_COMPANY", {"example.com": "FallbackCo"})

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo", "body": "bar", "date": "2025-09-29", "thread_id": "t7",
        "sender": "x", "sender_domain": "example.com",
        "timestamp": timestamp, "labels": [], "last_updated": "now"
    })
    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None, "follow_up_dates": [], "rejection_date": None, "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    # Parsed subject returns a bad company name
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False, "company": "careers", "job_title": "Engineer", "job_id": "123"
    })
    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "test_index")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m7")
    
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    # ‚úÖ Confirm company was rejected and fallback used
    assert captured_record["company"] == "FallbackCo"
    assert captured_record["company_source"] == "domain_mapping"
    
def test_ingest_ml_fallback(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))

    # Patch ML prediction directly
    monkeypatch.setattr("parser.predict_company", lambda subject, body: "MLCo")
    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo",
        "body": "This is a job application email",
        "date": "2025-09-29",
        "thread_id": "t9",
        "sender": "x",
        "sender_domain": "unknown.com",
        "timestamp": timestamp,
        "labels": [],
        "last_updated": "now"
    })

    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": None,
        "follow_up_dates": [],
        "rejection_date": None,
        "interview_date": None
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    # Parsed subject returns no company
    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False,
        "company": "",
        "job_title": "Engineer",
        "job_id": "123"
    })

    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "test_index")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m9")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    # ‚úÖ Confirm ML prediction was used
    assert captured_record["company"] == "MLCo"
    assert captured_record["company_source"] == "ml_prediction"
    assert captured_record["job_title"] == "Engineer"
    assert captured_record["company_job_index"] == "test_index"
    
    
def test_ingest_record_shape(monkeypatch, fake_stats, fake_message_model):
    queryset, manager = fake_message_model
    captured_record = {}
    monkeypatch.setattr("parser.insert_or_update_application", lambda record: captured_record.update(record))

    monkeypatch.setattr("parser.extract_metadata", lambda s, m: {
        "subject": "foo",
        "body": "This is a job application email",
        "date": "2025-09-29",
        "thread_id": "t10",
        "sender": "x",
        "sender_domain": "example.com",
        "timestamp": timestamp,
        "labels": ["inbox", "jobs"],
        "last_updated": "now"
    })

    monkeypatch.setattr("parser.classify_message", lambda b: "applied")
    monkeypatch.setattr("parser.extract_status_dates", lambda b, d: {
        "response_date": "2025-09-30",
        "follow_up_dates": ["2025-10-02"],
        "rejection_date": None,
        "interview_date": "2025-10-05"
    })
    monkeypatch.setattr("parser.insert_email_text", lambda *a, **k: None)

    monkeypatch.setattr("parser.parse_subject", lambda *a, **k: {
        "ignore": False,
        "company": "TestCorp",
        "job_title": "Engineer",
        "job_id": "123",
        "predicted_company": "TestCorp"
    })

    monkeypatch.setattr("parser.build_company_job_index", lambda *a, **k: "testcorp_engineer_123")
    monkeypatch.setattr("parser.get_stats", lambda: fake_stats)

    result = ingest_message(None, "m10")
    assert result == "inserted"
    assert fake_stats.total_inserted == 1

    # ‚úÖ Confirm full record shape
    assert captured_record == {
        "thread_id": "t10",
        "company": "TestCorp",
        "predicted_company": "TestCorp",
        "job_title": "Engineer",
        "job_id": "123",
        "first_sent": "2025-09-29",
        "response_date": "2025-09-30",
        "follow_up_dates": ["2025-10-02"],
        "rejection_date": None,
        "interview_date": "2025-10-05",
        "status": "applied",
        "labels": ["inbox", "jobs"],
        "subject": "foo",
        "sender": "x",
        "sender_domain": "example.com",
        "last_updated": "now",
        "company_source": "subject_parse",
        "company_job_index": "testcorp_engineer_123"
    }

===== END OF FILE: tracker/tests/test_ingest_message.py =====



===== START OF FILE: tracker/urls.py =====

from django.urls import path, include
from . import views
from tracker.admin import custom_admin_site, admin
urlpatterns = [
    path('', views.dashboard, name='dashboard'),
    path('company/<int:company_id>/', views.company_detail, name='company_detail'),
    path("label/", views.label_applications, name="label_applications"),
    path("label_messages/", views.label_messages, name="label_messages"),
    path("admin/", custom_admin_site.urls, name="custom_admin"),
    path("django_admin/", admin.site.urls, name="django_admin"),
    path("aliases/manage/", views.manage_aliases, name="manage_aliases"),
    path("aliases/approve_bulk/", views.approve_bulk_aliases, name="approve_bulk_aliases"),
    path("aliases/reject/", views.reject_alias, name="reject_alias"),
]

===== END OF FILE: tracker/urls.py =====



===== START OF FILE: tracker/views.py =====

# tracker/views.py

import json
import sys
from bs4 import BeautifulSoup
from django.shortcuts import render, redirect, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.views.decorators.csrf import csrf_exempt
from django.utils.timezone import now
from django.db import models
from django.db.models import F, Q
from tracker.models import Company, Application, Message, IngestionStats, UnresolvedCompany
from datetime import timedelta
from collections import defaultdict
from tracker.forms import ApplicationEditForm
from pathlib import Path
import subprocess

python_path = sys.executable
ALIAS_EXPORT_PATH = Path("alias_candidates.json")
PATTERNS_PATH = Path("patterns.json")
ALIAS_LOG_PATH = Path("alias_approvals.csv")
ALIAS_REJECT_LOG_PATH = Path("alias_rejections.csv")

@login_required
def manage_aliases(request):
    if not ALIAS_EXPORT_PATH.exists():
        return render(request, "tracker/manage_aliases.html", {"suggestions": []})

    with open(ALIAS_EXPORT_PATH, "r", encoding="utf-8") as f:
        suggestions = json.load(f)

    return render(request, "tracker/manage_aliases.html", {"suggestions": suggestions})

@csrf_exempt
def approve_bulk_aliases(request):
    if request.method == "POST":
        aliases = request.POST.getlist("alias")
        suggested = request.POST.getlist("suggested")

        # Load patterns
        if PATTERNS_PATH.exists():
            with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
                patterns = json.load(f)
        else:
            patterns = {"aliases": {}, "ignore": []}

        for alias, suggestion in zip(aliases, suggested):
            patterns["aliases"][alias] = suggestion
            with open(ALIAS_LOG_PATH, "a", encoding="utf-8") as log:
                log.write(f"{alias},{suggestion},{request.POST.get('timestamp')}\n")

        with open(PATTERNS_PATH, "w", encoding="utf-8") as f:
            json.dump(patterns, f, indent=2)

        return redirect("manage_aliases")

@csrf_exempt
def reject_alias(request):
    if request.method == "POST":
        alias = request.POST.get("alias")

        # Load patterns
        if PATTERNS_PATH.exists():
            with open(PATTERNS_PATH, "r", encoding="utf-8") as f:
                patterns = json.load(f)
        else:
            patterns = {"aliases": {}, "ignore": []}

        if alias not in patterns["ignore"]:
            patterns["ignore"].append(alias)

        with open(PATTERNS_PATH, "w", encoding="utf-8") as f:
            json.dump(patterns, f, indent=2)

        with open(ALIAS_REJECT_LOG_PATH, "a", encoding="utf-8") as log:
            log.write(f"{alias},{request.POST.get('timestamp')}\n")

        return redirect("manage_aliases")

def edit_application(request, pk):
    app = get_object_or_404(Application, pk=pk)
    if request.method == 'POST':
        form = ApplicationEditForm(request.POST, instance=app)
        if form.is_valid():
            form.save()
            return redirect('flagged_applications')
    else:
        form = ApplicationEditForm(instance=app)
    return render(request, 'tracker/edit.html', {'form': form})

def flagged_applications(request):
    flagged = Application.objects.filter(
        models.Q(company="") |
        models.Q(company_source__in=["none", "ml_prediction", "sender_name_match"])
    ).order_by('-first_sent')[:100]

    return render(request, 'tracker/flagged.html', {'applications': flagged})

@login_required
def dashboard(request):
    def clean_html(raw_html):
        soup = BeautifulSoup(raw_html, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        return str(soup)

    companies = Company.objects.count()
    companies_list = Company.objects.all()
    unresolved_companies = UnresolvedCompany.objects.filter(reviewed=False).order_by("-timestamp")[:50]
    
    applications = Application.objects.count()
    rejections_week = Application.objects.filter(
        rejection_date__gte=now() - timedelta(days=7)
    ).count()
    interviews_week = Application.objects.filter(
        interview_date__gte=now() - timedelta(days=7)
    ).count()
    upcoming_interviews = Application.objects.filter(
        interview_date__gte=now()
    ).order_by('interview_date')

    # ‚úÖ Recent messages with company preloaded
    messages = Message.objects.select_related("company").order_by('-timestamp')[:100]
    for msg in messages:
        raw_html = msg.body or ""
        msg.cleaned_body_html = extract_body_content(raw_html)

    # ‚úÖ Group messages by thread_id with company preloaded
    threads = defaultdict(list)
    seen = set()

    for msg in Message.objects.select_related("company").order_by("thread_id", "timestamp"):
        if msg.msg_id not in seen:
            threads[msg.thread_id].append(msg)
            seen.add(msg.msg_id)

    # ‚úÖ Filter to threads with >1 message, then sort and slice
    thread_list = sorted(
        [(tid, msgs) for tid, msgs in threads.items() if len(msgs) > 1],
        key=lambda t: t[1][-1].timestamp,
        reverse=True
    )[:50]

    # ‚úÖ Ingestion stats
    latest_stats = IngestionStats.objects.order_by('-date').first()

    ingested_today = latest_stats.total_inserted if latest_stats else 0
    ignored_today = latest_stats.total_ignored if latest_stats else 0
    skipped_today = latest_stats.total_skipped if latest_stats else 0
    
    # Last 7 days for chart
    seven_days_ago = now().date() - timedelta(days=6)
    stats_qs = IngestionStats.objects.filter(date__gte=seven_days_ago).order_by("date")

    chart_labels = [s.date.strftime("%Y-%m-%d") for s in stats_qs]
    chart_inserted = [s.total_inserted for s in stats_qs]
    chart_skipped = [s.total_skipped for s in stats_qs]
    chart_ignored = [s.total_ignored for s in stats_qs]

    return render(request, "tracker/dashboard.html", {
        "companies": companies,
        "companies_list": companies_list,
        "applications": applications,
        "rejections_week": rejections_week,
        "interviews_week": interviews_week,
        "upcoming_interviews": upcoming_interviews,
        "messages": messages,
        "threads": thread_list,
        "latest_stats": latest_stats,
        "chart_labels": chart_labels,
        "chart_inserted": chart_inserted,
        "chart_skipped": chart_skipped,
        "chart_ignored": chart_ignored,
        "unresolved_companies": unresolved_companies, 
        "ingested_today": ingested_today,
        "ignored_today": ignored_today,
        "skipped_today": skipped_today,
    })
  
    
def company_detail(request, company_id):
    company = get_object_or_404(Company, pk=company_id)
    applications = Application.objects.filter(company=company)
    messages = Message.objects.filter(company=company).order_by('timestamp')

    return render(request, "tracker/company_detail.html", {
        "company": company,
        "applications": applications,
        "messages": messages,
    })

def extract_body_content(raw_html):
    soup = BeautifulSoup(raw_html, "html.parser")

    # Remove script/style/noscript
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # Extract body content if present
    body = soup.body
    return str(body) if body else soup.get_text(separator=" ", strip=True)


def label_applications(request):
    if request.method == "POST":
        for key, value in request.POST.items():
            if key.startswith("label_") and value:
                app_id = int(key.split("_")[1])
                try:
                    app = Application.objects.get(pk=app_id)
                    app.ml_label = value
                    app.reviewed = True
                    app.save()
                except Message.DoesNotExist:
                    continue
              
        return redirect("label_applications")

    apps = Application.objects.filter(reviewed=False).order_by("sent_date")[:50]
    return render(request, "tracker/label_applications.html", {"applications": apps})

@login_required
def label_messages(request):
    training_output = None  # ‚úÖ Initialize outside POST block

    if request.method == "POST":
        for key, value in request.POST.items():
            if key.startswith("label_") and value:
                msg_id = int(key.split("_")[1])
                try:
                    msg = Message.objects.get(pk=msg_id)
                    msg.ml_label = value
                    msg.reviewed = True
                    msg.save()
                except Message.DoesNotExist:
                    continue

        # ‚úÖ Trigger model retraining and capture output
        try:
            result = subprocess.run(
                [python_path, "train_model.py"],
                capture_output=True,
                text=True,
                check=True
            )
            training_output = result.stdout
            print("‚úÖ Model retrained successfully.")
            print(result.stdout)
        except subprocess.CalledProcessError as e:
            training_output = f"‚ùå Model retraining failed:\n{e.stderr}"
            print(training_output)

    # ‚úÖ Continue rendering page with training_output in context
    filter_label = request.GET.get("label")
    qs = Message.objects.filter(reviewed=False)
    if filter_label:
        qs = qs.filter(ml_label=filter_label)

    msgs = qs.order_by(F("confidence").asc(nulls_first=True), "timestamp")[:50]

    for msg in msgs:
        msg.rendered_body = extract_body_content(msg.body or "")

    total_unreviewed = Message.objects.filter(reviewed=False).count()
    total_reviewed = Message.objects.filter(reviewed=True).count()

    return render(request, "tracker/label_messages.html", {
        "messages": msgs,
        "filter_label": filter_label,
        "total_unreviewed": total_unreviewed,
        "total_reviewed": total_reviewed,
        "training_output": training_output  # ‚úÖ Pass to template
    })

===== END OF FILE: tracker/views.py =====



===== START OF FILE: train_model.py =====

import os
import joblib
import pandas as pd
import json
from db import load_training_data
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime
import argparse


# --- Config ---
EXPORT_PATH = "labeled_subjects.csv"
MODEL_DIR = "model"
os.makedirs(MODEL_DIR, exist_ok=True)

parser = argparse.ArgumentParser(description="Train or retrain the ML model.")

parser.add_argument("--verbose", action="store_true",help="Enable verbose output")
args = parser.parse_args()
                    
print(f"[OK] Training started at {datetime.now().isoformat()}")

# --- Branch 1: If we have manually labeled messages, train on them ---
if os.path.exists(EXPORT_PATH):
    print(f"[OK] Found {EXPORT_PATH}, training on manually labeled messages...")
    df = pd.read_csv(EXPORT_PATH)
    if args.verbose:
        print(df.head())

    required_cols = ["subject", "body", "ml_label", "type"]
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        print(f"[Error] Missing columns in labeled_subjects.csv: {missing}")
        exit(1)
    # Filter only message rows
    df = df[df["type"] == "message"]

    if df.empty:
        print("[Warning[ No labeled messages found in export, falling back to company training.")
        df = None
    else:
        # Prepare text field
        df["text"] = df["subject"].fillna("") + " " + df["body"].fillna("")

        # Encode labels
        le = LabelEncoder()
        df["label_encoded"] = le.fit_transform(df["ml_label"])

        # Filter sparse classes
        label_counts = df["ml_label"].value_counts()
        df = df[df["ml_label"].isin(label_counts[label_counts > 1].index)]

        # Re-encode after filtering
        le = LabelEncoder()
        df["label_encoded"] = le.fit_transform(df["ml_label"])

        print("Label distribution:", df["ml_label"].value_counts())

        # Safety check
        if df["label_encoded"].nunique() < 2:
            print("üö´ Not enough unique labels to train a classifier.")
            exit(0)

        # Vectorize
        vectorizer = TfidfVectorizer(
            stop_words="english",
            max_features=5000,
            ngram_range=(1, 2),
        )
        X = vectorizer.fit_transform(df["text"])
        y = df["label_encoded"]

        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Train model
        clf = LogisticRegression(max_iter=1000, class_weight="balanced")
        clf.fit(X_train, y_train)

        # Evaluate
        y_pred = clf.predict(X_test)
        labels_in_test = sorted(set(y_test))
        target_names = le.inverse_transform(labels_in_test)
        print(classification_report(y_test, y_pred, labels=labels_in_test, target_names=target_names))
        
        with open("training_summary.log", "w", encoding="utf-8") as log:
            log.write(classification_report(y_test, y_pred, labels=labels_in_test, target_names=target_names))

        # Save artifacts
        joblib.dump(clf, os.path.join(MODEL_DIR, "message_classifier.pkl"))
        joblib.dump(vectorizer, os.path.join(MODEL_DIR, "message_vectorizer.pkl"))
        joblib.dump(le, os.path.join(MODEL_DIR, "message_label_encoder.pkl"))
        info = {
            "trained_on": datetime.now().isoformat(),
            "labels": le.classes_.tolist(),
            "num_samples": len(df),
            "features": vectorizer.get_feature_names_out().tolist()
        }

        with open(os.path.join(MODEL_DIR, "model_info.json"), "w", encoding="utf-8") as f:
            json.dump(info, f, indent=2)

        print("[OK] Message-level model artifacts saved to /model/")

# --- Branch 2: Otherwise, fall back to company-level training ---
if not os.path.exists(EXPORT_PATH) or df is None or df.empty:
    print("[Warning] No labeled messages available, training on company data from DB...")
    df = load_training_data()

    print(f"Dataset ready for training: {df.shape[0]} rows")
    print(df.head())

    # Prepare text field
    df["text"] = df["subject"].fillna("") + " " + df["body"].fillna("")

    # Encode labels
    le = LabelEncoder()
    df["company_encoded"] = le.fit_transform(df["company"])

    # Filter sparse classes
    company_counts = df["company"].value_counts()
    df = df[df["company"].isin(company_counts[company_counts > 1].index)]

    # Re-encode after filtering
    le = LabelEncoder()
    df["company_encoded"] = le.fit_transform(df["company"])

    print("Company label distribution:", df["company"].value_counts())

    if df["company_encoded"].nunique() < 2:
        print("[Warning] Not enough unique company labels to train a classifier.")
        exit(0)

    # Vectorize
    vectorizer = TfidfVectorizer(
        stop_words="english",
        max_features=5000,
        ngram_range=(1, 2),
    )
    X = vectorizer.fit_transform(df["text"])
    y = df["company_encoded"]

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Train model
    clf = LogisticRegression(max_iter=1000, class_weight="balanced")
    clf.fit(X_train, y_train)

    # Evaluate
    y_pred = clf.predict(X_test)
    labels_in_test = sorted(set(y_test))
    target_names = le.inverse_transform(labels_in_test)
    print(classification_report(y_test, y_pred, labels=labels_in_test, target_names=target_names))
    
    with open("training_summary.log", "w", encoding="utf-8") as log:
        log.write(classification_report(y_test, y_pred, labels=labels_in_test, target_names=target_names))

    # Save artifacts
    joblib.dump(clf, os.path.join(MODEL_DIR, "company_classifier.pkl"))
    joblib.dump(vectorizer, os.path.join(MODEL_DIR, "vectorizer.pkl"))
    joblib.dump(le, os.path.join(MODEL_DIR, "label_encoder.pkl"))
    print("Company-level model artifacts saved to /model/")

    info = {
    "trained_on": datetime.now().isoformat(),
    "labels": le.classes_.tolist(),
    "num_samples": len(df),
    "features": vectorizer.get_feature_names_out().tolist()
    }

    with open(os.path.join(MODEL_DIR, "model_info.json"), "w", encoding="utf-8") as f:
        json.dump(info, f, indent=2)

===== END OF FILE: train_model.py =====

